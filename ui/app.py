"""Streamlit interface for the RAG-based question answering workflow."""
from __future__ import annotations

import json
import textwrap
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

import streamlit as st

from components.analytics import render_context_summary
from components.context_viewer import render_contexts

import sys, os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from main import build_graph

SAMPLE_QUESTIONS = [
    "M√¥ h√¨nh RAG ƒë∆∞·ª£c gi·ªõi thi·ªáu ·ªü h·ªôi ngh·ªã n√†o?",
    "Nh·ªØng h∆∞·ªõng nghi√™n c·ª©u ti√™u bi·ªÉu v·ªÅ d·ªãch m√°y trong b·ªô d·ªØ li·ªáu n√†y?",
    "C√°c t√°c gi·∫£ Vi·ªát Nam c√≥ ƒë√≥ng g√≥p g√¨ n·ªïi b·∫≠t?",
]


@st.cache_resource(show_spinner=False)
def _load_query_graph():
    """Compile the LangGraph pipeline for query mode once per session."""

    return build_graph(force_mode="query")


def _invoke_query(
    question: str, top_k: int, context_limit: int, temperature: float
) -> Dict[str, Any]:
    graph = _load_query_graph()
    initial_state: Dict[str, Any] = {
        "user_input": question,
        "mode": "query",
        "trace": [],
        "top_k": top_k,
        "max_context_chars": context_limit,
        "llm_temperature": temperature,
        "ui_request": True,
    }
    return graph.invoke(initial_state)


def _render_history(history: List[Dict[str, Any]]) -> None:
    if not history:
        st.info("Ch∆∞a c√≥ l·ªãch s·ª≠. H√£y ƒë·∫∑t c√¢u h·ªèi ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
        return

    for item in reversed(history):
        title = item.get("question", "(kh√¥ng r√µ c√¢u h·ªèi)")
        ts = item.get("timestamp")
        if ts:
            title = f"{title} ‚Äî {ts}"
        with st.expander(f"‚ùì {title}"):
            st.markdown(item.get("response", "(Kh√¥ng c√≥ ph·∫£n h·ªìi)"))
            sources = item.get("sources")
            if sources:
                st.markdown("**Ngu·ªìn:**\n" + sources)
            latency = item.get("latency")
            if latency is not None:
                st.caption(f"Th·ªùi gian x·ª≠ l√Ω: {latency:.2f}s")
            if item.get("suggested_questions"):
                st.caption(
                    "G·ª£i √Ω ti·∫øp theo: "
                    + ", ".join(f"‚Äú{q}‚Äù" for q in item["suggested_questions"])
                )


def _serialize_history(history: List[Dict[str, Any]]) -> str:
    payload = []
    for item in history:
        payload.append(
            {
                "question": item.get("question"),
                "response": item.get("response"),
                "sources": item.get("sources"),
                "latency": item.get("latency"),
                "timestamp": item.get("timestamp"),
                "query_args": item.get("query_args"),
            }
        )
    return json.dumps(payload, ensure_ascii=False, indent=2)


def _ensure_session_state() -> None:
    st.session_state.setdefault("history", [])
    st.session_state.setdefault("question_input", "")


def _render_suggestion_buttons(suggestions: List[str]) -> None:
    if not suggestions:
        return

    st.markdown("#### G·ª£i √Ω c√¢u h·ªèi ti·∫øp theo")
    cols = st.columns(len(suggestions))
    for idx, suggestion in enumerate(suggestions):
        if cols[idx].button(f"‚û°Ô∏è {suggestion}", key=f"suggestion_{idx}"):
            st.session_state["question_input"] = suggestion
            st.rerun()


def _render_samples() -> None:
    st.markdown("#### G·ª£i √Ω nhanh")
    cols = st.columns(len(SAMPLE_QUESTIONS))
    for idx, sample in enumerate(SAMPLE_QUESTIONS):
        if cols[idx].button(sample, key=f"sample_{idx}"):
            st.session_state["question_input"] = sample
            st.rerun()


def main():
    st.set_page_config(
        page_title="RAG QA over Instruct2DS",
        page_icon="ü§ñ",
        layout="wide",
    )
    _ensure_session_state()

    st.title("RAG-Based Question Answering for Online PDF Documents")
    st.caption(
        "H·ªá th·ªëng h·ªèi-ƒë√°p ti·∫øng Vi·ªát s·ª≠ d·ª•ng ki·∫øn tr√∫c Retrieval-Augmented Generation "
        "v·ªõi LangGraph, ChromaDB, embedding `Alibaba-NLP/gte-Qwen2-1.5B-instruct` "
        "v√† m√¥ h√¨nh ƒëi·ªÅu ph·ªëi/sinh tr·∫£ l·ªùi `qwen2.5:7b` ch·∫°y qua Ollama."
    )

    with st.sidebar:
        st.header("Thi·∫øt l·∫≠p truy v·∫•n")
        top_k = st.slider("S·ªë ƒëo·∫°n context (top-k)", min_value=1, max_value=10, value=5, step=1)
        context_limit = st.slider(
            "Gi·ªõi h·∫°n ƒë·ªô d√†i ng·ªØ c·∫£nh (k√Ω t·ª±)", min_value=1000, max_value=8000, value=4000, step=500
        )
        temperature = st.slider(
            "Nhi·ªát ƒë·ªô tr·∫£ l·ªùi (0 = b·∫£o th·ªß, 1.0 = s√°ng t·∫°o)",
            min_value=0.0,
            max_value=1.0,
            step=0.05,
            value=0.1,
        )
        show_trace = st.toggle("Hi·ªÉn th·ªã trace chi ti·∫øt", value=False)

        if st.session_state["history"]:
            if st.button("üóëÔ∏è Xo√° l·ªãch s·ª≠", use_container_width=True):
                st.session_state["history"] = []
                st.success("ƒê√£ xo√° l·ªãch s·ª≠ truy v·∫•n.")

            st.download_button(
                "üíæ T·∫£i l·ªãch s·ª≠ (.json)",
                data=_serialize_history(st.session_state["history"]),
                file_name="rag_query_history.json",
                mime="application/json",
                use_container_width=True,
            )

        st.divider()
        st.markdown(
            textwrap.dedent(
                """
                **Quy tr√¨nh**

                1. C√¢u h·ªèi ƒë∆∞·ª£c nh√∫ng b·∫±ng m√¥ h√¨nh `Alibaba-NLP/gte-Qwen2-1.5B-instruct`.
                2. ChromaDB truy xu·∫•t top-k ƒëo·∫°n vƒÉn d·ª±a tr√™n cosine similarity.
                3. C√°c ƒëo·∫°n ƒë∆∞·ª£c n√©n ƒë·ªÉ ph√π h·ª£p gi·ªõi h·∫°n context v√† g·ª≠i t·ªõi `qwen2.5:7b` (Ollama).
                4. C√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c h·∫≠u x·ª≠ l√Ω v·ªõi ngu·ªìn tr√≠ch d·∫´n r√µ r√†ng.
                """
            )
        )

    _render_samples()

    question = st.text_area(
        "Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n",
        height=160,
        key="question_input",
        placeholder="V√≠ d·ª•: \"M√¥ h√¨nh RAG ƒë∆∞·ª£c gi·ªõi thi·ªáu ·ªü h·ªôi ngh·ªã n√†o?\"",
    )
    submit = st.button("Truy v·∫•n", type="primary", use_container_width=True)

    latest_result: Dict[str, Any] | None = None
    if submit:
        clean_question = question.strip()
        if not clean_question:
            st.warning("Vui l√≤ng nh·∫≠p c√¢u h·ªèi tr∆∞·ªõc khi truy v·∫•n.")
        else:
            with st.spinner("ƒêang truy v·∫•n v√† sinh c√¢u tr·∫£ l·ªùi..."):
                latest_result = _invoke_query(
                    clean_question,
                    top_k=top_k,
                    context_limit=context_limit,
                    temperature=temperature,
                )

            if latest_result:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                entry = {
                    "question": clean_question,
                    "response": latest_result.get("response"),
                    "sources": latest_result.get("sources_markdown"),
                    "latency": latest_result.get("latency_seconds"),
                    "retrieved_docs": list(latest_result.get("retrieved_docs", [])),
                    "trace": list(latest_result.get("trace", [])),
                    "answer_model": latest_result.get("answer_model"),
                    "query_args": dict(latest_result.get("query_args", {})),
                    "context_summary": latest_result.get("context_summary"),
                    "suggested_questions": latest_result.get("suggested_questions", []),
                    "llm_temperature": latest_result.get("llm_temperature"),
                    "timestamp": timestamp,
                }
                st.session_state["history"].append(entry)
                st.session_state["history"] = st.session_state["history"][-50:]
                latest_result = entry

    if latest_result is None and st.session_state["history"]:
        latest_result = st.session_state["history"][-1]

    tabs = st.tabs(["K·∫øt qu·∫£", "Ng·ªØ c·∫£nh", "Ph√¢n t√≠ch", "L·ªãch s·ª≠"])

    with tabs[0]:
        if latest_result:
            response_md = latest_result.get("response", "")
            if response_md:
                st.markdown(response_md)

            meta_cols = st.columns(4)
            latency_value = latest_result.get("latency")
            if latency_value is None:
                latency_value = latest_result.get("latency_seconds", 0.0)
            meta_cols[0].metric("Th·ªùi gian", f"{latency_value:.2f}s")
            meta_cols[1].metric("S·ªë ƒëo·∫°n", len(latest_result.get("retrieved_docs", [])))
            qa_args = latest_result.get("query_args", {})
            meta_cols[2].metric("Top-k", qa_args.get("top_k", top_k))

            _render_suggestion_buttons(latest_result.get("suggested_questions", []))

            if show_trace:
                st.markdown("### Trace LangGraph")
                for trace_line in latest_result.get("trace", []):
                    st.code(trace_line)
        else:
            st.info("Ch∆∞a c√≥ c√¢u tr·∫£ l·ªùi n√†o. H√£y nh·∫≠p c√¢u h·ªèi ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

    with tabs[1]:
        if latest_result:
            render_contexts(latest_result.get("retrieved_docs", []))
        else:
            st.info("Ch∆∞a c√≥ ng·ªØ c·∫£nh ƒë·ªÉ hi·ªÉn th·ªã.")

    with tabs[2]:
        render_context_summary(latest_result.get("context_summary") if latest_result else None)

    with tabs[3]:
        st.subheader("L·ªãch s·ª≠ truy v·∫•n")
        _render_history(st.session_state["history"])


if __name__ == "__main__":
    main()