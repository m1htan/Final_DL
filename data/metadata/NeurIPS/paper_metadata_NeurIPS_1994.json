{
    "0": {
        "TITLE": "SIMPLIFYING NEURAL NETS BY DISCOVERING FLAT MINIMA",
        "AUTHORS": "Sepp Hochreiter, Jürgen Schmidhuber",
        "ABSTRACT": "We present a new algorithm for finding low complexity networks  with high generalization capability. The algorithm searches for  large connected regions of so-called ''fiat'' minima of the error func(cid:173) tion. In the weight-space environment of a \"flat\" minimum, the  error remains approximately constant. Using an MDL-based ar(cid:173) gument, flat minima can be shown to correspond to low expected  overfitting. Although our algorithm requires the computation of  second order derivatives, it has backprop's order of complexity.  Experiments with feedforward and recurrent nets are described. In  an application to stock market prediction, the method outperforms  conventional backprop, weight decay, and \"optimal brain surgeon\" .",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/01882513d5fa7c329e940dda99b12147-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/01882513d5fa7c329e940dda99b12147-Bibtex.bib",
            "SUPP": ""
        }
    },
    "1": {
        "TITLE": "A Model of the Neural Basis of the Rat's Sense of Direction",
        "AUTHORS": "William E. Skaggs, James J. Knierim, Hemant S. Kudrimoti, Bruce L. McNaughton",
        "ABSTRACT": "In  the last decade  the outlines  of the neural  structures  subserving  the sense of direction have begun to emerge.  Several investigations  have shed  light on  the effects  of vestibular  input  and visual  input  on  the  head  direction  representation.  In  this  paper,  a  model  is  formulated of the neural mechanisms underlying the head direction  system.  The model is  built out of simple ingredients,  depending  on  nothing  more complicated than  connectional  specificity,  attractor  dynamics,  Hebbian  learning,  and  sigmoidal  nonlinearities,  but  it  behaves  in  a  sophisticated  way  and  is  consistent  with  most of the  observed properties ofreal head direction cells.  In addition it makes  a  number  of predictions  that  ought  to  be  testable  by  reasonably  straightforward experiments. \n1  Head  Direction  Cells  in  the Rat \nThere  is  quite  a  bit  of  behavioral  evidence  for  an  intrinsic  sense  of  direction  in  many species  of mammals, including  rats  and  humans  (e.g.,  Gallistel,  1990).  The  first  specific  information regarding  the  neural  basis  of this  \"sense\"  came with  the  discovery  by  Ranck  (1984)  of a  population  of  \"head  direction\"  cells  in  the  dorsal  presubiculum (also known as the  \"postsubiculum\") of the rat.  A head direction cell \n174 \nWilliam  Skaggs,  James  J.  Knierim,  Hemant  S. Kudrimoti,  Bruce L.  McNaughton \nfires  at  a  high  rate  if and  only  if the rat's  head  is  oriented  in  a  specific  direction.  Many  things  could  potentially  cause  a  cell  to  fire  in  a  head-direction  dependent  manner:  what made the postsubicular  cells  particularly interesting  was  that when  their directionality was tested with the rat at different locations, the head directions  corresponding to maximal firing  were  consistently parallel, within the experimental  resolution.  This  is  difficult  to  explain  with  a  simple sensory-based  mechanism;  it  implies something more sophisticated.1 \nThe  postsubicular  head  direction  cells  were  studied  in  depth  by  Taube  et  al.  (1990a,b),  and,  more  recently,  head  direction  cells  have  also  been  found  in  other  parts of the rat brain,  in  particular  the anterior nuclei  of the  thalamus (Mizumori  and  Williams,  1993)  and  the  retrosplenial  (posterior  cingulate)  cortex  (Chen  et  al.,  1994a,b).  Interestingly,  all  of these  areas  are  intimately  associated  with  the  hippocampal  formation,  which  in  the  rat  contains  large numbers  of  \"place\"  cells.  Thus,  the  brain  contains  separate  but  neighboring  populations  of cells  coding  for  location and  cells  coding for  direction,  which taken together  represent  much of the  information needed  for  navigation. \nFigure  1 shows  directional  tuning curves  for  three  typical head  direction cells from  the  anterior  thalamus.  In each  of them the  breadth  of tuning is  on the order of 90  degrees.  This value is  also typical for  head direction cells in the postsubiculum and  retrosplenial  cortex,  though  in  each  of the  three  areas  individual  cells  may  show  considerable variability. \nFigure  1:  Polar plots  of directional  tuning  (mean firing  rate  as  a  function  of head  direction)  for  three  typical head  direction  cells from the anterior thalamus of a  rat. \nEvery study  to date has indicated that the head  direction cells  constitute a unitary  system,  together  with  the  place  cells  of  the  hippocampus.  Whenever  two  head  direction  cells  have  been  recorded  simultaneously,  any  manipulation  that  caused  one of them to shift its directional  alignment caused the other to shift by  the same  amount;  and  when  head  direction  cells  have  been  recorded  simultaneously  with  place  cells,  any  manipulation that caused  the head  direction  cells  to  realign either  caused  the hippocampal place fields  to rotate correspondingly or  to  \"remap\" into a  different  pattern  (Knierim  et  al.,  1995). \nHead direction cells  maintain their directional tuning for  some time when  the lights  in  the recording  room are  turned  off,  leaving  an  animal in  complete  darkness;  the  directionality tends to gradually drift, though, especially if the animal moves around  (Mizumori and Williams, 1993).  Directional tuning is preserved to some degree even \n1 Sensitivity  to  the  Earth's  geomagnetic  field  has  been  ruled  out  as  an  explanation  of \nhead-directional  firing . \nA Model of the  Neural Basis of the Rat's Sense of Direction \n175 \nif an animal is  passively rotated in the dark, which indicates strongly that the head  direction system receives information (possibly indirect) from the vestibular system. \nVisual  input  influences  but  does  not  dictate  the  behavior  of head  direction  cells.  The  nature  of this  influence is  quite  interesting.  In  a  recent  series  of experiments  (Knierim  et  al.,  1995), rats were trained to forage for  food pellets in a gray cylinder  with  a  single salient  directional  cue,  a  white card  covering  90  degrees  of the  wall.  During  training,  half of the  rats were  disoriented  before  being  placed  in the  cylin(cid:173) der,  in  order  to  disrupt  the  relation  between  their  internal  sense  of direction  and  the  location  of the  cue  card;  the  other  half of the rats  were  not  disoriented.  Pre(cid:173) sumably,  the  rats  that were  not  disoriented  during  training  experienced  the same  initial relationship between their internal direction sense and the CUe card each time  they  were  placed  in  the  cylinder;  this  would  not have  been  true of the disoriented  rats.  Head  direction  cells  in  the  thalamus were  subsequently  recorded  from  both  groups of rats  as  they  moved  in  the cylinder.  All rats  were  disoriented  before  each  recording  session.  Under  these  conditions,  the  cue  card  had  much  weaker  control  over  the  head  direction  cells  in  the  rats  that had  been  disoriented  during training  than in  the  rats  that had not been  disoriented.  For  all rats the influence of the cue  card upon the head direction system weakened gradually over the course of multiple  recording  sessions,  and eventually they  broke free,  but this happened  much sooner  in  the rats  that had been  disoriented  during training.  The authors  concluded  that  a  visual  cue  could  only  develop  a  strong  influence  upon  the  head  direction  system  if  the  rat  experienced  it  as  stable. \nFigure 2 illustrates the shifts in  alignment during a typical recording session.  When  the  rat  is  initially placed  in  the  cylinder,  the  cell's  tuning  curve  is  aligned  to  the  west.  Over the first few  minutes of recording it gradually rotates to SSW, and there  it stays.  Note  the  \"tail\" of the curve.  This comes from spikes belonging to another,  neighboring head  direction cell, which could  not be perfectly isolated from the first.  Note that, even though they come from different cells,  both portions shift alignment  synchronously. \nFigure  2:  Shifts  in  alignment  of  a  head  direction  cell  over  the  course  of a  single  recording session  (one  minute intervals).",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/024d7f84fff11dd7e8d9c510137a2381-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/024d7f84fff11dd7e8d9c510137a2381-Bibtex.bib",
            "SUPP": ""
        }
    },
    "2": {
        "TITLE": "A Mixture Model System for Medical and Machine Diagnosis",
        "AUTHORS": "Magnus Stensmo, Terrence J. Sejnowski",
        "ABSTRACT": "Diagnosis of human disease or machine fault is a missing data problem  since many variables are initially unknown. Additional information needs  to be obtained. The j oint probability distribution of the data can be used to  solve this problem. We model this with mixture models whose parameters  are estimated by the EM algorithm.  This gives the benefit that missing  data in the database itself can also be handled correctly.  The request for  new information to refine the diagnosis is performed using the maximum  utility  principle.  Since  the  system  is  based  on  learning  it  is  domain  independent and less labor intensive than expert systems or probabilistic  networks.  An example using a heart disease database is presented.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/03e0704b5690a2dee1861dc3ad3316c9-Bibtex.bib",
            "SUPP": ""
        }
    },
    "3": {
        "TITLE": "Learning with Preknowledge: Clustering with Point and Graph Matching Distance Measures",
        "AUTHORS": "Steven Gold, Anand Rangarajan, Eric Mjolsness",
        "ABSTRACT": "Prior constraints are imposed upon a learning problem in the form  of distance measures. Prototypical 2-D point sets and graphs are  learned by clustering with point matching and graph matching dis(cid:173) tance measures. The point matching distance measure is approx.  invariant under affine transformations - translation, rotation, scale  and shear - and permutations. It operates between noisy images  with missing and spurious points. The graph matching distance  measure operates on weighted graphs and is invariant under per(cid:173) mutations. Learning is formulated as an optimization problem .  Large objectives so formulated ('\" million variables) are efficiently  minimized using a combination of optimization techniques - alge(cid:173) braic transformations, iterative projective scaling, clocked objec(cid:173) tives, and deterministic annealing.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/043c3d7e489c69b48737cc0c92d0f3a2-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/043c3d7e489c69b48737cc0c92d0f3a2-Bibtex.bib",
            "SUPP": ""
        }
    },
    "4": {
        "TITLE": "Learning Local Error Bars for Nonlinear Regression",
        "AUTHORS": "David A. Nix, Andreas S. Weigend",
        "ABSTRACT": "We  present a  new  method for  obtaining  local  error bars for  nonlinear  regression,  i.e.,  estimates of the confidence in predicted values that de(cid:173) pend on the input.  We approach this problem by applying a maximum(cid:173) likelihood framework to an assumed distribution of errors.  We demon(cid:173) strate our method first on computer-generated data with locally varying,  normally distributed target noise.  We then apply it to laser data from the  Santa Fe  Time  Series Competition where the underlying system noise is  known quantization error and the error bars give local estimates of model  misspecification.  In  both  cases,  the method also provides a  weighted(cid:173) regression effect that improves generalization performance. \n1  Learning Local Error Bars Using a Maximum Likelihood \nFramework:  Motivation, Concept, and Mechanics \nFeed-forward artificial neural networks used for nonlinear regression can be interpreted as  predicting the  mean of the  target distribution as  a function  of (conditioned on)  the  input  pattern (e.g., Buntine & Weigend, 1991; Bishop, 1994), typically using one linear output unit  per output variable. If parameterized, this conditional target distribution (CID) may also be \n·http://www.cs.colorado.edu/~andreas/Home.html. \nThis  paper  is  available  with  figures  in  colors  as  ftp://ftp.cs.colorado.edu/pub/  Time-Series/MyPapers/nix.weigenCLnips7.ps.Z  . \n490 \nDavid A.  Nix,  Andreas S.  Weigend \nviewed as an error model (Rumelhart et al.,  1995).  Here, we present a simple method that  provides higher-order information about the cm than simply the mean.  Such additional  information could come from  attempting  to  estimate  the  entire cm with  connectionist  methods (e.g., \"Mixture Density Networks,\" Bishop, 1994; \"fractional binning, \"Srivastava  & Weigend,  1994) or with non-connectionist methods such as  a Monte Carlo on a hidden  Markov model (Fraser & Dimitriadis, 1994). While non-parametric estimates of the shape  of a  C1D require large quantities of data,  our less data-hungry method (Weigend & Nix,  1994) assumes a specific parameterized form of the C1D (e.g., Gaussian) and gives us the  value of the error bar (e.g., the width of the Gaussian) by finding those parameters which  maximize the likelihood that the target data was generated by a particular network model.  In this paper we derive the specific update rules for the Gaussian case.  We  would like to  emphasize, however, that any parameterized unimodal distribution can be used for the em  in the method presented here. \nj------------,  I  I------T-------,  ,  I  A  I  A2,.  )  o y(x)  0  cr IX  /\\  i  '.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/061412e4a03c02f9902576ec55ebbe77-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/061412e4a03c02f9902576ec55ebbe77-Bibtex.bib",
            "SUPP": ""
        }
    },
    "5": {
        "TITLE": "Reinforcement Learning Methods for Continuous-Time Markov Decision Problems",
        "AUTHORS": "Steven J. Bradtke, Michael O. Duff",
        "ABSTRACT": "Semi-Markov  Decision  Problems  are  continuous  time  generaliza(cid:173) tions  of discrete  time  Markov  Decision  Problems.  A  number  of  reinforcement  learning  algorithms  have  been  developed  recently  for  the  solution  of Markov  Decision  Problems,  based on  the ideas  of asynchronous dynamic programming and stochastic approxima(cid:173) tion.  Among these  are TD(,x), Q-Iearning,  and Real-time Dynamic  Programming.  After  reviewing  semi-Markov  Decision  Problems  and Bellman's optimality equation in  that context,  we  propose al(cid:173) gorithms similar  to those  named above,  adapted to the solution  of  semi-Markov Decision  Problems.  We demonstrate these algorithms  by  applying  them to the  problem of determining  the  optimal con(cid:173) trol for  a  simple  queueing  system.  We  conclude  with  a  discussion  of circumstances  under which these  algorithms may be usefully  ap(cid:173) plied.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/07871915a8107172b3b5dc15a6574ad3-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/07871915a8107172b3b5dc15a6574ad3-Bibtex.bib",
            "SUPP": ""
        }
    },
    "6": {
        "TITLE": "Connectionist Speaker Normalization with Generalized Resource Allocating Networks",
        "AUTHORS": "Cesare Furlanello, Diego Giuliani, Edmondo Trentin",
        "ABSTRACT": "The paper presents  a  rapid speaker-normalization technique based  on  neural  network  spectral  mapping.  The  neural  network  is  used  as a front-end  of a  continuous speech  recognition system  (speaker(cid:173) dependent,  HMM-based) to normalize the input acoustic data from  a  new  speaker.  The  spectral  difference  between  speakers  can  be  reduced  using  a  limited amount  of new  acoustic  data (40  phonet(cid:173) ically  rich  sentences).  Recognition  error  of phone  units  from  the  acoustic-phonetic  continuous  speech  corpus  APASCI  is  decreased  with an adaptability ratio of 25%.  We used  local basis networks of  elliptical  Gaussian  kernels,  with  recursive  allocation  of units  and  on-line  optimization of parameters  (GRAN model).  For  this  ap(cid:173) plication,  the  model included  a  linear  term.  The results  compare  favorably  with  multivariate linear  mapping based  on  constrained  orthonormal transformations.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/08fe2621d8e716b02ec0da35256a998d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/08fe2621d8e716b02ec0da35256a998d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "7": {
        "TITLE": "A Novel Reinforcement Model of Birdsong Vocalization Learning",
        "AUTHORS": "Kenji Doya, Terrence J. Sejnowski",
        "ABSTRACT": "Songbirds learn to imitate a tutor song through auditory and motor learn(cid:173) ing.  We  have developed a theoretical framework  for song learning that  accounts  for response properties of neurons that have been  observed in  many  of the nuclei  that are involved in  song learning.  Specifically,  we  suggest that the anteriorforebrain pathway, which is not needed for song  production  in  the  adult  but is  essential  for  song  acquisition,  provides  synaptic perturbations and adaptive evaluations for syllable vocalization  learning.  A computer model based on reinforcement learning was  con(cid:173) structed  that could replicate a real  zebra finch  song with 90% accuracy  based on a spectrographic measure.  The second generation of the bird(cid:173) song model replicated the tutor song with 96% accuracy.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/0a113ef6b61820daa5611c870ed8d5ee-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/0a113ef6b61820daa5611c870ed8d5ee-Bibtex.bib",
            "SUPP": ""
        }
    },
    "8": {
        "TITLE": "Bias, Variance and the Combination of Least Squares Estimators",
        "AUTHORS": "Ronny Meir",
        "ABSTRACT": "We consider the effect of combining several least squares estimators  on  the expected  performance of a  regression  problem.  Computing  the exact bias and  variance curves  as  a function of the sample size  we  are able to quantitatively compare the effect of the combination  on the bias and variance separately, and thus on the expected  error  which  is  the sum of the  two.  Our exact  calculations,  demonstrate  that the combination of estimators is particularly useful  in the case  where the data set is small and noisy and the function to be learned  is  unrealizable.  For  large  data sets  the  single  estimator produces  superior  results.  Finally,  we  show  that  by  splitting  the  data set  into  several  independent  parts  and  training  each  estimator  on  a  different subset,  the performance can in some cases  be significantly  improved. \nKey  words:  Bias,  Variance,  Least  Squares,  Combination.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/0b8aff0438617c055eb55f0ba5d226fa-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/0b8aff0438617c055eb55f0ba5d226fa-Bibtex.bib",
            "SUPP": ""
        }
    },
    "9": {
        "TITLE": "Hierarchical Mixtures of Experts Methodology Applied to Continuous Speech Recognition",
        "AUTHORS": "Ying Zhao, Richard M. Schwartz, Jason J. Sroka, John Makhoul",
        "ABSTRACT": "In  this  paper,  we  incorporate  the  Hierarchical  Mixtures  of Experts  (HME)  method  of probability  estimation,  developed  by  Jordan  [1],  into  an  HMM(cid:173) based  continuous  speech  recognition  system.  The  resulting  system  can  be  thought of as a continuous-density HMM system, but instead of using gaussian  mixtures,  the HME system employs a large set of hierarchically organized but  relatively  small  neural  networks  to  perform the probability density estimation.  The  hierarchical  structure  is  reminiscent  of  a  decision  tree  except  for  two  important differences:  each  \"expert\" or neural  net  performs a  \"soft\" decision  rather than a hard decision, and,  unlike ordinary decision trees,  the parameters  of all  the  neural  nets  in  the  HME  are  automatically  trainable  using  the  EM  algorithm.  We  report results on  the ARPA  5,OOO-word  and 4O,OOO-word Wall  Street Journal  corpus using  HME  models.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/0d0871f0806eae32d30983b62252da50-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/0d0871f0806eae32d30983b62252da50-Bibtex.bib",
            "SUPP": ""
        }
    },
    "10": {
        "TITLE": "A Comparison of Discrete-Time Operator Models for Nonlinear System Identification",
        "AUTHORS": "Andrew D. Back, Ah Chung Tsoi",
        "ABSTRACT": "We present a unifying view of discrete-time operator models used in the  context of finite word length linear signal processing.  Comparisons are  made between the recently presented gamma operator model, and the delta  and rho operator models for performing nonlinear system identification  and prediction using neural networks.  A new model based on an adaptive  bilinear  transformation  which  generalizes  all  of the  above  models  is  presented.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/0efe32849d230d7f53049ddc4a4b0c60-Bibtex.bib",
            "SUPP": ""
        }
    },
    "11": {
        "TITLE": "Learning Many Related Tasks at the Same Time with Backpropagation",
        "AUTHORS": "Rich Caruana",
        "ABSTRACT": "Hinton  [6]  proposed  that  generalization  in  artificial  neural  nets  should improve  if nets  learn  to represent the  domain's underlying  regularities.  Abu-Mustafa's  hints work  [1]  shows  that the  outputs  of a  backprop  net  can  be  used  as  inputs  through  which  domain(cid:173) specific  information can be given to the net.  We extend these ideas  by showing that a  backprop net learning many related tasks at the  same time can use these  tasks  as inductive  bias for  each other and  thus learn better.  We  identify five  mechanisms by which multitask  backprop improves generalization and give  empirical evidence that  multi task backprop generalizes  better  in real  domains.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/0f840be9b8db4d3fbd5ba2ce59211f55-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/0f840be9b8db4d3fbd5ba2ce59211f55-Bibtex.bib",
            "SUPP": ""
        }
    },
    "12": {
        "TITLE": "Multidimensional Scaling and Data Clustering",
        "AUTHORS": "Thomas Hofmann, Joachim Buhmann",
        "ABSTRACT": "Visualizing and structuring pairwise dissimilarity data are difficult combinatorial op(cid:173) timization problems known as multidimensional scaling or pairwise data clustering.  Algorithms for embedding dissimilarity data set in a Euclidian space, for clustering  these data and for actively selecting data to support the clustering process are discussed  in the maximum entropy framework. Active data selection provides a strategy to discover  structure in a data set efficiently with partially unknown data.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/1587965fb4d4b5afe8428a4a024feb0d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/1587965fb4d4b5afe8428a4a024feb0d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "13": {
        "TITLE": "Predicting the Risk of Complications in Coronary Artery Bypass Operations using Neural Networks",
        "AUTHORS": "Richard P Lippmann, Linda Kukolich, David Shahian",
        "ABSTRACT": "Experiments  demonstrated  that  sigmoid  multilayer  perceptron  (MLP)  networks  provide  slightly  better  risk  prediction  than  conventional  logistic  regression  when  used  to  predict  the  risk  of death,  stroke,  and  renal  failure  on  1257  patients  who  underwent  coronary  artery  bypass  operations at the Lahey Clinic. MLP networks with no hidden layer and  networks  with one hidden  layer were trained  using  stochastic  gradient  descent with early stopping. MLP networks and logistic regression used  the  same  input  features  and  were  evaluated  using  bootstrap  sampling  with  50  replications.  ROC  areas  for  predicting  mortality  using  preoperative  input  features  were  70.5%  for  logistic  regression  and  76.0%  for  MLP  networks.  Regularization  provided  by  early  stopping  was  an  important  component  of improved  perfonnance.  A  simplified  approach  to  generating  confidence  intervals  for  MLP risk  predictions  using  an  auxiliary  \"confidence  MLP\"  was  developed.  The  confidence  MLP  is  trained  to  reproduce  confidence  intervals  that  were  generated  during  training  using  the  outputs  of  50  MLP  networks  trained  with  different bootstrap samples.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/168908dd3227b8358eababa07fcaf091-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/168908dd3227b8358eababa07fcaf091-Bibtex.bib",
            "SUPP": ""
        }
    },
    "14": {
        "TITLE": "Spatial Representations in the Parietal Cortex May Use Basis Functions",
        "AUTHORS": "Alexandre Pouget, Terrence J. Sejnowski",
        "ABSTRACT": "The  parietal  cortex  is  thought  to  represent  the  egocentric  posi(cid:173) tions  of objects  in  particular  coordinate systems.  We  propose  an  alternative  approach  to  spatial  perception  of objects  in  the  pari(cid:173) etal  cortex from  the  perspective  of sensorimotor  transformations.  The responses of single parietal neurons can be modeled as  a gaus(cid:173) sian  function  of retinal  position  multiplied by  a  sigmoid function  of eye  position,  which  form  a  set of basis functions.  We  show  here  how  these  basis functions  can  be  used  to generate  receptive  fields  in  either  retinotopic or head-centered  coordinates by simple linear  transformations.  This raises the possibility that the parietal cortex  does  not  attempt to compute the positions of objects  in  a  partic(cid:173) ular  frame  of  reference  but  instead  computes  a  general  purpose  representation  of the retinal  location and eye  position from  which  any  transformation can  be  synthesized  by  direct  projection.  This  representation  predicts that hemineglect,  a  neurological syndrome  produced  by  parietal lesions,  should  not  be  confined  to egocentric  coordinates,  but should be observed in multiple frames of reference  in  single patients,  a  prediction supported  by several experiments. \n158 \nAlexandre Pouget,  Terrence J.  Sejnowski",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/170c944978496731ba71f34c25826a34-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/170c944978496731ba71f34c25826a34-Bibtex.bib",
            "SUPP": ""
        }
    },
    "15": {
        "TITLE": "Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems",
        "AUTHORS": "Tommi Jaakkola, Satinder P. Singh, Michael I. Jordan",
        "ABSTRACT": "Increasing  attention has  been  paid to reinforcement  learning  algo(cid:173) rithms  in  recent  years,  partly  due  to  successes  in  the  theoretical  analysis  of their  behavior in  Markov  environments.  If the  Markov  assumption  is  removed,  however,  neither  generally  the  algorithms  nor  the  analyses  continue  to  be  usable.  We  propose  and  analyze  a  new  learning  algorithm  to  solve  a  certain  class  of  non-Markov  decision  problems.  Our  algorithm  applies  to  problems  in  which  the  environment  is  Markov,  but  the  learner  has  restricted  access  to  state  information.  The  algorithm involves  a  Monte-Carlo  pol(cid:173) icy evaluation combined with a  policy improvement method that is  similar to  that  of Markov  decision  problems  and  is  guaranteed  to  converge to a local maximum.  The algorithm operates in the space  of stochastic  policies,  a  space  which  can  yield  a  policy  that  per(cid:173) forms  considerably  better  than any deterministic policy.  Although  the  space  of stochastic  policies  is  continuous-even  for  a  discrete  action space-our algorithm is computationally tractable. \n346 \nTommi  Jaakkola,  Satinder P.  Singh,  Michaell.  Jordan",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/1c1d4df596d01da60385f0bb17a4a9e0-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/1c1d4df596d01da60385f0bb17a4a9e0-Bibtex.bib",
            "SUPP": ""
        }
    },
    "16": {
        "TITLE": "FINANCIAL APPLICATIONS OF LEARNING FROM HINTS",
        "AUTHORS": "Yaser S. Abu-Mostafa",
        "ABSTRACT": "The basic paradigm for learning in neural networks is 'learning from  examples' where a training set of input-output examples is used to  teach the network the target function. Learning from hints is a gen(cid:173) eralization of learning from examples where additional information  about the target function can be incorporated in the same learning  process. Such information can come from common sense rules or  special expertise. In financial market applications where the train(cid:173) ing data is very noisy, the use of such hints can have a decisive  advantage. We demonstrate the use of hints in foreign-exchange  trading of the U.S. Dollar versus the British Pound, the German  Mark, the Japanese Yen, and the Swiss Franc, over a period of 32  months. We explain the general method of learning from hints and  how it can be applied to other markets. The learning model for  this method is not restricted to neural networks.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/1cc3633c579a90cfdd895e64021e2163-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/1cc3633c579a90cfdd895e64021e2163-Bibtex.bib",
            "SUPP": ""
        }
    },
    "17": {
        "TITLE": "An Auditory Localization and Coordinate Transform Chip",
        "AUTHORS": "Timothy K. Horiuchi",
        "ABSTRACT": "The  localization  and  orientation  to  various  novel  or  interesting  events  in  the  environment  is  a  critical  sensorimotor  ability in  all  animals,  predator  or  prey.  In  mammals,  the  superior  colliculus  (SC)  plays  a  major  role  in  this  behavior,  the  deeper  layers  ex(cid:173) hibiting topographically mapped responses to visual, auditory, and  somatosensory  stimuli.  Sensory  information arriving from  differ(cid:173) ent modalities should  then be  represented  in  the same coordinate  frame.  Auditory  cues,  in  particular,  are  thought to be  computed  in head-based coordinates which must then be transformed to reti(cid:173) nal coordinates.  In this paper, an analog VLSI implementation for  auditory localization in the azimuthal plane is described  which ex(cid:173) tends  the  architecture  proposed for  the  barn owl to a  primate eye  movement system  where  further  transformation  is  required.  This  transformation is intended to model the projection in primates from  auditory  cortical areas to the deeper layers of the primate superior  colliculus.  This  system  is  interfaced  with  an  analog  VLSI-based  saccadic eye movement system also being constructed  in our labo(cid:173) ratory.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/1ce927f875864094e3906a4a0b5ece68-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/1ce927f875864094e3906a4a0b5ece68-Bibtex.bib",
            "SUPP": ""
        }
    },
    "18": {
        "TITLE": "Limits on Learning Machine Accuracy Imposed by Data Quality",
        "AUTHORS": "Corinna Cortes, L. D. Jackel, Wan-Ping Chiang",
        "ABSTRACT": "Random  errors  and  insufficiencies  in  databases  limit  the  perfor(cid:173) mance  of any classifier  trained  from  and applied  to the  database.  In this paper we  propose a method to estimate the limiting perfor(cid:173) mance of classifiers imposed by the database.  We demonstrate this  technique  on  the  task  of predicting  failure  in  telecommunication  paths.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/1e056d2b0ebd5c878c550da6ac5d3724-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/1e056d2b0ebd5c878c550da6ac5d3724-Bibtex.bib",
            "SUPP": ""
        }
    },
    "19": {
        "TITLE": "Interference in Learning Internal Models of Inverse Dynamics in Humans",
        "AUTHORS": "Reza Shadmehr, Tom Brashers-Krug, Ferdinando A. Mussa-Ivaldi",
        "ABSTRACT": "Experiments were  performed  to  reveal  some of the  computational  properties  of the  human  motor  memory  system.  We  show  that  as  humans  practice  reaching  movements while  interacting  with  a  novel  mechanical environment, they learn an  internal model of the  inverse dynamics of that environment.  Subjects show recall of this  model  at testing  sessions  24  hours  after  the  initial practice.  The  representation  of the internal model in  memory is  such  that there  is  interference  when  there  is  an  attempt  to  learn  a  new  inverse  dynamics  map  immediately after  an  anticorrelated  mapping  was  learned.  We  suggest  that  this  interference  is  an  indication  that  the same computational elements  used  to encode  the first  inverse  dynamics  map  are  being  used  to  learn  the  second  mapping.  We  predict  that this leads to a forgetting  of the initially learned skill.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/1e48c4420b7073bc11916c6c1de226bb-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/1e48c4420b7073bc11916c6c1de226bb-Bibtex.bib",
            "SUPP": ""
        }
    },
    "20": {
        "TITLE": "Optimal Movement Primitives",
        "AUTHORS": "Terence D. Sanger",
        "ABSTRACT": "The  theory  of Optimal  Unsupervised  Motor  Learning  shows  how  a  network  can  discover  a  reduced-order  controller for  an  unknown  nonlinear system by  representing only  the most significant  modes.  Here,  I extend the theory  to apply to command sequences,  so that  the most significant  components discovered  by  the  network  corre(cid:173) spond  to  motion  \"primitives\".  Combinations of these  primitives  can  be  used  to  produce  a  wide  variety  of  different  movements.  I  demonstrate  applications  to  human  handwriting  decomposition  and  synthesis,  as  well  as  to  the  analysis  of  electrophysiological  experiments  on  movements resulting  from  stimulation of the  frog  spinal cord.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/1f4477bad7af3616c1f933a02bfabe4e-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/1f4477bad7af3616c1f933a02bfabe4e-Bibtex.bib",
            "SUPP": ""
        }
    },
    "21": {
        "TITLE": "Using a Saliency Map for Active Spatial Selective Attention: Implementation & Initial Results",
        "AUTHORS": "Shumeet Baluja, Dean A. Pomerleau",
        "ABSTRACT": "In many vision based tasks, the ability to focus attention on the important  portions of a scene is crucial for good performance on the tasks. In this paper  we present a simple method of achieving spatial selective attention through  the use of a saliency map. The saliency map indicates which regions of the  input retina are important for performing the task. The saliency map is cre(cid:173) ated through predictive auto-encoding. The performance of this method is  demonstrated on two simple tasks which have multiple very strong distract(cid:173) ing features in the input retina. Architectural extensions and application  directions for this model are presented.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/1f50893f80d6830d62765ffad7721742-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/1f50893f80d6830d62765ffad7721742-Bibtex.bib",
            "SUPP": ""
        }
    },
    "22": {
        "TITLE": "Factorial Learning and the EM Algorithm",
        "AUTHORS": "Zoubin Ghahramani",
        "ABSTRACT": "Many real world learning problems are best characterized by an  interaction of multiple independent causes or factors. Discover(cid:173) ing such causal structure from the data is the focus of this paper.  Based on Zemel and Hinton's cooperative vector quantizer (CVQ)  architecture, an unsupervised learning algorithm is derived from  the Expectation-Maximization (EM) framework. Due to the com(cid:173) binatorial nature of the data generation process, the exact E-step  is computationally intractable. Two alternative methods for com(cid:173) puting the E-step are proposed: Gibbs sampling and mean-field  approximation, and some promising empirical results are presented.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/20aee3a5f4643755a79ee5f6a73050ac-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/20aee3a5f4643755a79ee5f6a73050ac-Bibtex.bib",
            "SUPP": ""
        }
    },
    "23": {
        "TITLE": "Pairwise Neural Network Classifiers with Probabilistic Outputs",
        "AUTHORS": "David Price, Stefan Knerr, Léon Personnaz, Gérard Dreyfus",
        "ABSTRACT": "Multi-class  classification  problems  can  be  efficiently  solved  by  partitioning the  original problem into  sub-problems  involving  only  two  classes:  for each pair of classes,  a (potentially small)  neural  network is  trained  using  only  the  data  of  these  two  classes.  We  show  how  to  combine the outputs of the  two-class  neural  networks in order to  obtain  posterior probabilities for  the class decisions.  The resulting probabilistic  pairwise classifier is part of a handwriting recognition  system which  is  currently applied to check reading. We present results on real world data  bases and show that, from a practical point of view, these results compare  favorably to other neural network approaches.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/210f760a89db30aa72ca258a3483cc7f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/210f760a89db30aa72ca258a3483cc7f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "24": {
        "TITLE": "Real-Time Control of a Tokamak Plasma Using Neural Networks",
        "AUTHORS": "Chris M. Bishop, Paul S. Haynes, Mike E U Smith, Tom N. Todd, David L. Trotman, Colin G. Windsor",
        "ABSTRACT": "This  paper  presents  results  from  the  first  use  of neural  networks  for  the real-time feedback  control  of high  temperature plasmas in  a  tokamak fusion  experiment.  The tokamak is  currently  the  prin(cid:173) cipal  experimental  device  for  research  into  the  magnetic  confine(cid:173) ment  approach  to  controlled  fusion.  In  the  tokamak,  hydrogen  plasmas,  at  temperatures  of  up  to  100  Million  K,  are  confined  by  strong  magnetic  fields.  Accurate  control  of the  position  and  shape  of the plasma boundary  requires  real-time feedback  control  of the magnetic field  structure on  a  time-scale of a  few  tens of mi(cid:173) croseconds.  Software simulations have demonstrated that a  neural  network  approach  can  give  significantly  better  performance  than  the linear technique currently  used  on  most tokamak experiments.  The practical  application of the neural  network  approach  requires  high-speed  hardware,  for  which  a  fully  parallel  implementation of  the  multilayer perceptron,  using  a  hybrid  of digital and  analogue  technology,  has been  developed.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/2387337ba1e0b0249ba90f55b2ba2521-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/2387337ba1e0b0249ba90f55b2ba2521-Bibtex.bib",
            "SUPP": ""
        }
    },
    "25": {
        "TITLE": "An Actor/Critic Algorithm that is Equivalent to Q-Learning",
        "AUTHORS": "Robert H. Crites, Andrew G. Barto",
        "ABSTRACT": "We prove the convergence of an actor/critic algorithm that is equiv(cid:173) alent  to Q-Iearning by construction.  Its equivalence is  achieved by  encoding Q-values within  the  policy  and value function  of the  ac(cid:173) tor and critic.  The resultant actor/critic algorithm is novel in two  ways:  it  updates the critic  only  when  the  most  probable  action  is  executed from  any given state, and it  rewards  the actor using cri(cid:173) teria that depend on the relative probability of the action that was  executed.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/23ce1851341ec1fa9e0c259de10bf87c-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/23ce1851341ec1fa9e0c259de10bf87c-Bibtex.bib",
            "SUPP": ""
        }
    },
    "26": {
        "TITLE": "Template-Based Algorithms for Connectionist Rule Extraction",
        "AUTHORS": "Jay A. Alexander, Michael Mozer",
        "ABSTRACT": "Casting  neural  network  weights  in  symbolic  terms  is  crucial  for  interpreting  and explaining  the  behavior of a network.  Additionally,  in  some  domains,  a  symbolic  description  may  lead  to  more  robust  generalization.  We  present  a  principled  approach  to  symbolic  rule  extraction  based  on  the  notion  of  weight  templates,  parameterized  regions of weight space corresponding to specific symbolic expressions.  With  an  appropriate  choice  of representation,  we  show  how  template  parameters  may  be  efficiently  identified  and  instantiated  to  yield  the  optimal match to a unit's actual weights. Depending on the requirements  of  the  application  domain,  our  method  can  accommodate  arbitrary  disjunctions  and  conjunctions  with  O(k)  complexity,  simple  n-of-m  expressions with O( k!) complexity, or a more general class of recursive  n-of-m  expressions  with  O(k!)  complexity,  where  k  is  the  number  of  inputs  to  a  unit.  Our method  of rule  extraction  offers  several  benefits  over alternative approaches in the literature, and simulation results on  a  variety of problems demonstrate its effectiveness.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/24896ee4c6526356cc127852413ea3b4-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/24896ee4c6526356cc127852413ea3b4-Bibtex.bib",
            "SUPP": ""
        }
    },
    "27": {
        "TITLE": "Reinforcement Learning with Soft State Aggregation",
        "AUTHORS": "Satinder P. Singh, Tommi Jaakkola, Michael I. Jordan",
        "ABSTRACT": "It is  widely  accepted  that the use  of more compact representations  than lookup tables is crucial to scaling reinforcement learning (RL)  algorithms to real-world problems.  Unfortunately almost all of the  theory  of reinforcement  learning assumes  lookup table representa(cid:173) tions.  In  this  paper  we  address  the  pressing  issue  of  combining  function  approximation and RL,  and present  1)  a function approx(cid:173) imator  based  on  a  simple extension  to  state  aggregation  (a  com(cid:173) monly  used  form  of  compact  representation),  namely  soft  state  aggregation,  2)  a  theory  of convergence for  RL  with arbitrary, but  fixed,  soft  state  aggregation,  3)  a  novel  intuitive  understanding of  the effect  of state aggregation on online RL, and 4)  a new heuristic  adaptive  state aggregation algorithm that finds  improved compact  representations  by  exploiting the  non-discrete  nature  of soft  state  aggregation.  Preliminary empirical results  are  also  presented.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/287e03db1d99e0ec2edb90d079e142f3-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/287e03db1d99e0ec2edb90d079e142f3-Bibtex.bib",
            "SUPP": ""
        }
    },
    "28": {
        "TITLE": "A Connectionist Technique for Accelerated Textual Input: Letting a Network Do the Typing",
        "AUTHORS": "Dean Pomerleau",
        "ABSTRACT": "Each year people spend a huge amount of time typing. The text people type  typically contains a tremendous amount of redundancy due to predictable  word  usage  patterns  and  the  text's  structure.  This  paper  describes  a  neural network system call AutoTypist that monitors a person's typing and  predicts what will be entered  next.  AutoTypist displays the most likely  subsequent word to the typist, who can accept it with a single keystroke,  instead of typing it in its entirety.  The multi-layer perceptron at the heart  of Auto'JYpist adapts its predictions of likely subsequent text to the user's  word usage pattern,  and to the characteristics of the text currently being  typed.  Increases in typing speed of 2-3% when typing English prose and  10-20% when typing C code have been demonstrated using the system,  suggesting a potential time savings of more than 20 hours per user per year.  In addition to increasing typing speed, AutoTypist reduces the number of  keystrokes a user must type by a similar amount (2-3% for English,  10- 20% for computer programs).  This keystroke savings has the potential to  significantly reduce the frequency  and severity of repeated stress injuries  caused by typing, which are the most common injury suffered in today's  office environment.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/298923c8190045e91288b430794814c4-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/298923c8190045e91288b430794814c4-Bibtex.bib",
            "SUPP": ""
        }
    },
    "29": {
        "TITLE": "Advantage Updating Applied to a Differential Game",
        "AUTHORS": "Mance E. Harmon, Leemon C. Baird III, A. Harry Klopf",
        "ABSTRACT": "An application of reinforcement learning to a linear-quadratic, differential  game  is  presented.  The reinforcement learning  system  uses  a  recently  developed  algorithm,  the residual gradient form  of advantage updating.  The  game  is a  Markov  Decision  Process  (MDP)  with continuous  time,  states, and actions, linear dynamics, and a quadratic cost function.  The  game consists of two players, a missile and a plane;  the missile pursues  the plane and  the plane evades the  missile.  The reinforcement learning  algorithm for optimal control is modified for differential games in order to  find the minimax point, rather than  the maximum.  Simulation results are  compared  to  the  optimal  solution,  demonstrating  that  the  simulated  reinforcement learning  system  converges  to  the optimal  answer.  The  performance of both the residual gradient and non-residual gradient forms  of advantage updating and Q-learning are compared.  The results show that  advantage  updating  converges faster  than  Q-learning in  all  simulations.  The results also show advantage updating converges regardless of the time  step duration; Q-learning is  unable to converge as the  time step duration  ~rows small. \nU.S .A.F.  Academy,  2354  Fairchild  Dr.  Suite 6K4l,  USAFA,  CO  80840-6234 \n354 \nMance E.  Hannon,  Leemon C.  Baird ll/, A.  Harry Klopf \n1  ADVANTAGE  UPDATING \nThe advantage updating algorithm (Baird, 1993) is a reinforcement learning algorithm in  which  two types  of information  are stored.  For each  state x, the  value V(x)  is  stored,  representing an estimate of the total discounted return expected when starting in  state x  and performing optimal actions.  For each state x and action u, the advantage, A(x,u), is  stored, representing  an  estimate of the  degree  to  which  the expected  total  discounted  reinforcement  is  increased  by  performing  action  u  rather  than  the  action  currently  considered best.  The optimal value function V* (x) represents the true value of each state.  The optimal advantage function A * (x,u)  will be zero if u is the optimal action (because u  confers no advantage relative to itself) and A * (x,u) will be negative for any suboptimal u  (because a suboptimal action has a negative advantage relative to  the best action).  The  optimal advantage function A * can be defined in terms of the optimal value function v*: \nA(x,u) = ~[RN(X,U)- V(x)+ rNV*(x')]",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Bibtex.bib",
            "SUPP": ""
        }
    },
    "30": {
        "TITLE": "Pattern Playback in the 90s",
        "AUTHORS": "Malcolm Slaney",
        "ABSTRACT": "Deciding  the  appropriate  representation  to  use  for  modeling  human  auditory processing  is  a critical issue  in  auditory  science. While  engi(cid:173) neers have successfully performed many single-speaker tasks with LPC  and  spectrogram  methods,  more  difficult  problems  will  need  a  richer  representation. This paper describes a powerful auditory representation  known as the correlogram and shows how this non-linear representation  can be converted back into  sound, with no  loss  of perceptually  impor(cid:173) tant information.  The correlogram  is  interesting because  it is  a  neuro(cid:173) physiologically  plausible  representation  of  sound.  This  paper  shows  improved  methods  for  spectrogram  inversion  (conventional  pattern  playback), inversion of a cochlear model, and inversion of the  correlo(cid:173) gram representation.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/2ab56412b1163ee131e1246da0955bd1-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/2ab56412b1163ee131e1246da0955bd1-Bibtex.bib",
            "SUPP": ""
        }
    },
    "31": {
        "TITLE": "An Analog Neural Network Inspired by Fractal Block Coding",
        "AUTHORS": "Fernando J. Pineda, Andreas G. Andreou",
        "ABSTRACT": "We consider the problem of decoding block coded data, using a physical  dynamical system. We sketch out a decompression algorithm for fractal  block  codes  and  then  show  how  to  implement  a  recurrent  neural  network  using  physically  simple  but highly-nonlinear,  analog  circuit  models of neurons and synapses. The nonlinear system has many fixed  points, but we have at our disposal a procedure to choose the parameters  in  such a way  that only one solution, the desired solution, is stable. As  a  partial  proof of the  concept,  we  present experimental  data  from  a  small system a 16-neuron analog CMOS chip fabricated in a 2m analog  p-well process. This chip operates in the subthreshold regime and,  for  each choice of parameters, converges to a unique stable state. Each state  exhibits a qualitatively fractal shape.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/2ba596643cbbbc20318224181fa46b28-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/2ba596643cbbbc20318224181fa46b28-Bibtex.bib",
            "SUPP": ""
        }
    },
    "32": {
        "TITLE": "Phase-Space Learning",
        "AUTHORS": "Fu-Sheng Tsung, Garrison W. Cottrell",
        "ABSTRACT": "Existing recurrent  net learning algorithms are inadequate.  We  in(cid:173) troduce the conceptual framework of viewing recurrent  training as  matching vector fields of dynamical systems in phase space.  Phase(cid:173) space  reconstruction  techniques  make  the  hidden  states  explicit,  reducing  temporal  learning  to  a  feed-forward  problem.  In  short,  we  propose  viewing  iterated  prediction  [LF88]  as  the  best  way  of  training  recurrent  networks  on  deterministic  signals.  Using  this  framework,  we  can  train  multiple trajectories,  insure  their  stabil(cid:173) ity,  and  design  arbitrary dynamical systems.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/2f885d0fbe2e131bfc9d98363e55d1d4-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/2f885d0fbe2e131bfc9d98363e55d1d4-Bibtex.bib",
            "SUPP": ""
        }
    },
    "33": {
        "TITLE": "An experimental comparison of recurrent neural networks",
        "AUTHORS": "Bill G. Horne, C. Lee Giles",
        "ABSTRACT": "Many  different  discrete-time  recurrent  neural  network  architec(cid:173) tures  have  been  proposed.  However,  there  has  been  virtually  no  effort  to compare these arch:tectures experimentally.  In  this paper  we  review  and categorize many of these architectures and compare  how  they  perform on various classes  of simple problems including  grammatical inference  and nonlinear system identification.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/31b3b31a1c2f8a370206f111127c0dbd-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/31b3b31a1c2f8a370206f111127c0dbd-Bibtex.bib",
            "SUPP": ""
        }
    },
    "34": {
        "TITLE": "Inferring Ground Truth from Subjective Labelling of Venus Images",
        "AUTHORS": "Padhraic Smyth, Usama M. Fayyad, Michael C. Burl, Pietro Perona, Pierre Baldi",
        "ABSTRACT": "In  remote sensing  applications  \"ground-truth\"  data is  often  used  as  the  basis for  training  pattern  recognition  algorithms  to  gener(cid:173) ate  thematic  maps  or  to  detect  objects  of  interest.  In  practical  situations, experts may visually examine the images and provide a  subjective noisy  estimate of the  truth.  Calibrating the  reliability  and bias  of expert labellers is  a  non-trivial problem.  In this paper  we  discuss  some  of  our  recent  work  on  this  topic  in  the  context  of  detecting  small  volcanoes  in  Magellan  SAR  images  of  Venus.  Empirical results (using the Expectation-Maximization procedure)  suggest  that  accounting  for  subjective  noise  can  be  quite  signifi(cid:173) cant in terms of quantifying both human and algorithm detection  performance.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/3cef96dcc9b8035d23f69e30bb19218a-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/3cef96dcc9b8035d23f69e30bb19218a-Bibtex.bib",
            "SUPP": ""
        }
    },
    "35": {
        "TITLE": "Learning Prototype Models for Tangent Distance",
        "AUTHORS": "Trevor Hastie, Patrice Simard",
        "ABSTRACT": "Simard,  LeCun  & Denker  (1993)  showed  that the performance of  nearest-neighbor  classification  schemes  for  handwritten  character  recognition  can  be  improved  by  incorporating  invariance  to  spe(cid:173) the  so  cific  transformations in  the  underlying  distance  metric  - called  tangent  distance.  The  resulting  classifier,  however,  can  be  prohibitively slow and memory intensive due to the large amount of  prototypes that need to be stored and used in the distance compar(cid:173) isons.  In this  paper we  develop  rich  models for  representing  large  subsets of the prototypes.  These models are either used singly per  class,  or  as  basic building blocks  in conjunction with the K-means  clustering  algorithm. \n*This work was performed while Trevor Hastie was a member of the Statistics and Data \nAnalysis  Research  Group,  AT&T Bell  Laboratories,  Murray  Hill,  NJ  07974.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/3df1d4b96d8976ff5986393e8767f5b2-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/3df1d4b96d8976ff5986393e8767f5b2-Bibtex.bib",
            "SUPP": ""
        }
    },
    "36": {
        "TITLE": "Diffusion of Credit in Markovian Models",
        "AUTHORS": "Yoshua Bengio, Paolo Frasconi",
        "ABSTRACT": "This  paper  studies the  problem of diffusion  in  Markovian  models,  such  as  hidden  Markov  models  (HMMs)  and  how  it  makes  very  difficult the task of learning of long-term dependencies in sequences.  Using results from Markov chain theory,  we show that the problem  of diffusion is reduced if the transition probabilities approach 0 or 1.  Under  this condition, standard HMMs have very  limited modeling  capabilities,  but input/output HMMs can still perform interesting  computations.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/3e89ebdb49f712c7d90d1b39e348bbbf-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/3e89ebdb49f712c7d90d1b39e348bbbf-Bibtex.bib",
            "SUPP": ""
        }
    },
    "37": {
        "TITLE": "The Ni1000: High Speed Parallel VLSI for Implementing Multilayer Perceptrons",
        "AUTHORS": "Michael P. Perrone, Leon N. Cooper",
        "ABSTRACT": "In  this paper  we  present  a  new  version  of the standard  multilayer  perceptron  (MLP) algorithm for  the state-of-the-art in neural net(cid:173) work VLSI implementations:  the Intel Ni1000.  This new version of  the  MLP  uses  a fundamental property of high dimensional spaces  which  allows  the  12-norm  to  be  accurately  approximated  by  the  It -norm.  This  approach  enables  the  standard  MLP  to  utilize  the  parallel architecture of the Ni1000 to achieve on the order of 40000,  256-dimensional classifications per second. \n1  The Intel NilOOO  VLSI Chip \nThe Nestor/Intel radial basis function  neural chip (Ni1000) contains the equivalent  of  1024  256-dimensional  artificial  digital  neurons  and can  perform  at  least  40000  classifications  per  second  [Sullivan, 1993].  To attain  this  great  speed,  the  Ni1000  was  designed  to  calculate  \"city  block\"  distances  (Le.  the  II-norm)  and  thus  to  avoid the large  number  of multiplication units that  would be required  to calculate  Euclidean dot  products in  parallel.  Each  neuron calculates the city block distance  between  its stored weights and the current  input: \nneuron  activity = L IWi  - :eil \n(1)  where  w,  is  the  neuron's stored  weight  for  the  ith  input  and  :ei  is  the  ith  input.  Thus the Nil000 is ideally suited to perform both the RCE [Reillyet al.,  1982]  and \n748 \nMichael P.  Perrone.  Leon N.  Cooper \nPRCE  [Scofield et al.,  1987]  algorithms or  any of the  other  commonly used  radial  basis function  (RBF) algorithms.  However,  dot products are central in the calcula(cid:173) tions performed by most neural network algorithms (e.g.  MLP, Cascade Correlation,  etc.).  Furthermore, for  high dimensional data, the dot product becomes the compu(cid:173) tation bottleneck (i.e.  most ofthe network's time is spent calculating dot products).  If the  dot  product  can  not  be  performed in  parallel  there  will  be  little  advantage  using  the  NilOOO  for  such  algorithms.  In  this  paper,  we  address  this  problem  by  showing  that  we  can  extend  the  NilOOO  to  many of the  standard  neural  network  algorithms by  representing  the  Euclidean  dot  product  as  a  function  of Euclidean  norms and by then using a  city block norm approximation to the  Euclidean norm.  Section  2,  introduces  the  approximate dot  productj  Section  3  describes  the  City  Block  MLP  which  uses  the  approximate dot  productj  and  Section  4  presents  ex(cid:173) periments which demonstrate that the City Block  MLP performs well on the  NIST  OCR data and on human face  recognition data. \n2  Approximate  Dot  Product \nConsider  the following approximation [Perrone,  1993]: \n1",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/430c3626b879b4005d41b8a46172e0c0-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/430c3626b879b4005d41b8a46172e0c0-Bibtex.bib",
            "SUPP": ""
        }
    },
    "38": {
        "TITLE": "Model of a Biological Neuron as a Temporal Neural Network",
        "AUTHORS": "Sean D. Murphy, Edward W. Kairiss",
        "ABSTRACT": "A biological neuron can be viewed as a device that maps a multidimen(cid:173) sional  temporal  event signal  (dendritic  postsynaptic  activations)  into a  unidimensional  temporal  event  signal  (action  potentials).  We  have  designed  a  network,  the  Spatio-Temporal  Event  Mapping  (STEM)  architecture, which can learn to perform this mapping for arbitrary bio(cid:173) physical  models  of  neurons.  Such  a  network  appropriately  trained,  called a STEM cell, can be used in place of a conventional compartmen(cid:173) tal  model  in  simulations  where only the  transfer function  is  important,  such  as  network  simulations.  The  STEM  cell  offers  advantages  over  compartmental  models in  terms of computational efficiency, analytical  tractabili1ty,  and as a framework  for  VLSI  implementations of biologi(cid:173) cal neurons.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/4311359ed4969e8401880e3c1836fbe1-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/4311359ed4969e8401880e3c1836fbe1-Bibtex.bib",
            "SUPP": ""
        }
    },
    "39": {
        "TITLE": "Non-linear Prediction of Acoustic Vectors Using Hierarchical Mixtures of Experts",
        "AUTHORS": "Steve R. Waterhouse, Anthony J. Robinson",
        "ABSTRACT": "In this paper we consider speech coding as a problem of speech  modelling. In particular, prediction of parameterised speech over  short time segments is performed using the Hierarchical Mixture of  Experts (HME) (Jordan & Jacobs 1994). The HME gives two ad(cid:173) vantages over traditional non-linear function approximators such  as the Multi-Layer Percept ron (MLP); a statistical understand(cid:173) ing of the operation of the predictor and provision of information  about the performance of the predictor in the form of likelihood  information and local error bars. These two issues are examined  on both toy and real world problems of regression and time series  prediction. In the speech coding context, we extend the principle  of combining local predictions via the HME to a Vector Quantiza(cid:173) tion scheme in which fixed local codebooks are combined on-line  for each observation.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/437d7d1d97917cd627a34a6a0fb41136-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/437d7d1d97917cd627a34a6a0fb41136-Bibtex.bib",
            "SUPP": ""
        }
    },
    "40": {
        "TITLE": "JPMAX: Learning to Recognize Moving Objects as a Model-fitting Problem",
        "AUTHORS": "Suzanna Becker",
        "ABSTRACT": "Unsupervised learning procedures have been successful at low-level  feature  extraction  and  preprocessing of raw  sensor  data.  So  far,  however,  they  have  had  limited  success  in  learning  higher-order  representations, e.g.,  of objects in  visual images.  A promising ap(cid:173) proach  is  to  maximize  some  measure  of  agreement  between  the  outputs of two groups of units which receive inputs physically sep(cid:173) arated in  space, time or modality,  as  in  (Becker and Hinton,  1992;  Becker, 1993; de Sa,  1993).  Using the same approach, a much sim(cid:173) pler  learning  procedure is  proposed  here  which  discovers  features  in a single-layer network consisting of several populations of units,  and  can  be  applied  to  multi-layer  networks  trained  one  layer  at  a  time.  When  trained with  this  algorithm  on  image  sequences  of  moving geometric objects a two-layer network can learn to perform  accurate position-invariant object classification. \n1  LEARNING  COHERENT CLASSIFICATIONS \nA powerful constraint in  sensory data is  coherence over time,  in  space,  and  across  different sensory modalities.  An  unsupervised learning procedure which can capital(cid:173) ize on these constraints may be able to explain much of perceptual self-organization  in  the mammalian brain.  The problem is to derive an appropriate cost function for  unsupervised  learning which  will  capture coherence constraints in  sensory  signals;  we  would  also  like  it  to  be  applicable  to  multi-layer  nets  to  train  hidden  as  well  as  output  layers.  Our  ultimate  goal  is  for  the  network  to  discover  natural object  classes  based on these coherence assumptions. \n934",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/4b0250793549726d5c1ea3906726ebfe-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/4b0250793549726d5c1ea3906726ebfe-Bibtex.bib",
            "SUPP": ""
        }
    },
    "41": {
        "TITLE": "The Electrotonic Transformation: a Tool for Relating Neuronal Form to Function",
        "AUTHORS": "Nicholas T Carnevale, Kenneth Y. Tsai, Brenda J. Claiborne, Thomas H. Brown",
        "ABSTRACT": "The spatial distribution and time course of electrical signals in neurons  have important theoretical and practical consequences. Because it is  difficult to infer how neuronal form affects electrical signaling, we  have developed a quantitative yet intuitive approach to the analysis of  electrotonus. This approach transforms the architecture of the cell  from anatomical to electrotonic space, using the logarithm of voltage  attenuation as the distance metric. We describe the theory behind this  approach and illustrate its use.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/4b6538a44a1dfdc2b83477cd76dee98e-Bibtex.bib",
            "SUPP": ""
        }
    },
    "42": {
        "TITLE": "Dynamic Modelling of Chaotic Time Series with Neural Networks",
        "AUTHORS": "Jose C. Principe, Jyh-Ming Kuo",
        "ABSTRACT": "The auditory system of the barn owl contains several spatial maps.  In young barn owls raised with optical prisms over their eyes, these  auditory maps are shifted to stay in register with the visual map,  suggesting that the visual input imposes a frame of reference on  the auditory maps. However, the optic tectum, the first site of  convergence of visual with auditory information, is not the site of  plasticity for the shift of the auditory maps; the plasticity occurs  instead in the inferior colliculus, which contains an auditory map  and projects into the optic tectum. We explored a model of the owl  remapping in which a global reinforcement signal whose delivery is  controlled by visual foveation. A hebb learning rule gated by rein(cid:173) forcement learned to appropriately adjust auditory maps. In addi(cid:173) tion, reinforcement learning preferentially adjusted the weights in  the inferior colliculus, as in the owl brain, even though the weights  were allowed to change throughout the auditory system. This ob(cid:173) servation raises the possibility that the site of learning does not  have to be genetically specified, but could be determined by how  the learning procedure interacts with the network architecture. \n126 \nAlexandre Pouget, Cedric Deffayet, Te\"ence J. Sejnowski \nc:::======:::::» •",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/4daa3db355ef2b0e64b472968cb70f0d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/4daa3db355ef2b0e64b472968cb70f0d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "43": {
        "TITLE": "Boltzmann Chains and Hidden Markov Models",
        "AUTHORS": "Lawrence K. Saul, Michael I. Jordan",
        "ABSTRACT": "We  propose  a  statistical  mechanical  framework  for  the  modeling  of discrete  time series.  Maximum likelihood estimation is  done via  Boltzmann learning in one-dimensional networks with tied weights.  We  call  these  networks  Boltzmann  chains  and  show  that  they  contain  hidden  Markov  models  (HMMs)  as  a  special  case.  Our  framework  also  motivates  new  architectures  that  address  partic(cid:173) ular  shortcomings of HMMs.  We  look  at two  such  architectures:  parallel  chains  that model feature  sets  with  disparate  time scales,  and  looped  networks  that  model long-term  dependencies  between  hidden  states.  For  these  networks,  we  show  how  to  implement  the  Boltzmann learning rule  exactly,  in  polynomial time,  without  resort  to simulated or  mean-field  annealing.  The  necessary  com(cid:173) putations are done by exact decimation procedures from statistical  mechanics. \n1 \nINTRODUCTION  AND  SUMMARY \nStatistical  models  of discrete  time series  have  a  wide  range  of applications,  most  notably to problems in speech  recognition  (Juang &  Rabiner,  1991)  and molecular  biology  (Baldi,  Chauvin,  Hunkapiller,  &  McClure,  1992).  A  common problem in  these  fields  is  to find  a  probabilistic  model,  and  a  set  of model  parameters,  that \n436 \nLawrence K.  Saul,  Michael I.  Jordan \naccount for sequences  of observed data.  Hidden  Markov models (HMMs)  have been  particularly successful  at  modeling discrete  time series.  One  reason  for  this  is  the  powerful learning rule (Baum) 1972») a special case of the Expectation-Maximization  (EM)  procedure  for  maximum likelihood  estimation  (Dempster)  Laird)  &  Rubin)  1977).  In  this  work)  we  develop  a  statistical  mechanical  framework  for  the  modeling  of  discrete  time series.  The framework  enables  us  to  relate  HMMs  to  a  large family  of exactly  solvable  models  in  statistical  mechanics.  The  connection  to  statistical  mechanics  was  first  noticed  by  Sourlas  (1989»)  who  studied  spin  glass  models  of  error-correcting  codes.  We  view  the  estimation procedure  for  HMMs  as  a  special  (and particularly tractable) case of the Boltzmann learning rule  (Ackley)  Hinton)  &  Sejnowski)  1985;  Byrne)  1992).  The rest of this paper is  organized as follows .  In Section 2)  we  review  the modeling  problem for  discrete  time series  and  establish  the  connection  between  HMMs  and  Boltzmann  machines.  In  Section  3)  we  show  how  to  quickly  determine  whether  or  not  a  particular  Boltzmann  machine  is  tractable)  and  if so)  how  to  efficiently  compute  the  correlations  in  the  Boltzmann  learning  rule.  Finally)  in  Section  4)  we  look  at  two  architectures  that  address  particular  weaknesses  of  HMMs:  the  modelling of disparate  time scales  and long-term dependencies. \n2  MODELING DISCRETE TIME SERIES \nA discrete  time series is  a sequence of symbols {jdr=l in which each symbol belongs  to a finite countable set)  i.e.  jl E {1) 2)  .. . ) m}.  Given one long sequence)  or perhaps  many shorter ones)  the  modeling task is  to characterize  the probability distribution  from which  the time series  are generated. \n2.1  HIDDEN  MARKOV MODELS \nA  first-order  Hidden  Markov  Model  (HMM)  is  characterized  by  a  set  of n  hidden  states)  an  alphabet  of m  symbols)  a  transmission  matrix ajj')  an  emission  matrix  bjj )  and a prior distribution 7I'j  over  the initial hidden state.  The sequence  of states  {idr=l  and symbols {jdr=l is  modeled to occur  with probability \n(1) \nThe  modeling problem is  to find  the  parameter  values  (ajj' , bij ) 7I'j)  that maximize  the  likelihood  of observed  sequences  of training  data.  We  will  elaborate  on  the  learning  rule  in  section  2.3)  but first  let  us  make  the  connection  to  a  well-known  family of stochastic  neural networks, namely Boltzmann machines. \n2.2  BOLTZMANN MACHINES \nConsider a Boltzmann machine with m-state visible units) n-state hidden units) tied  weights)  and the linear architecture shown in Figure 1.  This example represents  the  simplest  possible  Boltzmann  \"chain)))  one  that  is  essentially  equivalent  to  a  first(cid:173) order  HMM  unfolded in time (MacKay)  1994).  The transition weights Aii'  connect  adjacent  hidden units)  while  the  emission weights  Bjj  connect  each  hidden  unit to \nBoltzmann  Chains and Hidden  Markov  Models",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/4e0cb6fb5fb446d1c92ede2ed8780188-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/4e0cb6fb5fb446d1c92ede2ed8780188-Bibtex.bib",
            "SUPP": ""
        }
    },
    "44": {
        "TITLE": "Learning with Product Units",
        "AUTHORS": "Laurens R. Leerink, C. Lee Giles, Bill G. Horne, Marwan A. Jabri",
        "ABSTRACT": "The  TNM  staging  system  has  been  used  since  the  early  1960's  to  predict  breast  cancer  patient  outcome.  In  an  attempt  to  in(cid:173) crease  prognostic  accuracy,  many putative prognostic factors  have  been  identified.  Because  the  TNM  stage  model  can  not  accom(cid:173) modate  these  new  factors,  the  proliferation  of factors  in  breast  cancer  has  lead  to  clinical  confusion.  What  is  required  is  a  new  computerized  prognostic system  that  can test  putative prognostic  factors  and  integrate  the  predictive  factors  with  the  TNM  vari(cid:173) ables  in order to increase  prognostic  accuracy.  Using  the area un(cid:173) der  the curve of the receiver  operating characteristic,  we  compare  the  accuracy  of the  following  predictive  models  in  terms  of five  year  breast cancer-specific  survival:  pTNM staging system,  princi(cid:173) pal component analysis,  classification and regression  trees,  logistic  regression,  cascade  correlation neural network,  conjugate gradient  descent  neural,  probabilistic neural network,  and backpropagation  neural network.  Several statistical models are significantly more ac-",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/4fac9ba115140ac4f1c22da82aa0bc7f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/4fac9ba115140ac4f1c22da82aa0bc7f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "45": {
        "TITLE": "Efficient Methods for Dealing with Missing Data in Supervised Learning",
        "AUTHORS": "Volker Tresp, Ralph Neuneier, Subutai Ahmad",
        "ABSTRACT": "We present efficient algorithms for dealing with the problem of mis(cid:173) sing inputs (incomplete feature  vectors) during training and recall.  Our approach is based on the approximation of the input data dis(cid:173) tribution using  Parzen  windows.  For  recall,  we  obtain closed form  solutions for arbitrary feedforward networks.  For training, we show  how  the  backpropagation  step  for  an  incomplete  pattern  can  be  approximated by  a  weighted  averaged backpropagation step.  The  complexity  of the  solutions for  training  and  recall  is  independent  of the number of missing features.  We verify our theoretical results  using one classification and one regression  problem.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/54a367d629152b720749e187b3eaa11b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/54a367d629152b720749e187b3eaa11b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "46": {
        "TITLE": "Predictive Coding with Neural Nets: Application to Text Compression",
        "AUTHORS": "Jürgen Schmidhuber, Stefan Heil",
        "ABSTRACT": "To compress text files,  a neural predictor network  P  is used to ap(cid:173) proximate the conditional probability distribution of possible  \"next  characters\",  given  n  previous  characters.  P's outputs are fed  into  standard coding algorithms that generate short codes for characters  with  high  predicted  probability and  long  codes  for  highly  unpre(cid:173) dictable  characters.  Tested  on  short  German  newspaper  articles,  our method outperforms widely used  Lempel-Ziv algorithms (used  in  UNIX  functions such  as  \"compress\"  and  \"gzip\").",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/5705e1164a8394aace6018e27d20d237-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/5705e1164a8394aace6018e27d20d237-Bibtex.bib",
            "SUPP": ""
        }
    },
    "47": {
        "TITLE": "Computational Structure of coordinate transformations: A generalization study",
        "AUTHORS": "Zoubin Ghahramani, Daniel M. Wolpert, Michael I. Jordan",
        "ABSTRACT": "One  of the fundamental properties  that both neural  networks  and  the  central  nervous  system share is  the  ability to learn  and gener(cid:173) alize  from examples.  While this  property  has  been  studied  exten(cid:173) sively  in  the  neural  network  literature  it  has  not  been  thoroughly  explored in human perceptual and motor learning.  We have chosen  a  coordinate  transformation  system-the  visuomotor  map  which  transforms visual coordinates into motor coordinates-to study the  generalization effects  of learning new  input-output  pairs.  Using  a  paradigm of computer  controlled  altered  visual  feedback,  we  have  studied  the  generalization  of the  visuomotor  map  subsequent  to  both local  and context-dependent  remappings.  A  local  remapping  of one  or  two  input-output  pairs  induced  a  significant  global,  yet  decaying,  change  in  the  visuomotor map, suggesting  a  representa(cid:173) tion for  the  map composed of units  with large functional  receptive  fields.  Our study of context-dependent  remappings indicated that  a  single  point  in  visual  space  can  be  mapped to  two different  fin(cid:173) ger  locations  depending  on  a  context  variable-the starting  point  of  the  movement.  Furthermore,  as  the  context  is  varied  there  is  a  gradual  shift  between  the  two  remappings,  consistent  with  two  visuomotor  modules  being  learned  and  gated  smoothly  with  the  context.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Bibtex.bib",
            "SUPP": ""
        }
    },
    "48": {
        "TITLE": "Recognizing Handwritten Digits Using Mixtures of Linear Models",
        "AUTHORS": "Geoffrey E. Hinton, Michael Revow, Peter Dayan",
        "ABSTRACT": "We construct a mixture of locally linear generative models of a col(cid:173) lection of pixel-based images of digits, and use them for  recogni(cid:173) tion. Different models of a given digit are used to capture different  styles of writing, and new images are classified by evaluating their  log-likelihoods under each model. We use an EM-based algorithm  in which the M-step is computationally straightforward principal  components analysis (PCA).  Incorporating tangent-plane informa(cid:173) tion [12]  about expected local deformations only requires adding  tangent vectors into the sample covariance matrices for  the PCA,  and it demonstrably improves performance.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/5c936263f3428a40227908d5a3847c0b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/5c936263f3428a40227908d5a3847c0b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "49": {
        "TITLE": "A Critical Comparison of Models for Orientation and Ocular Dominance Columns in the Striate Cortex",
        "AUTHORS": "E. Erwin, K. Obermayer, K. Schulten",
        "ABSTRACT": "More than ten of the most prominent models for the structure  and for the activity dependent formation of orientation and ocu(cid:173) lar dominance columns in the striate cort(>x have been evaluated.  We implemented those models on parallel machines, we extensively  explored parameter space, and we quantitatively compared model  predictions with experimental data which were recorded optically  from macaque striate cortex.  In our contribution we present a summary of our results to date.  Briefly, we find that (i) despite apparent differences, many models  are based on similar principles and, consequently, make similar pre(cid:173) dictions, (ii) certain \"pattern models\" as well as the developmental  \"correlation-based learning\" models disagree with the experimen(cid:173) tal data, and (iii) of the models we have investigated, \"competitive  Hebbian\" models and the recent model of Swindale provide the  best match with experimental data. \n1 Models and Data \nThe models for the formation and structure of orientation and ocular dominance  columns which we have investigated are summarized in table 1. Models fall into  two categories: \"Pattern models\" whose aim is to achieve a concise description of  the observed patterns and \"developmental models\" which are focussed on the pro-\n94 \nE. Erwin, K. Obermayer, K. Schulten \nClass  Pattern  Models Models",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/5d616dd38211ebb5d6ec52986674b6e4-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/5d616dd38211ebb5d6ec52986674b6e4-Bibtex.bib",
            "SUPP": ""
        }
    },
    "50": {
        "TITLE": "Classifying with Gaussian Mixtures and Clusters",
        "AUTHORS": "Nanda Kambhatla, Todd K. Leen",
        "ABSTRACT": "In this paper, we derive classifiers which are winner-take-all (WTA)  approximations  to  a  Bayes  classifier  with  Gaussian  mixtures  for  class conditional densities.  The derived classifiers include clustering  based algorithms like LVQ  and k-Means.  We propose a  constrained  rank  Gaussian  mixtures model and derive a WTA algorithm for  it.  Our experiments with two speech classification tasks indicate that  the constrained rank model and the WTA approximations improve  the performance over the unconstrained models.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/621461af90cadfdaf0e8d4cc25129f91-Bibtex.bib",
            "SUPP": ""
        }
    },
    "51": {
        "TITLE": "Anatomical origin and computational role of diversity in the response properties of cortical neurons",
        "AUTHORS": "Kalanit Grill Spector, Shimon Edelman, Rafael Malach",
        "ABSTRACT": "The  maximization of  diversity  of neuronal  response  properties  has  been  recently  suggested  as  an  organizing  principle  for  the  formation  of such  prominent features of the functional architecture of the brain as the corti(cid:173) cal columns and the associated patchy projection patterns (Malach,  1994).  We show that (1) maximal diversity is attained when the ratio of dendritic  and  axonal  arbor  sizes  is  equal  to  one,  as  found  in  many  cortical  areas  and  across  species  (Lund  et al.,  1993;  Malach,  1994),  and  (2)  that maxi(cid:173) mization of diversity leads  to  better  performance  in  systems of receptive  fields  implementing steerable/shiftable  filters,  and  in  matching spatially  distributed  signals,  a  problem that  arises  in  many high-level  visual tasks. \n1  Anatomical substrate for  sampling diversity \nA  fundamental  feature  of  cortical  architecture  is  its  columnar  organization,  mani(cid:173) fested  in  the tendency  of neurons  with similar properties to be organized  in  columns  that run perpendicular to the cortical surface.  This organization of the cortex was ini(cid:173) tially discovered  by  physiological experiments  (Mouncastle,  1957;  Hubel  and  Wiesel,  1962),  and  subsequently  confirmed  with  the  demonstration  of histologically  defined  columns.  Tracing  experiments  have  shown  that  axonal  projections  throughout  the  cerebral  cortex tend to be organized in  vertically  aligned  clusters  or patches.  In  par(cid:173) ticular,  intrinsic  horizontal  connections  linking neighboring  cortical sites,  which  may  extend  up  to  2 - 3  mm,  have  a  striking tendency  to  arborize  selectively  in preferred  sites,  forming  distinct  axonal patches  200 - 300  J.lm  in  diameter. \nRecently,  it  has  been  observed  that  the  size  of  these  patches  matches  closely  the  average  diameter  of  individual  dendritic  arbors  of  upper-layer  pyramidal  cells \n118 \nKalanit Grill Spector,  Shimon  Edelman, Rafael Malach",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/6395ebd0f4b478145ecfbaf939454fa4-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/6395ebd0f4b478145ecfbaf939454fa4-Bibtex.bib",
            "SUPP": ""
        }
    },
    "52": {
        "TITLE": "Synchrony and Desynchrony in Neural Oscillator Networks",
        "AUTHORS": "Deliang Wang, David Terman",
        "ABSTRACT": "An novel class of locally excitatory, globally inhibitory oscillator  networks is proposed. The model of each oscillator corresponds to a  standard relaxation oscillator with two time scales. The network  exhibits a mechanism of selective gating, whereby an oscillator  jumping up to its active phase rapidly recruits the oscillators stimulated  by the same pattern, while preventing others from jumping up. We  show analytically that with the selective gating mechanism the network  rapidly achieves both synchronization within blocks of oscillators that  are stimulated by connected regions and desynchronization between  different blocks. Computer simulations demonstrate the network's  promising ability for segmenting multiple input patterns in real time.  This model lays a physical foundation for the oscillatory correlation  theory of feature binding, and may provide an effective computational  framework for scene segmentation and figure/ground segregation.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/64223ccf70bbb65a3a4aceac37e21016-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/64223ccf70bbb65a3a4aceac37e21016-Bibtex.bib",
            "SUPP": ""
        }
    },
    "53": {
        "TITLE": "A Computational Model of Prefrontal Cortex Function",
        "AUTHORS": "Todd S. Braver, Jonathan D. Cohen, David Servan-Schreiber",
        "ABSTRACT": "Accumulating  data  from  neurophysiology  and  neuropsychology  have  suggested  two  information processing roles  for  prefrontal cor(cid:173) tex  (PFC):  1)  short-term  active  memory;  and  2)  inhibition.  We  present  a  new  behavioral  task  and  a  computational model  which  were  developed  in  parallel.  The  task  was  developed  to probe  both  of these  prefrontal  functions  simultaneously,  and  produces  a  rich  set  of behavioral  data  that  act  as  constraints  on  the  model.  The  model is  implemented in continuous-time, thus providing a  natural  framework  in  which  to study  the  temporal dynamics of processing  in the task.  We show how the model can be used to examine the be(cid:173) havioral  consequences of neuromodulation in PFC . Specifically, we  use  the model to make novel and testable predictions regarding the  behavioral performance of schizophrenics,  who  are hypothesized  to  suffer  from  reduced  dopaminergic tone  in  this brain  area.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/65cc2c8205a05d7379fa3a6386f710e1-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/65cc2c8205a05d7379fa3a6386f710e1-Bibtex.bib",
            "SUPP": ""
        }
    },
    "54": {
        "TITLE": "Combining Estimators Using Non-Constant Weighting Functions",
        "AUTHORS": "Volker Tresp, Michiaki Taniguchi",
        "ABSTRACT": "This paper  discusses  the  linearly  weighted combination of estima(cid:173) tors  in which  the weighting functions  are  dependent  on the input .  We  show  that  the  weighting  functions  can  be  derived  either  by  evaluating  the  input  dependent  variance  of each  estimator  or  by  estimating how  likely  it is  that a given  estimator has  seen  data in  the  region  of the input space  close  to the input pattern.  The lat(cid:173) ter  solution  is  closely  related  to  the  mixture of experts  approach  and we  show how  learning rules  for  the mixture of experts  can  be  derived from the theory  about learning with missing features.  The  presented  approaches  are  modular  since  the  weighting  functions  can  easily  be  modified  (no  retraining)  if more  estimators  are  ad(cid:173) ded.  Furthermore, it is  easy  to incorporate estimators which  were  not  derived from  data such as  expert systems or  algorithms.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/6602294be910b1e3c4571bd98c4d5484-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/6602294be910b1e3c4571bd98c4d5484-Bibtex.bib",
            "SUPP": ""
        }
    },
    "55": {
        "TITLE": "Stochastic Dynamics of Three-State Neural Networks",
        "AUTHORS": "Toru Ohira, Jack D. Cowan",
        "ABSTRACT": "We  present  here  an  analysis  of  the  stochastic  neurodynamics  of  a  neural  network  composed  of  three-state  neurons  described  by  a  master equation.  An  outer-product representation of the mas(cid:173) ter  equation  is  employed.  In  this  representation,  an  extension  of  the  analysis  from  two  to  three-state neurons is  easily  performed.  We  apply  this  formalism  with  approximation  schemes  to  a  sim(cid:173) ple three-state network and compare the results with Monte Carlo  simulations.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/692f93be8c7a41525c0baf2076aecfb4-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/692f93be8c7a41525c0baf2076aecfb4-Bibtex.bib",
            "SUPP": ""
        }
    },
    "56": {
        "TITLE": "On the Computational Utility of Consciousness",
        "AUTHORS": "Donald W. Mathis, Michael Mozer",
        "ABSTRACT": "We  propose  a  computational  framework  for  understanding  and  modeling  human consciousness.  This  framework  integrates  many  existing theoretical perspectives,  yet is sufficiently concrete to allow  simulation experiments.  We do not attempt to explain qualia (sub(cid:173) jective  experience),  but  instead  ask  what  differences  exist  within  the cognitive information processing  system  when a  person is  con(cid:173) scious  of mentally-represented  information versus  when that infor(cid:173) mation is  unconscious.  The central idea we explore is  that the con(cid:173) tents  of consciousness  correspond  to  temporally  persistent  states  in a  network of computational modules.  Three simulations are de(cid:173) scribed  illustrating  that  the  behavior  of  persistent  states  in  the  models  corresponds  roughly  to  the  behavior  of  conscious  states  people  experience when  performing similar tasks.  Our simulations  show that periodic  settling to persistent  (i.e.,  conscious)  states im(cid:173) proves  performance  by cleaning  up inaccuracies  and noise,  forcing  decisions,  and helping  keep the system on track  toward a  solution.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/6aab1270668d8cac7cef2566a1c5f569-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/6aab1270668d8cac7cef2566a1c5f569-Bibtex.bib",
            "SUPP": ""
        }
    },
    "57": {
        "TITLE": "Ocular Dominance and Patterned Lateral Connections in a Self-Organizing Model of the Primary Visual Cortex",
        "AUTHORS": "Joseph Sirosh, Risto Miikkulainen",
        "ABSTRACT": "A neural network model for the self-organization of ocular dominance and  lateral connections from binocular input is presented.  The self-organizing  process results in a network where (1) afferent weights of each neuron or(cid:173) ganize into smooth hill-shaped receptive fields primarily on one of the reti(cid:173) nas, (2) neurons with common eye preference form connected, intertwined  patches, and (3) lateral connections primarily link regions of the same eye  preference.  Similar self-organization of cortical  structures has  been  ob(cid:173) served  experimentally in strabismic kittens.  The model  shows how  pat(cid:173) terned lateral connections in  the cortex may  develop based on correlated  activity and explains why lateral connection patterns follow receptive field  properties such as ocular dominance.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/6b180037abbebea991d8b1232f8a8ca9-Bibtex.bib",
            "SUPP": ""
        }
    },
    "58": {
        "TITLE": "Effects of Noise on Convergence and Generalization in Recurrent Networks",
        "AUTHORS": "Kam Jim, Bill G. Horne, C. Lee Giles",
        "ABSTRACT": "We introduce and study methods of inserting synaptic noise into  dynamically-driven recurrent neural networks and show that ap(cid:173) plying a controlled amount of noise during training may improve  convergence and generalization. In addition, we analyze the effects  of each noise parameter (additive vs. multiplicative, cumulative vs.  non-cumulative, per time step vs. per string) and predict that best  overall performance can be achieved by injecting additive noise at  each time step. Extensive simulations on learning the dual parity  grammar from temporal strings substantiate these predictions.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/6c3cf77d52820cd0fe646d38bc2145ca-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/6c3cf77d52820cd0fe646d38bc2145ca-Bibtex.bib",
            "SUPP": ""
        }
    },
    "59": {
        "TITLE": "An Integrated Architecture of Adaptive Neural Network Control for Dynamic Systems",
        "AUTHORS": "Ke Liu, Robert L. Tokar, Brain D. McVey",
        "ABSTRACT": "In  this  study,  an  integrated  neural  network  control  architecture  for  nonlinear  dynamic  systems  is  presented.  Most of the recent emphasis in  the neural network control field has no error feedback as the  control  input,  which  rises  the  lack  of adaptation  problem.  The  integrated  architecture  in  this  paper  combines  feed  forward  control  and error  feedback  adaptive  control  using  neural  networks.  The  paper  reveals the  different  internal  functionality  of these  two  kinds of neural  network  controllers  for  certain  input  styles,  e.g.,  state  feedback  and error feedback.  With  error  feedback,  neural  network  controllers  learn  the  slopes  or  the  gains  with  respect  to  the  error  feedback,  producing  an  error  driven  adaptive  control  systems.  The  results  demonstrate  that  the  two  kinds  of control  scheme  can  be  combined  to  realize  their individual  advantages.  Testing  with disturbances added to  the  plant  shows  good  tracking  and adaptation with the integrated neural control architecture.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/6cfe0e6127fa25df2a0ef2ae1067d915-Bibtex.bib",
            "SUPP": ""
        }
    },
    "60": {
        "TITLE": "Implementation of Neural Hardware with the Neural VLSI of URAN in Applications with Reduced Representations",
        "AUTHORS": "Il Song Han, Ki-Chul Kim, Hwang-Soo Lee",
        "ABSTRACT": "implement  Korean \nThis  paper  describes  a  way  of  neural  hardware  implementation  with  the  analog-digital  mixed  mode  neural  chip.  The  full  custom  neural  VLSI  of  Universally  Reconstructible  Artificial  Neural  network (URAN)  is  used  system.  A  to  multi-layer  perceptron  with  is  trained  successfully  under  the  limited  accuracy  in  computations.  The  network  with  a  large  frame  input  layer  is  tested  to  recognize  spoken  korean  words  at  a  forward  retrieval.  Multichip  hardware  module  is  suggested  with  eight  chips  or  more  for  the  extended  performance  and  capacity.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/704afe073992cbe4813cae2f7715336f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/704afe073992cbe4813cae2f7715336f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "61": {
        "TITLE": "Estimating Conditional Probability Densities for Periodic Variables",
        "AUTHORS": "Chris M. Bishop, Claire Legleye",
        "ABSTRACT": "Most of the common techniques for estimating conditional prob(cid:173) ability densities are inappropriate for applications involving peri(cid:173) odic variables. In this paper we introduce three novel techniques  for tackling such problems, and investigate their performance us(cid:173) ing synthetic data. We then apply these techniques to the problem  of extracting the distribution of wind vector directions from radar  scatterometer data gathered by a remote-sensing satellite.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/74bba22728b6185eec06286af6bec36d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/74bba22728b6185eec06286af6bec36d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "62": {
        "TITLE": "Analysis of Unstandardized Contributions in Cross Connected Networks",
        "AUTHORS": "Thomas R. Shultz, Yuriko Oshima-Takane, Yoshio Takane",
        "ABSTRACT": "Understanding  knowledge  representations  in  neural  nets  has  been  a  difficult  problem.  Principal  components  analysis  (PCA)  of  contributions (products of sending activations and connection weights)  has yielded valuable insights into knowledge representations, but much  of this work has focused on the correlation matrix of contributions. The  present work  shows  that analyzing  the  variance-covariance matrix  of  contributions yields more valid  insights by taking account of weights.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/7504adad8bb96320eb3afdd4df6e1f60-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/7504adad8bb96320eb3afdd4df6e1f60-Bibtex.bib",
            "SUPP": ""
        }
    },
    "63": {
        "TITLE": "A Rigorous Analysis of Linsker-type Hebbian Learning",
        "AUTHORS": "J. Feng, H. Pan, V. P. Roychowdhury",
        "ABSTRACT": "We propose a novel rigorous approach for the analysis of Linsker's  unsupervised Hebbian learning network. The behavior of this  model is determined by the underlying nonlinear dynamics which  are parameterized by a set of parameters originating from the Heb(cid:173) bian rule and the arbor density of the synapses. These parameters  determine the presence or absence of a specific receptive field (also  referred to as a 'connection pattern') as a saturated fixed point  attractor of the model. In this paper, we perform a qualitative  analysis of the underlying nonlinear dynamics over the parameter  space, determine the effects of the system parameters on the emer(cid:173) gence of various receptive fields, and predict precisely within which  parameter regime the network will have the potential to develop  a specially designated connection pattern. In particular, this ap(cid:173) proach exposes, for the first time, the crucial role played by the  synaptic density functions, and provides a complete precise picture  of the parameter space that defines the relationships among the  different receptive fields. Our theoretical predictions are confirmed  by numerical simulations. \n320 \nlian/eng Feng, H. Pan, V. P. Roychowdhury",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/7634ea65a4e6d9041cfd3f7de18e334a-Bibtex.bib",
            "SUPP": ""
        }
    },
    "64": {
        "TITLE": "Associative Decorrelation Dynamics: A Theory of Self-Organization and Optimization in Feedback Networks",
        "AUTHORS": "Dawei W. Dong",
        "ABSTRACT": "This  paper  outlines  a  dynamic  theory  of  development  and  adap(cid:173) tation  in  neural  networks  with  feedback  connections.  Given  in(cid:173) put ensemble,  the  connections  change in strength according  to  an  associative  learning  rule  and  approach  a  stable  state  where  the  neuronal  outputs  are  decorrelated .  We  apply  this  theory  to  pri(cid:173) mary  visual  cortex and  examine  the implications of the  dynamical  decorrelation  of the  activities  of orientation  selective  cells  by  the  intracortical connections.  The theory gives a  unified  and quantita(cid:173) tive explanation of the  psychophysical experiments on  orientation  contrast and orientation adaptation.  Using only one parameter, we  achieve  good  agreements  between  the  theoretical  predictions  and  the experimental data.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/766d856ef1a6b02f93d894415e6bfa0e-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/766d856ef1a6b02f93d894415e6bfa0e-Bibtex.bib",
            "SUPP": ""
        }
    },
    "65": {
        "TITLE": "Visual Speech Recognition with Stochastic Networks",
        "AUTHORS": "Javier R. Movellan",
        "ABSTRACT": "This  paper presents  ongoing  work  on  a  speaker  independent  visual  speech recognition system. The work presented here builds on  previous  research  efforts  in  this  area  and  explores  the  potential  use  of simple  hidden  Markov  models  for  limited  vocabulary,  speaker  independent  visual  speech recognition.  The  task  at  hand  is  recognition  of the  first  four  English  digits,  a  task  with  possible  applications  in  car-phone  images  were  modeled  as  mixtures  of  independent  dialing.  The  Gaussian  distributions,  and  the  temporal  dependencies  were  captured  with standard left-to-right hidden Markov  models.  The  results  indicate  that  simple  hidden  Markov  models  may  be  used  to  successfully  recognize relatively unprocessed image sequences. The system  achieved  performance  levels  equivalent  to  untrained  humans  when  asked  to  recognize the fIrst four English digits.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/7b13b2203029ed80337f27127a9f1d28-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/7b13b2203029ed80337f27127a9f1d28-Bibtex.bib",
            "SUPP": ""
        }
    },
    "66": {
        "TITLE": "Finding Structure in Reinforcement Learning",
        "AUTHORS": "Sebastian Thrun, Anton Schwartz",
        "ABSTRACT": "Reinforcement learning addresses  the problem of learning to select actions in order to  maximize one's performance in unknown environments.  To scale reinforcement learning  to complex real-world tasks, such as typically studied in AI, one must ultimately be able  to discover the structure in the world, in order to abstract away the myriad of details and  to operate in more tractable problem spaces.  This paper presents the SKILLS algorithm.  SKILLS discovers skills, which are partially  defined action policies that arise in the context of multiple, related tasks.  Skills collapse  whole action sequences into single operators.  They are learned by minimizing the com(cid:173) pactness of action policies, using a description length argument on their representation.  Empirical results  in  simple grid  navigation tasks  illustrate the successful  discovery  of  structure in reinforcement learning.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/7ce3284b743aefde80ffd9aec500e085-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/7ce3284b743aefde80ffd9aec500e085-Bibtex.bib",
            "SUPP": ""
        }
    },
    "67": {
        "TITLE": "Active Learning with Statistical Models",
        "AUTHORS": "David A. Cohn, Zoubin Ghahramani, Michael I. Jordan",
        "ABSTRACT": "For many types of learners one can compute the statistically \"op(cid:173) timal\" way to select data. We review how these techniques have  been used with feedforward neural networks [MacKay, 1992; Cohn,  1994] . We then show how the same principles may be used to select  data for two alternative, statistically-based learning architectures:  mixtures of Gaussians and locally weighted regression. While the  techniques for neural networks are expensive and approximate, the  techniques for mixtures of Gaussians and locally weighted regres(cid:173) sion are both efficient and accurate. \n1 ACTIVE LEARNING - BACKGROUND \nAn active learning problem is one where the learner has the ability or need to  influence or select its own training data. Many problems of great practical interest  allow active learning, and many even require it.  We consider the problem of actively learning a mapping X - Y based on a set of  training examples {(Xi,Yi)}~l' where Xi E X and Yi E Y. The learner is allowed  to iteratively select new inputs x (possibly from a constrained set), observe the  resulting output y, and incorporate the new examples (x, y) into its training set.  The primary question of active learning is how to choose which x to try next.  There are many heuristics for choosing x based on intuition, including choosing  places where we don't have data, where we perform poorly [Linden and Weber,  1993], where we have low confidence [Thrun and Moller, 1992], where we expect it \n706 \nDavid Cohn, Zoubin Ghahramani, Michael I. Jordon \nto change our model [Cohn et aI, 1990], and where we previously found data that  resulted in learning [Schmidhuber and Storck, 1993].  In this paper we consider how one may select x \"optimally\" from a statistical  viewpoint. We first review how the statistical approach can be applied to neural  networks, as described in MacKay [1992] and Cohn [1994]. We then consider two  alternative, statistically-based learning architectures: mixtures of Gaussians and  locally weighted regression. While optimal data selection for a neural network is  computationally expensive and approximate, we find that optimal data selection for  the two statistical models is efficient and accurate. \n2 ACTIVE LEARNING - A STATISTICAL APPROACH \nWe denote the learner's output given input x as y(x). The mean squared error of  this output can be expressed as the sum of the learner's bias and variance. The  variance 0'3 (x) indicates the learner's uncertainty in its estimate at x. 1 Our goal  will be to select a new example x such that when the resulting example (x, y) is  added to the training set, the integrated variance IV is minimized: \nIV = J 0'3 P (x)dx. \n(1) \nHere, P(x) is the (known) distribution over X. In practice, we will compute a  Monte Carlo approximation of this integral, evaluating 0'3 at a number of random  points drawn according to P(x).  Selecting x so as to minimize IV requires computing 0-3, the new variance at x given  (x, y). Until we actually commit to an x, we do not know what corresponding y we  will see, so the minimization cannot be performed deterministically.2 Many learning  architectures, however, provide an estimate of PWlx) based on current data, so we  can use this estimate to compute the expectation of 0-3. Selecting x to minimize  the expected integrated variance provides a solid statistical basis for choosing new  examples. \n2.1 EXAMPLE: ACTIVE LEARNING WITH A NEURAL",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/7f975a56c761db6506eca0b37ce6ec87-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/7f975a56c761db6506eca0b37ce6ec87-Bibtex.bib",
            "SUPP": ""
        }
    },
    "68": {
        "TITLE": "From Data Distributions to Regularization in Invariant Learning",
        "AUTHORS": "Todd K. Leen",
        "ABSTRACT": "Ideally pattern recognition machines provide constant output when  the inputs are transformed under a group 9 of desired invariances.  These invariances can be achieved  by  enhancing the training data  to include examples of inputs transformed by elements of g,  while  leaving  the  corresponding  targets  unchanged.  Alternatively  the  cost  function  for  training  can  include  a  regularization  term  that  penalizes changes in the output when the input is  transformed un(cid:173) der the group. \nThis paper relates the two approaches, showing precisely the sense  in  which  the regularized  cost function  approximates the result  of  adding  transformed (or distorted)  examples  to  the  training data.  The cost function for the enhanced training set is equivalent to the  sum of the original  cost function plus  a  regularizer.  For  unbiased  models,  the regularizer reduces  to the intuitively obvious choice - a  term that penalizes changes  in  the output when  the inputs are  transformed under  the  group.  For  infinitesimal  transformations,  the coefficient of the regularization term reduces to the variance of  the distortions introduced into the training data.  This correspon(cid:173) dence provides a simple bridge between the two  approaches.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/7fa732b517cbed14a48843d74526c11a-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/7fa732b517cbed14a48843d74526c11a-Bibtex.bib",
            "SUPP": ""
        }
    },
    "69": {
        "TITLE": "An Input Output HMM Architecture",
        "AUTHORS": "Yoshua Bengio, Paolo Frasconi",
        "ABSTRACT": "We  introduce  a  recurrent  architecture  having  a  modular structure  and we formulate a training procedure based on the EM  algorithm.  The resulting model has similarities to hidden  Markov models, but  supports  recurrent  networks  processing style and allows  to exploit  the supervised  learning paradigm while using maximum likelihood  estimation.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/8065d07da4a77621450aa84fee5656d9-Bibtex.bib",
            "SUPP": ""
        }
    },
    "70": {
        "TITLE": "Grouping Components of Three-Dimensional Moving Objects in Area MST of Visual Cortex",
        "AUTHORS": "Richard S. Zemel, Terrence J. Sejnowski",
        "ABSTRACT": "Many  cells  in  the  dorsal  part  of  the  medial  superior  temporal  (MST)  area  of  visual  cortex  respond  selectively  to  spiral  flow  patterns-specific  combinations of expansion/ contraction  and  ro(cid:173) tation  motions.  Previous  investigators  have  suggested  that  these  cells  may represent  self-motion.  Spiral patterns  can  also  be  gener(cid:173) ated by the relative motion of the observer  and a particular object.  An  MST  cell  may  then  account  for  some  portion  of the  complex  flow  field,  and  the  set  of active  cells  could  encode  the  entire  flow;  in  this  manner,  MST  effectively  segments  moving  objects.  Such  a  grouping operation  is  essential  in  interpreting  scenes  containing  several  independent  moving objects  and  observer  motion.  We  de(cid:173) scribe  a  model  based  on  the  hypothesis  that  the  selective  tuning  of MST  cells  reflects  the grouping of object  components  undergo(cid:173) ing  coherent  motion.  Inputs  to  the  model  were  generated  from  sequences  of ray-traced  images that simulated realistic  motion sit(cid:173) uations,  combining observer  motion, eye  movements, and indepen(cid:173) dent  object  motion.  The  input  representation  was  modeled  after  response  properties of neurons in area MT,  which provides the pri(cid:173) mary input to area MST.  After  applying an  unsupervised  learning  algorithm,  the  units  became  tuned  to  patterns  signaling  coherent  motion.  The results  match many of the  known  properties  of MST  cells  and  are  consistent  with  recent  studies  indicating  that  these  cells  process  3-D  object  motion information. \n166 \nRichard S.  Zemel,  Terrence  J.  Sejnowski",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/860320be12a1c050cd7731794e231bd3-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/860320be12a1c050cd7731794e231bd3-Bibtex.bib",
            "SUPP": ""
        }
    },
    "71": {
        "TITLE": "Higher Order Statistical Decorrelation without Information Loss",
        "AUTHORS": "Gustavo Deco, Wilfried Brauer",
        "ABSTRACT": "A neural network learning paradigm based on information theory is pro(cid:173) posed as a way to perform in an unsupervised fashion, redundancy  reduction among the elements of the output layer without loss of infor(cid:173) mation from the sensory input. The model developed performs nonlin(cid:173) ear decorrelation up to higher orders of the cumulant tensors and results  in probabilistic ally independent components of the output layer. This  means that we don't need to assume Gaussian distribution neither at the  input nor at the output. The theory presented is related to the unsuper(cid:173) vised-learning theory of Barlow, which proposes redundancy reduction  as the goal of cognition. When nonlinear units are used nonlinear princi(cid:173) pal component analysis is obtained. In this case nonlinear manifolds can  be reduced to minimum dimension manifolds. If such units are used the  network performs a generalized principal component analysis in the  sense that non-Gaussian distributions can be linearly decorrelated and  higher orders of the correlation tensors are also taken into account. The  basic structure of the architecture involves a general transfOlmation that  is volume conserving and therefore the entropy, yielding a map without  loss of infoIIDation. Minimization of the mutual infoIIDation among the  output neurons eliminates the redundancy between the outputs and  results in statistical decorrelation of the extracted features. This is  known as factorialleaming. \n248 \nGustavo Deco, Wilfried Brauer",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/892c91e0a653ba19df81a90f89d99bcd-Bibtex.bib",
            "SUPP": ""
        }
    },
    "72": {
        "TITLE": "Sample Size Requirements for Feedforward Neural Networks",
        "AUTHORS": "Michael J. Turmon, Terrence L. Fine",
        "ABSTRACT": "We estimate the number of training samples required to ensure that  the performance of a neural network on its training data matches  that obtained when fresh data is applied to the network. Existing  estimates are higher by orders of magnitude than practice indicates.  This work seeks to narrow the gap between theory and practice by  transforming the problem into determining the distribution of the  supremum of a random field in the space of weight vectors, which  in turn is attacked by application of a recent technique called the  Poisson clumping heuristic. \n1 \nINTRODUCTION AND KNOWN RESULTS \nWe investigate the tradeofi\"s among network complexity, training set size, and sta(cid:173) tistical performance of feedforward neural networks so as to allow a reasoned choice  of network architecture in the face of limited training data. Nets are functions  7](x; w), parameterized by their weight vector w E W ~ Rd , which take as input  points x E Rk. For classifiers, network output is restricted to {a, 1} while for fore(cid:173) casting it may be any real number. The architecture of all nets under consideration  is N, whose complexity may be gauged by its Vapnik-Chervonenkis (VC) dimension  v, the size of the largest set of inputs the architecture can classify in any desired way  ('shatter'). Nets 7] EN are chosen on the basis of a training set T = {(Xi, YiHr=l.  These n samples are i.i.d. according to an unknown probability law P. Performance  of a network is measured by the mean-squared error",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/89fcd07f20b6785b92134bd6c1d0fa42-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/89fcd07f20b6785b92134bd6c1d0fa42-Bibtex.bib",
            "SUPP": ""
        }
    },
    "73": {
        "TITLE": "Generalisation in Feedforward Networks",
        "AUTHORS": "Adam Kowalczyk, Herman L. Ferrá",
        "ABSTRACT": "We  discuss  a  model  of consistent  learning  with  an  additional  re(cid:173) striction  on  the  probability  distribution  of training  samples,  the  target concept  and hypothesis class.  We  show  that the model pro(cid:173) vides  a  significant  improvement  on  the  upper  bounds  of sample  complexity,  i.e.  the  minimal number  of random  training samples  allowing  a  selection  of the  hypothesis  with  a  predefined  accuracy  and  confidence.  Further,  we  show  that  the  model  has  the  poten(cid:173) tial  for  providing  a  finite  sample  complexity  even  in  the  case  of  infinite  VC-dimension  as  well  as  for  a  sample  complexity  below  VC-dimension.  This is  achieved  by  linking sample  complexity  to  an  \"average\"  number  of implement able dichotomies of a  training  sample  rather  than  the  maximal  size  of a  shattered  sample,  i.e.  VC-dimension.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/8b5040a8a5baf3e0e67386c2e3a9b903-Bibtex.bib",
            "SUPP": ""
        }
    },
    "74": {
        "TITLE": "The Use of Dynamic Writing Information in a Connectionist On-Line Cursive Handwriting Recognition System",
        "AUTHORS": "Stefan Manke, Michael Finke, Alex Waibel",
        "ABSTRACT": "In this paper we present NPen ++, a connectionist system for  writer independent, large vocabulary on-line cursive handwriting  recognition. This system combines a robust input representation,  which preserves the dynamic writing information, with a neural  network architecture, a so called Multi-State Time Delay Neural  Network (MS-TDNN), which integrates rec.ognition and segmen(cid:173) tation in a single framework. Our preprocessing transforms the  original coordinate sequence into a (still temporal) sequence offea(cid:173) ture vectors, which combine strictly local features, like curvature  or writing direction, with a bitmap-like representation of the co(cid:173) ordinate's proximity. The MS-TDNN architecture is well suited  for handling temporal sequences as provided by this input rep(cid:173) resentation. Our system is tested both on writer dependent and  writer independent tasks with vocabulary sizes ranging from 400  up to 20,000 words. For example, on a 20,000 word vocabulary we  achieve word recognition rates up to 88.9% (writer dependent) and  84.1 % (writer independent) without using any language models.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/8b6dd7db9af49e67306feb59a8bdc52c-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/8b6dd7db9af49e67306feb59a8bdc52c-Bibtex.bib",
            "SUPP": ""
        }
    },
    "75": {
        "TITLE": "Direct Multi-Step Time Series Prediction Using TD(λ)",
        "AUTHORS": "Peter T. Kazlas, Andreas S. Weigend",
        "ABSTRACT": "Abstract Unavailable",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/8d6dc35e506fc23349dd10ee68dabb64-Bibtex.bib",
            "SUPP": ""
        }
    },
    "76": {
        "TITLE": "Capacity and Information Efficiency of a Brain-like Associative Net",
        "AUTHORS": "Bruce Graham, David Willshaw",
        "ABSTRACT": "We have determined the capacity and information efficiency of an  associative net configured in a brain-like way with partial connec(cid:173) tivity and noisy input cues. Recall theory was used to calculate  the capacity when pattern recall is achieved using a winners-take(cid:173) all strategy. Transforming the dendritic sum according to input  activity and unit usage can greatly increase the capacity of the  associative net under these conditions. For moderately sparse pat(cid:173) terns, maximum information efficiency is achieved with very low  connectivity levels (~ 10%). This corresponds to the level of con(cid:173) nectivity commonly seen in the brain and invites speculation that  the brain is connected in the most information efficient way.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/8f468c873a32bb0619eaeb2050ba45d1-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/8f468c873a32bb0619eaeb2050ba45d1-Bibtex.bib",
            "SUPP": ""
        }
    },
    "77": {
        "TITLE": "SARDNET: A Self-Organizing Feature Map for Sequences",
        "AUTHORS": "Daniel L. James, Risto Miikkulainen",
        "ABSTRACT": "A  self-organizing  neural  network  for  sequence  classification  called  SARDNET is described  and analyzed experimentally.  SARDNET  extends the Kohonen  Feature Map architecture  with activation re(cid:173) tention  and  decay  in  order  to  create  unique  distributed  response  patterns for different sequences.  SARDNET yields extremely dense  yet descriptive representations of sequential input in very few  train(cid:173) ing  iterations.  The  network  has  proven  successful  on  mapping ar(cid:173) bitrary sequences  of binary and real numbers,  as well  as  phonemic  representations  of  English  words.  Potential  applications  include  isolated  spoken  word  recognition  and  cognitive  science  models  of  sequence  processing.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/90794e3b050f815354e3e29e977a88ab-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/90794e3b050f815354e3e29e977a88ab-Bibtex.bib",
            "SUPP": ""
        }
    },
    "78": {
        "TITLE": "Deterministic Annealing Variant of the EM Algorithm",
        "AUTHORS": "Naonori Ueda, Ryohei Nakano",
        "ABSTRACT": "We  present  a  deterministic annealing variant of the EM  algorithm  for  maximum likelihood  parameter  estimation  problems.  In  our  approach,  the  EM  process  is  reformulated  as the  problem of min(cid:173) imizing  the  thermodynamic free  energy  by  using  the  principle  of  maximum  entropy and  statistical mechanics  analogy.  Unlike simu(cid:173) lated  annealing approaches,  this  minimization is  deterministically  performed.  Moreover,  the  derived  algorithm,  unlike  the  conven(cid:173) tional EM  algorithm, can obtain better estimates free of the initial  parameter values.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/92262bf907af914b95a0fc33c3f33bf6-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/92262bf907af914b95a0fc33c3f33bf6-Bibtex.bib",
            "SUPP": ""
        }
    },
    "79": {
        "TITLE": "A Non-linear Information Maximisation Algorithm that Performs Blind Separation",
        "AUTHORS": "Anthony J. Bell, Terrence J. Sejnowski",
        "ABSTRACT": "A new learning algorithm is derived which performs online stochas(cid:173) tic gradient ascent in the mutual information between outputs and  inputs  of a  network.  In  the  absence  of a  priori knowledge  about  the  'signal'  and  'noise'  components  of the  input,  propagation  of  information depends  on  calibrating network  non-linearities to  the  detailed  higher-order  moments of the  input  density functions.  By  incidentally  minimising  mutual  information  between  outputs,  as  well  as  maximising  their  individual  entropies,  the  network  'fac(cid:173) torises'  the  input  into  independent  components.  As  an  example  application,  we  have  achieved  near-perfect  separation of ten  digi(cid:173) tally mixed speech signals.  Our simulations lead us to believe  that  our network performs better at blind separation than the Herault(cid:173) J utten network,  reflecting the fact that it is derived rigorously from  the  mutual information objective. \n468 \nAnthony J.  Bell,  Terrence  J.  Sejnowski",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/9232fe81225bcaef853ae32870a2b0fe-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/9232fe81225bcaef853ae32870a2b0fe-Bibtex.bib",
            "SUPP": ""
        }
    },
    "80": {
        "TITLE": "Pulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories",
        "AUTHORS": "A. J. Holmes, Alan F. Murray, Stephen Churcher, J. Hajto, M. J. Rose",
        "ABSTRACT": "This  paper  presents  results  from  the  first  use  of neural  networks  for  the real-time feedback  control  of high  temperature plasmas in  a  tokamak fusion  experiment.  The tokamak is  currently  the  prin(cid:173) cipal  experimental  device  for  research  into  the  magnetic  confine(cid:173) ment  approach  to  controlled  fusion.  In  the  tokamak,  hydrogen  plasmas,  at  temperatures  of  up  to  100  Million  K,  are  confined  by  strong  magnetic  fields.  Accurate  control  of the  position  and  shape  of the plasma boundary  requires  real-time feedback  control  of the magnetic field  structure on  a  time-scale of a  few  tens of mi(cid:173) croseconds.  Software simulations have demonstrated that a  neural  network  approach  can  give  significantly  better  performance  than  the linear technique currently  used  on  most tokamak experiments.  The practical  application of the neural  network  approach  requires  high-speed  hardware,  for  which  a  fully  parallel  implementation of  the  multilayer perceptron,  using  a  hybrid  of digital and  analogue  technology,  has been  developed.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/9246444d94f081e3549803b928260f56-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/9246444d94f081e3549803b928260f56-Bibtex.bib",
            "SUPP": ""
        }
    },
    "81": {
        "TITLE": "Dynamic Cell Structures",
        "AUTHORS": "Jörg Bruske, Gerald Sommer",
        "ABSTRACT": "Dynamic Cell Structures (DCS) represent a family  of artificial  neural  architectures  suited  both  for  unsupervised  and  supervised  learning.  They belong to the recently [Martinetz94] introduced class of Topology  Representing  Networks  (TRN)  which  build  perlectly topology  pre(cid:173) serving feature maps. DCS empI'oy a modified Kohonen learning rule  in conjunction with competitive Hebbian learning. The Kohonen type  learning rule serves to adjust the synaptic weight vectors while Hebbian  learning  establishes  a  dynamic  lateral  connection  structure  between  the units reflecting the topology of the feature manifold. In case of super(cid:173) vised learning, i.e. function approximation, each neural unit implements  a Radial Basis Function, and an  additional layer of linear output units  adjusts according to a delta-rule. DCS is the first RBF-based approxima(cid:173) tion  scheme attempting to concurrently learn and  utilize a perfectly to(cid:173) pology preserving map for improved performance.  Simulations on a selection of CMU-Benchmarks indicate that the DCS  idea applied to the Growing Cell Structure algorithm [Fritzke93] leads  to an  efficient and elegant algorithm  that can  beat conventional  models  on similar tasks.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/92977ae4d2ba21425a59afb269c2a14e-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/92977ae4d2ba21425a59afb269c2a14e-Bibtex.bib",
            "SUPP": ""
        }
    },
    "82": {
        "TITLE": "Single Transistor Learning Synapses",
        "AUTHORS": "Paul E. Hasler, Chris Diorio, Bradley A. Minch, Carver Mead",
        "ABSTRACT": "We  describe single-transistor silicon synapses that compute, learn,  and  provide  non-volatile  memory retention.  The single  transistor  synapses  simultaneously  perform long  term  weight  storage,  com(cid:173) pute the product of the input and the weight value, and update the  weight value according to a Hebbian or a backpropagation learning  rule.  Memory  is  accomplished  via  charge  storage  on  polysilicon  floating  gates, providing long-term  retention without refresh.  The  synapses efficiently use the physics of silicon to perform weight  up(cid:173) dates; the weight value is  increased using tunneling and the weight  value  decreases  using  hot  electron  injection.  The  small  size  and  low  power operation of single transistor synapses allows  the devel(cid:173) opment  of  dense  synaptic  arrays.  We  describe  the  design,  fabri(cid:173) cation,  characterization,  and  modeling  of an  array  of single  tran(cid:173) sistor  synapses.  When  the steady state source  current  is  used  as  the representation of the weight  value,  both the incrementing and  decrementing functions  are  proportional to a  power of the source  current.  The synaptic  array  was  fabricated  in  the standard  21'm  double - poly,  analog process available from  MOSIS.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/934815ad542a4a7c5e8a2dfa04fea9f5-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/934815ad542a4a7c5e8a2dfa04fea9f5-Bibtex.bib",
            "SUPP": ""
        }
    },
    "83": {
        "TITLE": "Comparing the prediction accuracy of artificial neural networks and other statistical models for breast cancer survival",
        "AUTHORS": "Harry B. Burke, David B. Rosen, Philip H. Goodman",
        "ABSTRACT": "The  TNM  staging  system  has  been  used  since  the  early  1960's  to  predict  breast  cancer  patient  outcome.  In  an  attempt  to  in(cid:173) crease  prognostic  accuracy,  many putative prognostic factors  have  been  identified.  Because  the  TNM  stage  model  can  not  accom(cid:173) modate  these  new  factors,  the  proliferation  of factors  in  breast  cancer  has  lead  to  clinical  confusion.  What  is  required  is  a  new  computerized  prognostic system  that  can test  putative prognostic  factors  and  integrate  the  predictive  factors  with  the  TNM  vari(cid:173) ables  in order to increase  prognostic  accuracy.  Using  the area un(cid:173) der  the curve of the receiver  operating characteristic,  we  compare  the  accuracy  of the  following  predictive  models  in  terms  of five  year  breast cancer-specific  survival:  pTNM staging system,  princi(cid:173) pal component analysis,  classification and regression  trees,  logistic  regression,  cascade  correlation neural network,  conjugate gradient  descent  neural,  probabilistic neural network,  and backpropagation  neural network.  Several statistical models are significantly more ac-",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/9908279ebbf1f9b250ba689db6a0222b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/9908279ebbf1f9b250ba689db6a0222b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "84": {
        "TITLE": "Learning direction in global motion: two classes of psychophysically-motivated models",
        "AUTHORS": "V. Sundareswaran, Lucia M. Vaina",
        "ABSTRACT": "Perceptual learning is  defined  as  fast  improvement  in  performance and  retention  of the  learned  ability  over  a  period  of time.  In a  set  of psy(cid:173) chophysical experiments  we  demonstrated  that  perceptual learning oc(cid:173) curs for the discrimination of direction in stochastic motion stimuli.  Here  we  model  this  learning  using  two  approaches:  a clustering  model  that  learns  to  accommodate  the motion  noise,  and an averaging  model  that  learns to  ignore the noise.  Simulations of the models show  performance  similar to the psychophysical results.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/9ab0d88431732957a618d4a469a0d4c3-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/9ab0d88431732957a618d4a469a0d4c3-Bibtex.bib",
            "SUPP": ""
        }
    },
    "85": {
        "TITLE": "On-line Learning of Dichotomies",
        "AUTHORS": "N. Barkai, H. S. Seung, H. Sompolinsky",
        "ABSTRACT": "The performance of on-line algorithms for learning dichotomies is studied. In on-line learn(cid:173) ing, the number of examples P is equivalent to the learning time, since each example is  presented only once. The learning curve, or generalization error as a function of P, depends  on the schedule at which the learning rate is lowered. For a target that is a perceptron rule,  the learning curve of the perceptron algorithm can decrease as fast as p- 1 , if the sched(cid:173) ule is optimized. If the target is not realizable by a perceptron, the perceptron algorithm  does not generally converge to the solution with lowest generalization error. For the case  of unrealizability due to a simple output noise, we propose a new on-line algorithm for a  perceptron yielding a learning curve that can approach the optimal generalization error as  fast as p-l/2. We then generalize the perceptron algorithm to any class of thresholded  smooth functions learning a target from that class. For \"well-behaved\" input distributions,  if this algorithm converges to the optimal solution, its learning curve can decrease as fast  as p-l.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/9c01802ddb981e6bcfbec0f0516b8e35-Bibtex.bib",
            "SUPP": ""
        }
    },
    "86": {
        "TITLE": "Asymptotics of Gradient-based Neural Network Training Algorithms",
        "AUTHORS": "Sayandev Mukherjee, Terrence L. Fine",
        "ABSTRACT": "We study the asymptotic properties of the sequence of iterates of  weight-vector estimates obtained by training a multilayer feed for(cid:173) ward neural network with a basic gradient-descent method using  a fixed learning constant and no batch-processing. In the one(cid:173) dimensional case, an exact analysis establishes the existence of a  limiting distribution that is not Gaussian in general. For the gen(cid:173) eral case and small learning constant, a linearization approximation  permits the application of results from the theory of random ma(cid:173) trices to again establish the existence of a limiting distribution.  We study the first few moments of this distribution to compare  and contrast the results of our analysis with those of techniques of  stochastic approximation.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/9f53d83ec0691550f7d2507d57f4f5a2-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/9f53d83ec0691550f7d2507d57f4f5a2-Bibtex.bib",
            "SUPP": ""
        }
    },
    "87": {
        "TITLE": "Convergence Properties of the K-Means Algorithms",
        "AUTHORS": "Léon Bottou, Yoshua Bengio",
        "ABSTRACT": "This paper studies the convergence properties of the well known  K-Means clustering algorithm. The K-Means algorithm can be de(cid:173) scribed either as a gradient descent algorithm or by slightly extend(cid:173) ing the mathematics of the EM algorithm to this hard threshold  case. We show that the K-Means algorithm actually minimizes the  quantization error using the very fast Newton algorithm.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/a1140a3d0df1c81e24ae954d935e8926-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/a1140a3d0df1c81e24ae954d935e8926-Bibtex.bib",
            "SUPP": ""
        }
    },
    "88": {
        "TITLE": "Using Voice Transformations to Create Additional Training Talkers for Word Spotting",
        "AUTHORS": "Eric I. Chang, Richard P Lippmann",
        "ABSTRACT": "Speech  recognizers  provide  good  performance  for  most  users  but  the  error rate often  increases dramatically  for a small  percentage of talkers  who are \"different\" from  those talkers used for training.  One expensive  solution to  this problem is  to  gather more training data in an attempt to  sample these outlier users. A second solution, explored in this paper,  is  to artificially enlarge the number of training talkers by transforming the  speech of existing training talkers. This approach is similar to enlarging  the training set for  OCR digit recognition  by  warping the training digit  images,  but  is  more  difficult  because  continuous  speech  has  a  much  larger number of dimensions  (e.g.  linguistic,  phonetic,  style,  temporal,  spectral) that differ across talkers. We explored the use of simple linear  spectral warping to enlarge a 48-talker training data base used for  word  spotting.  The  average  detection  rate  overall  was  increased  by  2.9  percentage  points  (from  68.3%  to  71.2%)  for  male  speakers  and  2.5  percentage  points  (from  64.8%  to  67.3%)  for  female  speakers.  This  increase is  small but similar to  that obtained by  doubling the amount of  training data.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/a3d68b461bd9d3533ee1dd3ce4628ed4-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/a3d68b461bd9d3533ee1dd3ce4628ed4-Bibtex.bib",
            "SUPP": ""
        }
    },
    "89": {
        "TITLE": "Forward dynamic models in human motor control: Psychophysical evidence",
        "AUTHORS": "Daniel M. Wolpert, Zoubin Ghahramani, Michael I. Jordan",
        "ABSTRACT": "Based  on  computational  principles,  with  as  yet  no  direct  experi(cid:173) mental  validation,  it  has  been  proposed  that  the  central  nervous  system  (CNS)  uses  an internal model to simulate the  dynamic be(cid:173) havior of the motor system in planning, control and learning (Sut(cid:173) ton  and  Barto,  1981;  Ito,  1984;  Kawato  et  aI.,  1987;  Jordan  and  Rumelhart,  1992;  Miall et aI.,  1993).  We  present  experimental re(cid:173) sults  and simulations based on a  novel  approach  that investigates  the temporal propagation of errors  in the sensorimotor integration  process.  Our results  provide  direct  support for  the  existence  of an  internal model.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/a4300b002bcfb71f291dac175d52df94-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/a4300b002bcfb71f291dac175d52df94-Bibtex.bib",
            "SUPP": ""
        }
    },
    "90": {
        "TITLE": "Direction Selectivity In Primary Visual Cortex Using Massive Intracortical Connections",
        "AUTHORS": "Humbert Suarez, Christof Koch, Rodney Douglas",
        "ABSTRACT": "Almost all models of orientation and direction selectivity in visual  cortex are based on feedforward connection schemes, where genicu(cid:173) late input provides all excitation to both pyramidal and inhibitory  neurons. The latter neurons then suppress the response of the for(cid:173) mer for non-optimal stimuli. However, anatomical studies show  that up to 90 % of the excitatory synaptic input onto any corti(cid:173) cal cell is provided by other cortical cells. The massive excitatory  feedback nature of cortical circuits is embedded in the canonical  microcircuit of Douglas &. Martin (1991). We here investigate ana(cid:173) lytically and through biologically realistic simulations the function(cid:173) ing of a detailed model of this circuitry, operating in a hysteretic  mode. In the model, weak geniculate input is dramatically ampli(cid:173) fied by intracortical excitation, while inhibition has a dual role: (i)  to prevent the early geniculate-induced excitation in the null di(cid:173) rection and (ii) to restrain excitation and ensure that the neurons  fire only when the stimulus is in their receptive-field. Among the \n4 \nHumbert Suarez, Christo! Koch, Rodney Douglas \ninsights gained are the possibility that hysteresis underlies visual  cortical function, paralleling proposals for short-term memory, and  strong limitations on linearity tests that use gratings. Properties  of visual cortical neurons are compared in detail to this model and  to a classical model of direction selectivity that does not include  excitatory corti co-cortical connections. The model explain a num(cid:173) ber of puzzling features of direction-selective simple cells, includ(cid:173) ing the small somatic input conductance changes that have been  measured experimentally during stimulation in the null direction.  The model also allows us to understand why the velocity-response  curve of area 17 neurons is different from that of their LG N affer(cid:173) ents, and the origin of expansive and compressive nonlinearities in  the contrast-response curve of striate cortical neurons.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/a64c94baaf368e1840a1324e839230de-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/a64c94baaf368e1840a1324e839230de-Bibtex.bib",
            "SUPP": ""
        }
    },
    "91": {
        "TITLE": "Bayesian Query Construction for Neural Network Models",
        "AUTHORS": "Gerhard Paass, Jörg Kindermann",
        "ABSTRACT": "If data collection is costly, there is much to be gained by actively se(cid:173) lecting particularly informative data points in a sequential  way.  In  a  Bayesian decision-theoretic framework we  develop  a  query selec(cid:173) tion  criterion  which  explicitly takes  into account  the intended  use  of the model predictions.  By  Markov Chain Monte Carlo methods  the  necessary  quantities  can  be  approximated to  a  desired  preci(cid:173) sion.  As  the  number  of data  points  grows,  the  model  complexity  is  modified  by  a  Bayesian  model  selection  strategy.  The  proper(cid:173) ties of two  versions  of the criterion  ate demonstrated in numerical  experiments.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/a9b7ba70783b617e9998dc4dd82eb3c5-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/a9b7ba70783b617e9998dc4dd82eb3c5-Bibtex.bib",
            "SUPP": ""
        }
    },
    "92": {
        "TITLE": "A Silicon Axon",
        "AUTHORS": "Bradley A. Minch, Paul E. Hasler, Chris Diorio, Carver Mead",
        "ABSTRACT": "We  present  a  silicon  model of an  axon  which  shows  promise  as  a  building  block for  pulse-based  neural  computations involving cor(cid:173) relations of pulses  across  both space  and time.  The  circuit shares  a  number  of features  with  its  biological  counterpart  including  an  excitation  threshold,  a  brief refractory  period  after  pulse  comple(cid:173) tion, pulse amplitude restoration, and pulse width restoration.  We  provide  a simple explanation of circuit operation and present  data  from  a  chip fabricated  in  a standard  2Jlm  CMOS  process  through  the MOS  Implementation Service  (MOSIS).  We  emphasize the ne(cid:173) cessity of the restoration of the width of the pulse in time for stable  propagation in  axons.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/aa169b49b583a2b5af89203c2b78c67c-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/aa169b49b583a2b5af89203c2b78c67c-Bibtex.bib",
            "SUPP": ""
        }
    },
    "93": {
        "TITLE": "Plasticity-Mediated Competitive Learning",
        "AUTHORS": "Nicol N. Schraudolph, Terrence J. Sejnowski",
        "ABSTRACT": "Differentiation between the nodes of a competitive learning net(cid:173) work is conventionally achieved through competition on the ba(cid:173) sis of neural activity.  Simple inhibitory mechanisms are  limited  to  sparse  representations,  while  decorrelation  and  factorization  schemes that support distributed representations are computation(cid:173) ally unattractive.  By letting neural plasticity mediate the compet(cid:173) itive interaction instead, we obtain diffuse, nonadaptive alterna(cid:173) tives for fully distributed representations.  We  use this technique  to  Simplify and improve our binary information gain optimiza(cid:173) tion algorithm for feature extraction (Schraudolph and Sejnowski,  1993); the same approach could be used to improve other learning  algorithms.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/aa68c75c4a77c87f97fb686b2f068676-Bibtex.bib",
            "SUPP": ""
        }
    },
    "94": {
        "TITLE": "Active Learning for Function Approximation",
        "AUTHORS": "Kah Kay Sung, Partha Niyogi",
        "ABSTRACT": "We develop a principled strategy to sample a function optimally for  function  approximation tasks within a  Bayesian framework.  Using  ideas  from  optimal experiment  design,  we  introduce  an  objective  function (incorporating both bias and variance)  to measure the de(cid:173) gree  of approximation, and the  potential utility of the  data points  towards optimizing this objective.  We show how  the general strat(cid:173) egy  can be used  to  derive  precise  algorithms to select  data for  two  cases:  learning  unit  step  functions  and  polynomial functions.  In  particular, we investigate whether such active algorithms can learn  the  target  with fewer  examples.  We obtain theoretical  and empir(cid:173) ical  results  to suggest  that  this is  the  case. \nINTRODUCTION  AND  MOTIVATION \n1  Learning  from  examples is  a  common supervised  learning paradigm that  hypothe(cid:173) sizes a target concept given a stream of training examples that describes the concept.  In function  approximation,  example-based  learning can be formulated as synthesiz(cid:173) ing  an  approximation function  for  data sampled from  an unknown  target function  (Poggio and Girosi,  1990). \nActive learning describes  a class of example-based learning paradigms that seeks out  new  training examples from specific regions of the input space,  instead of passively  accepting examples from some data generating source.  By  judiciously selecting ex-\n594 \nKah  Kay Sung,  Parlha  Niyogi \namples instead of allowing for possible random sampling, active  learning techniques  can  conceivably  have  faster  learning  rates  and  better  approximation results  than  passive learning methods. \nThis paper presents  a  Bayesian formulation for  active  learning within the function  approximation framework.  Specifically, here is the problem we  want to address:  Let  Dn  = {(Xi, Yi)li  = 1, ... , n}  be  a  set  of n  data points  sampled from  an  unknown  target function  g,  possibly in the presence  of noise.  Given  an  approximation func(cid:173) tion  concept  class,  :F,  where  each  f  E :F  has prior  probability P;:-[J],  one  can  use  regularization  techniques  to  approximate 9 from  Dn  (in  the  Bayes  optimal sense)  by  means  of a  function  9 E:F.  We  want  a  strategy  to  determine  at  what  input  location one  should  sample the next  data point,  (XN+l, YN+d,  in order  to  obtain  the  \"best\"  possible Bayes optimal approximation of the unknown target function  9  with our concept  class  :F. \nThe data sampling problem consists  of two  parts: \n1)  Defining  what  we  mean  by  the  \"best\"  possible  Bayes  optimal  ap(cid:173) proximation of an  unknown  target  function.  In  this  paper,  we  propose  an  optimality criterion for  evaluating the  \"goodness\"  of a  solution  with  respect  to an  unknown  target function. \n2)  Formalizing precisely  the  task  of determining where  in  input  space  to  sample  the  next  data  point.  We  express  the  above  mentioned optimality  criterion  as  a  cost  function  to  be  minimized,  and  the  task  of choosing  the  next  sample  as  one  of minimizing  the  cost  function  with  respect  to  the  input  space  location of the  next sample point. \nEarlier  work  (Cohn,  1991;  MacKay,  1992)  have  tried  to use similar optimal experi(cid:173) ment design (Fedorov, 1972) techniques to collect data that would provide maximum  information about  the  target function.  Our  work  differs  from  theirs  in several  re(cid:173) spects.  First, we  use  a  different,  and perhaps more general,  optimality criterion for  evaluating solutions to an unknown target function, based on a measure of function  uncertainty that incorporates both bias and variance components of the total output  generalization error.  In contrast,  MacKay  and Cohn use only  variance components  in  model  parameter space.  Second,  we  address  the  important sample  complexity  question,  i.e.,  does  the  active  strategy  require  fewer  examples  to  learn  the  target  to  the  same  degree  of uncertainty?  Our  results  are  stated  in  PAC-style  (Valiant,  1984).  After completion of this work,  we learnt that Sollich (1994) had also recently  developed  a  similar formulation  to  ours.  His  analysis  is  conducted  in  a  statistical  physics framework. \nThe rest of the paper is organized as follows:  Section 2,  develops our active sampling  paradigm.  In Sections 3 and 4,  we  consider two classes  offunctions for  which active  strategies  are  obtained,  and  investigate  their  performance  both  theoretically  and  empirically. \n2  THE MATHEMATICAL  FRAMEWORK  In  order  to optimally select  examples for  a  learning  task,  one  should  first  have  a  clear notion of what  an  \"ideal\"  learning goal is for  the task.  We can  then  measure  an example's utility in  terms of how  well  the example helps the learner achieve the \nActive  Learning for  Function  Approximation \n595 \ngoal,  and  devise  an  active sampling strategy  that selects  examples  with maximum  potential  utility.  In  this  section,  we  propose  one such  learning goal  - to  find  an  approximation function  g  E :F  that  \"best\"  estimates  the  unknown  target  function  g.  We  then derive  an example utility cost  function for  the goal and finally  present  a  general procedure for  selecting  examples. \n2.1  EVALUATING  A  SOLUTION TO  AN  UNKNOWN  TARGET - THE EXPECTED  INTEGRATED  SQUARED DIFFERENCE \nLet 9  be the target function that we want to estimate by means of an approximation  function 9 E  :F.  If the  target function  9  were  known,  then one natural measure of  how  well  (or  badly)  g  approximates 9  would  be  the  Integrated  Squared  Difference  (ISD)  of the  two functions:",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Bibtex.bib",
            "SUPP": ""
        }
    },
    "95": {
        "TITLE": "Patterns of damage in neural networks: The effects of lesion area, shape and number",
        "AUTHORS": "Eytan Ruppin, James A. Reggia",
        "ABSTRACT": "Current understanding of the effects of damage on neural networks  is  rudimentary, even  though such  understanding  could lead to im(cid:173) portant insights concerning neurological and psychiatric disorders.  Motivated  by  this  consideration,  we  present  a  simple  analytical  framework for  estimating the functional  damage resulting from fo(cid:173) cal  structural  lesions  to  a  neural  network.  The  effects  of focal  le(cid:173) sions of varying area,  shape and number on the retrieval capacities  of a spatially-organized associative memory.  Although our analyti(cid:173) cal results are based on some approximations, they correspond well  with simulation results.  This study sheds light on  some important  features  characterizing  the  clinical  manifestations of multi-infarct  dementia, including the strong  association between  the number of  infarcts and the prevalence of dementia after stroke,  and the 'mul(cid:173) tiplicative' interaction that has been  postulated  to occur  between  Alzheimer's disease  and multi-infarct dementia. \n*Dr.  Reggia  is  also  with  the  Department  of Neurology  and  the Institute  of Advanced \nComputer Studies  at the University  of Maryland. \n36 \nEytan  Ruppin,  James A.  Reggia",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/b55ec28c52d5f6205684a473a2193564-Bibtex.bib",
            "SUPP": ""
        }
    },
    "96": {
        "TITLE": "A Study of Parallel Perturbative Gradient Descent",
        "AUTHORS": "D. Lippe, Joshua Alspector",
        "ABSTRACT": "We  have  continued  our  study  of a  parallel  perturbative  learning  method  [Alspector  et al.,  1993]  and implications for  its implemen(cid:173) tation in analog VLSI. Our new results indicate that, in most cases,  a single parallel perturbation (per pattern presentation) of the func(cid:173) tion  parameters  (weights in a  neural network)  is  theoretically  the  best  course.  This  is  not  true,  however,  for  certain  problems  and  may  not  generally  be  true  when  faced  with  issues  of implemen(cid:173) tation  such  as limited  precision.  In  these  cases,  multiple  parallel  perturbations may be best as indicated in our previous results.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/b56a18e0eacdf51aa2a5306b0f533204-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/b56a18e0eacdf51aa2a5306b0f533204-Bibtex.bib",
            "SUPP": ""
        }
    },
    "97": {
        "TITLE": "A Neural Model of Delusions and Hallucinations in Schizophrenia",
        "AUTHORS": "Eytan Ruppin, James A. Reggia, David Horn",
        "ABSTRACT": "We  implement and study a computational model of Stevens'  [19921  theory  of the  pathogenesis  of schizophrenia.  This  theory  hypoth(cid:173) esizes  that  the  onset  of schizophrenia  is  associated  with  reactive  synaptic regeneration occurring in brain regions receiving degener(cid:173) ating temporal lobe  projections.  Concentrating on  one  such  area,  the  frontal  cortex,  we  model  a  frontal  module  as  an  associative  memory neural  network  whose  input synapses  represent  incoming  temporal  projections.  We  analyze  how,  in  the  face  of weakened  external input projections, compensatory strengthening of internal  synaptic connections and increased  noise levels can maintain mem(cid:173) ory  capacities  (which  are  generally  preserved  in  schizophrenia) .  However,  These  compensatory  changes  adversely  lead  to  sponta(cid:173) neous,  biased  retrieval  of stored  memories,  which  corresponds  to  the occurrence  of schizophrenic  delusions  and hallucinations with(cid:173) out  any  apparent  external  trigger,  and  for  their  tendency  to  con(cid:173) centrate on just few  central themes.  Our  results explain why  these  symptoms tend  to wane  as  schizophrenia progresses,  and why  de(cid:173) layed  therapeutical intervention leads to a  much slower  response. \n150 \nEytan Ruppin,  James  A.  Reggia,  David Hom",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/b6a1085a27ab7bff7550f8a3bd017df8-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/b6a1085a27ab7bff7550f8a3bd017df8-Bibtex.bib",
            "SUPP": ""
        }
    },
    "98": {
        "TITLE": "Correlation and Interpolation Networks for Real-time Expression Analysis/Synthesis",
        "AUTHORS": "Trevor Darrell, Irfan A. Essa, Alex Pentland",
        "ABSTRACT": "We  describe  a  framework  for  real-time  tracking  of facial  expressions  that  uses  neurally-inspired  correlation  and  interpolation  methods.  A  distributed view-based representation is used to characterize facial state,  and is  computed using a  replicated correlation network.  The ensemble  response of the set of view correlation scores is input to a network based  interpolation method, which maps perceptual state to motor control states  for  a  simulated  3-D  face  model.  Activation levels  of the  motor  state  correspond to muscle activations in an  anatomically derived model.  By  integrating fast and robust 2-D processing with 3-D models, we obtain a  system that is able to quickly track and interpret complex facial motions  in real-time.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/b706835de79a2b4e80506f582af3676a-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/b706835de79a2b4e80506f582af3676a-Bibtex.bib",
            "SUPP": ""
        }
    },
    "99": {
        "TITLE": "Neural Network Ensembles, Cross Validation, and Active Learning",
        "AUTHORS": "Anders Krogh, Jesper Vedelsby",
        "ABSTRACT": "Learning  of continuous  valued  functions  using  neural  network  en(cid:173) sembles  (committees) can give  improved accuracy,  reliable estima(cid:173) tion of the generalization error,  and active learning.  The  ambiguity  is  defined as the variation of the output of ensemble members aver(cid:173) aged  over  unlabeled  data, so  it quantifies the  disagreement  among  the networks.  It is  discussed  how  to use the ambiguity in combina(cid:173) tion with cross-validation to give a reliable estimate of the ensemble  generalization error, and how this type of ensemble cross-validation  can sometimes improve performance.  It is  shown  how  to estimate  the optimal weights of the ensemble members using unlabeled  data.  By a generalization of query  by  committee, it is finally shown how  the ambiguity can be used to select new training data to be labeled  in an active learning scheme.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/b8c37e33defde51cf91e1e03e51657da-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/b8c37e33defde51cf91e1e03e51657da-Bibtex.bib",
            "SUPP": ""
        }
    },
    "100": {
        "TITLE": "Extracting Rules from Artificial Neural Networks with Distributed Representations",
        "AUTHORS": "Sebastian Thrun",
        "ABSTRACT": "Although artificial neural networks have been applied in a variety of real-world scenarios  with remarkable success,  they  have often been criticized for exhibiting a low degree of  human comprehensibility.  Techniques that compile compact sets of symbolic rules out  of artificial  neural  networks  offer  a  promising perspective  to  overcome  this  obvious  deficiency of neural network representations.  This paper  presents  an  approach  to  the  extraction of if-then rules  from  artificial  neu(cid:173) Its  key  mechanism  is  validity  interval  analysis,  which  is  a  generic  ral  networks.  tool  for  extracting  symbolic  knowledge  by  propagating rule-like knowledge through  Backpropagation-style neural  networks.  Empirical studies in a robot arm domain illus(cid:173) trate the appropriateness of the proposed method for extracting rules from networks with  real-valued and distributed representations.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/bea5955b308361a1b07bc55042e25e54-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/bea5955b308361a1b07bc55042e25e54-Bibtex.bib",
            "SUPP": ""
        }
    },
    "101": {
        "TITLE": "A model of the hippocampus combining self-organization and associative memory function",
        "AUTHORS": "Michael E. Hasselmo, Eric Schnell, Joshua Berke, Edi Barkai",
        "ABSTRACT": "A model of the hippocampus is presented which forms rapid self -orga(cid:173) nized representations of input arriving via the perforant path, performs  recall of previous associations in region CA3, and performs comparison  of this recall with afferent input in region CA 1.  This comparison drives  feedback regulation of cholinergic modulation to set appropriate  dynamics for learning of new representations in region CA3 and CA 1.  The network responds to novel patterns with increased cholinergic mod(cid:173) ulation, allowing storage of new self-organized representations, but  responds to familiar patterns with a decrease in acetylcholine,  allowing  recall based on previous representations.  This requires selectivity of the  cholinergic suppression of synaptic transmission in stratum radiatum of  regions CA3 and CAl, which has been demonstrated experimentally.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/c22abfa379f38b5b0411bc11fa9bf92f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/c22abfa379f38b5b0411bc11fa9bf92f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "102": {
        "TITLE": "Glove-TalkII: Mapping Hand Gestures to Speech Using Neural Networks",
        "AUTHORS": "Sidney Fels, Geoffrey E. Hinton",
        "ABSTRACT": "Glove-TaikII is  a  system which  translates  hand gestures  to speech  through  an  adaptive interface.  Hand  gestures  are  mapped  contin(cid:173) uously  to  10  control  parameters  of a  parallel formant  speech  syn(cid:173) thesizer.  The mapping allows the hand to act  as  an  artificial  vocal  tract  that  produces  speech  in  real  time.  This  gives  an  unlimited  vocabulary  in addition to direct  control of fundamental  frequency  and  volume.  Currently,  the  best  version  of Glove-TalkII  uses  sev(cid:173) eral  input  devices  (including  a  CyberGlove,  a  ContactGlove,  a  3- space  tracker,  and a foot-pedal),  a  parallel formant speech  synthe(cid:173) sizer  and  3 neural  networks.  The gesture-to-speech  task  is  divided  into  vowel  and  consonant  production  by  using  a  gating  network  to weight  the outputs of a  vowel  and  a  consonant  neural  network.  The  gating  network  and  the  consonant  network  are  trained  with  examples  from  the  user.  The  vowel  network  implements  a  fixed,  user-defined  relationship  between  hand-position  and  vowel  sound  and does  not require any training examples from the user.  Volume,  fundamental  frequency  and  stop  consonants  are  produced  with  a  fixed  mapping from  the  input devices.  One subject  has  trained  to  speak  intelligibly with Glove-TalkII. He  speaks slowly with speech  quality  similar  to a  text-to-speech  synthesizer  but  with  far  more  natural-sounding pitch  variations. \n844 \nS.  Sidney  Fe Is,  Geoffrey  Hinton",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/c2aee86157b4a40b78132f1e71a9e6f1-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/c2aee86157b4a40b78132f1e71a9e6f1-Bibtex.bib",
            "SUPP": ""
        }
    },
    "103": {
        "TITLE": "Learning in large linear perceptrons and why the thermodynamic limit is relevant to the real world",
        "AUTHORS": "Peter Sollich",
        "ABSTRACT": "We  present  a  new  method  for  obtaining  the  response  function  9  and  its  average  G  from  which  most  of the  properties  of learning  and  generalization  in  linear  perceptrons  can  be  derived.  We  first  rederive the  known results for  the 'thermodynamic limit' of infinite  perceptron  size  N  and  show  explicitly  that  9  is  self-averaging  in  this limit.  We  then discuss extensions of our  method to  more gen(cid:173) eral  learning scenarios with  anisotropic teacher  space priors,  input  distributions, and weight  decay  terms.  Finally, we  use our method  to calculate the  finite  N  corrections of order  1/ N  to G  and discuss  the corresponding  finite  size  effects  on generalization  and learning  dynamics.  An  important  spin-off  is  the  observation  that  results  obtained  in  the  thermodynamic limit are often  directly  relevant  to  systems of fairly  modest, 'real-world' sizes.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/c32d9bf27a3da7ec8163957080c8628e-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/c32d9bf27a3da7ec8163957080c8628e-Bibtex.bib",
            "SUPP": ""
        }
    },
    "104": {
        "TITLE": "Learning Saccadic Eye Movements Using Multiscale Spatial Filters",
        "AUTHORS": "Rajesh P. N. Rao, Dana H. Ballard",
        "ABSTRACT": "We  describe  a  framework  for  learning  saccadic  eye  movements  using  a  photometric representation of target points  in natural scenes.  The rep(cid:173) resentation takes the form of a high-dimensional vector comprised of the  responses  of spatial filters  at different  orientations and  scales.  We  first  demonstrate the use  of this  response  vector in  the task of locating pre(cid:173) viously foveated  points in a  scene and subsequently use  this property in  a  multisaccade strategy to derive  an adaptive motor map for  delivering  accurate saccades.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/c4015b7f368e6b4871809f49debe0579-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/c4015b7f368e6b4871809f49debe0579-Bibtex.bib",
            "SUPP": ""
        }
    },
    "105": {
        "TITLE": "A Charge-Based CMOS Parallel Analog Vector Quantizer",
        "AUTHORS": "Gert Cauwenberghs, Volnei Pedroni",
        "ABSTRACT": "We present an analog VLSI chip for parallel analog vector quantiza(cid:173) tion. The MOSIS 2.0 J..Lm double-poly CMOS Tiny chip contains an  array of 16 x 16 charge-based distance estimation cells, implementing a  mean absolute difference (MAD) metric operating on a 16-input analog  vector field and 16 analog template vectors. The distance cell includ(cid:173) ing dynamic template storage measures 60 x 78 J..Lm2 • Additionally,  the chip features a winner-take-all (WTA) output circuit of linear com(cid:173) plexity, with global positive feedback for fast and decisive settling of a  single winner output. Experimental results on the complete 16 x 16 VQ  system demonstrate correct operation with 34 dB analog input dynamic  range and 3 J..Lsec cycle time at 0.7 mW power dissipation.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/c4b31ce7d95c75ca70d50c19aef08bf1-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/c4b31ce7d95c75ca70d50c19aef08bf1-Bibtex.bib",
            "SUPP": ""
        }
    },
    "106": {
        "TITLE": "Boosting the Performance of RBF Networks with Dynamic Decay Adjustment",
        "AUTHORS": "Michael R. Berthold, Jay Diamond",
        "ABSTRACT": "Radial  Basis  Function  (RBF)  Networks,  also  known  as  networks  of locally-tuned processing units  (see  [6])  are well  known for  their  ease  of use.  Most  algorithms  used  to  train  these  types  of  net(cid:173) works,  however,  require a  fixed  architecture, in  which  the number  of units  in  the  hidden  layer  must  be  determined  before  training  starts.  The RCE training algorithm, introduced by  Reilly,  Cooper  and Elbaum  (see  [8]),  and its  probabilistic extension,  the P-RCE  algorithm,  take advantage of a  growing structure in  which  hidden  units are only introduced when  necessary.  The nature of these al(cid:173) gorithms allows training to reach stability much faster  than is  the  case  for  gradient-descent  based  methods.  Unfortunately  P-RCE  networks do not adjust the standard deviation of their prototypes  individually, using only one global value for  this parameter.  This  paper introduces the Dynamic Decay Adjustment  (DDA)  al(cid:173) gorithm  which  utilizes  the  constructive  nature  of the  P-RCE  al(cid:173) gorithm together with independent adaptation of each prototype's  decay factor.  In addition, this radial adjustment is  class dependent  and  distinguishes  between  different  neighbours.  It is  shown  that  networks  trained  with  the  presented  algorithm  perform  substan(cid:173) tially better than common RBF networks. \n522 \nMichael  R.  Berthold,  Jay  Diamond",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/c8c41c4a18675a74e01c8a20e8a0f662-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/c8c41c4a18675a74e01c8a20e8a0f662-Bibtex.bib",
            "SUPP": ""
        }
    },
    "107": {
        "TITLE": "An Alternative Model for Mixtures of Experts",
        "AUTHORS": "Lei Xu, Michael I. Jordan, Geoffrey E. Hinton",
        "ABSTRACT": "We propose an alternative model for mixtures of experts which uses  a  different  parametric form  for  the gating network.  The  modified  model is  trained by the EM  algorithm.  In comparison with earlier  models-trained by either EM  or gradient ascent-there is no need  to  select  a  learning  stepsize.  We  report  simulation experiments  which  show  that  the  new  architecture  yields  faster  convergence.  We  also  apply the new  model to two  problem domains:  piecewise  nonlinear function approximation and the combination of multiple  previously  trained classifiers.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/c8fbbc86abe8bd6a5eb6a3b4d0411301-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/c8fbbc86abe8bd6a5eb6a3b4d0411301-Bibtex.bib",
            "SUPP": ""
        }
    },
    "108": {
        "TITLE": "Catastrophic Interference in Human Motor Learning",
        "AUTHORS": "Tom Brashers-Krug, Reza Shadmehr, Emanuel Todorov",
        "ABSTRACT": "Biological sensorimotor systems are not static maps that transform  input  (sensory  information)  into  output  (motor  behavior).  Evi(cid:173) dence  from  many  lines  of research  suggests  that  their  representa(cid:173) tions are plastic, experience-dependent entities.  While this plastic(cid:173) ity is  essential for  flexible  behavior, it presents  the nervous system  with difficult organizational challenges.  If the sensorimotor system  adapts itself to perform well  under one set of circumstances,  will it  then perform poorly when placed  in an environment with different  demands  (negative  transfer)?  Will  a  later  experience-dependent  change  undo  the benefits  of previous  learning  (catastrophic  inter(cid:173) ference)?  We  explore  the first  question  in  a separate paper in  this  volume  (Shadmehr et  al.  1995).  Here  we  present  psychophysical  and computational results that explore the question of catastrophic  interference  in the context of a  dynamic motor learning task.  Un(cid:173) der  some conditions, subjects show  evidence  of catastrophic  inter(cid:173) ference.  Under  other  conditions,  however,  subjects  appear  to  be  immune to  its  effects.  These  results  suggest  that  motor  learning  can  undergo  a  process  of consolidation.  Modular neural  networks  are  well  suited  for  the  demands of learning multiple input/output  mappings.  By  incorporating the notion of fast- and slow-changing  connections  into  a  modular architecture,  we  were  able  to  account  for  the psychophysical results. \n20 \nTom  Brashers-Krug,  Reza Shadmelzr,  Emanuel Todorov",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/ca75910166da03ff9d4655a0338e6b09-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/ca75910166da03ff9d4655a0338e6b09-Bibtex.bib",
            "SUPP": ""
        }
    },
    "109": {
        "TITLE": "On the Computational Complexity of Networks of Spiking Neurons",
        "AUTHORS": "Wolfgang Maass",
        "ABSTRACT": "We investigate the computational power of a formal model for net(cid:173) works of spiking neurons,  both for  the assumption of an unlimited  timing precision, and for  the case of a limited timing precision.  We  also prove upper and lower bounds for the number of examples that  are  needed  to train such networks. \n1 \nIntroduction and  Basic Definitions \nThere  exists substantial evidence  that timing phenomena such  as  temporal  differ(cid:173) ences  between  spikes  and  frequencies  of oscillating  subsystems  are  integral  parts  of various  information processing  mechanisms  in  biological  neural  systems  (for  a  survey  and references  see  e.g.  Abeles,  1991; Churchland and Sejnowski,  1992; Aert(cid:173) sen,  1993).  Furthermore simulations of a  variety  of specific  mathematical models  for  networks of spiking neurons have shown  that temporal coding offers interesting  possibilities for  solving  classical  benchmark-problems such  as  associative  memory,  binding, and pattern segmentation (for an overview see Gerstner et al., 1992).  Some  aspects  of these  models have  also  been studied  analytically, but  almost nothing is  known about their computational complexity (see Judd and Aihara, 1993, for some  first  results  in  this  direction).  In  this article  we  introduce  a  simple formal model  SNN  for  networks  of spiking neurons  that allows us  to model the  most  important  timing phenomena of neural nets (including synaptic modulation), and we prove up(cid:173) per and lower bounds for its computational power and learning complexity.  Further \n184",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Bibtex.bib",
            "SUPP": ""
        }
    },
    "110": {
        "TITLE": "New Algorithms for 2D and 3D Point Matching: Pose Estimation and Correspondence",
        "AUTHORS": "Steven Gold, Chien-Ping Lu, Anand Rangarajan, Suguna Pappu, Eric Mjolsness",
        "ABSTRACT": "A  fundamental  open  problem  in  computer  vision-determining  pose  and  correspondence  between  two  sets  of  points  in  space(cid:173) is  solved with a novel, robust  and easily implementable algorithm.  The  technique  works  on  noisy  point sets  that  may be  of unequal  sizes  and  may  differ  by  non-rigid  transformations.  A  2D  varia(cid:173) tion  calculates  the  pose  between  point  sets  related  by  an  affine  transformation-translation, rotation, scale and shear.  A 3D to 3D  variation calculates translation and rotation.  An objective describ(cid:173) ing  the  problem is  derived  from  Mean field  theory.  The objective  is  minimized with clocked  (EM-like)  dynamics.  Experiments with  both  handwritten  and  synthetic  data provide  empirical  evidence  for  the method.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/cc1aa436277138f61cda703991069eaf-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/cc1aa436277138f61cda703991069eaf-Bibtex.bib",
            "SUPP": ""
        }
    },
    "111": {
        "TITLE": "PCA-Pyramids for Image Compression",
        "AUTHORS": "Horst Bischof, Kurt Hornik",
        "ABSTRACT": "This paper presents a new  method for image compression by neural  networks.  First, we  show  that we  can use neural networks in  a  py(cid:173) ramidal framework, yielding the so-called PCA pyramids.  Then we  present an image compression method based on the PCA pyramid,  which  is  similar  to  the  Laplace  pyramid  and  wavelet  transform.  Some experimental results  with  real images are reported.  Finally,  we  present  a  method  to  combine  the  quantization  step  with  the  learning of the PCA pyramid.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/ccc0aa1b81bf81e16c676ddb977c5881-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/ccc0aa1b81bf81e16c676ddb977c5881-Bibtex.bib",
            "SUPP": ""
        }
    },
    "112": {
        "TITLE": "Morphogenesis of the Lateral Geniculate Nucleus: How Singularities Affect Global Structure",
        "AUTHORS": "Svilen Tzonev, Klaus Schulten, Joseph G. Malpeli",
        "ABSTRACT": "The macaque lateral geniculate nucleus (LGN) exhibits an intricate  lamination pattern, which changes midway through the nucleus at a  point coincident with small gaps due to the blind spot in the retina.  We  present a  three-dimensional model of morphogenesis in  which  local cell interactions cause a  wave  of development of neuronal re(cid:173) ceptive fields  to propagate through the nucleus  and establish two  distinct lamination patterns.  We  examine the interactions between  the wave  and the localized singularities due  to the gaps,  and find  that the gaps induce the change in lamination pattern.  We  explore  critical factors which  determine general LGN organization.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/cfbce4c1d7c425baf21d6b6f2babe6be-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/cfbce4c1d7c425baf21d6b6f2babe6be-Bibtex.bib",
            "SUPP": ""
        }
    },
    "113": {
        "TITLE": "Reinforcement Learning Predicts the Site of Plasticity for Auditory Remapping in the Barn Owl",
        "AUTHORS": "Alexandre Pouget, Cedric Deffayet, Terrence J. Sejnowski",
        "ABSTRACT": "The auditory system of the barn owl  contains several spatial maps.  In young barn owls raised with optical prisms over their eyes,  these  auditory  maps are  shifted  to stay  in  register  with the  visual  map,  suggesting  that  the  visual  input  imposes  a  frame  of reference  on  the  auditory  maps.  However,  the  optic  tectum,  the  first  site  of  convergence  of visual  with  auditory information, is  not the  site  of  plasticity for  the  shift  of the  auditory  maps;  the  plasticity occurs  instead  in  the  inferior  colliculus, which  contains  an  auditory  map  and projects into the optic tectum.  We explored a model of the owl  remapping in which  a global reinforcement signal whose  delivery  is  controlled  by  visual foveation.  A hebb  learning rule gated by  rein(cid:173) forcement  learned  to appropriately adjust auditory  maps.  In addi(cid:173) tion,  reinforcement  learning  preferentially  adjusted  the  weights  in  the inferior colliculus,  as in the owl brain, even  though the weights  were  allowed  to  change  throughout  the auditory system.  This ob(cid:173) servation  raises  the  possibility  that  the  site  of learning  does  not  have  to  be  genetically specified,  but  could  be  determined  by  how  the learning procedure  interacts with the network  architecture. \n126 \nAlexandre Pouget,  Cedric Deffayet,  Te\"ence J.  Sejnowski \nc:::======:::::»  •",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/d045c59a90d7587d8d671b5f5aec4e7c-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/d045c59a90d7587d8d671b5f5aec4e7c-Bibtex.bib",
            "SUPP": ""
        }
    },
    "114": {
        "TITLE": "A Lagrangian Formulation For Optical Backpropagation Training In Kerr-Type Optical Networks",
        "AUTHORS": "James Edward Steck, Steven R. Skinner, Alvaro A. Cruz-Cabrara, Elizabeth C. Behrman",
        "ABSTRACT": "A training method based on a form of continuous spatially distributed  optical error back-propagation is presented for an all optical network  composed of nondiscrete neurons and weighted interconnections. The all  optical network is feed-forward and is composed of thin layers of a Kerr(cid:173) type self focusing/defocusing nonlinear optical material. The training  method is derived from a Lagrangian formulation of the constrained  minimization of the network error at the output. This leads to a  formulation that describes training as a calculation of the distributed error  of the optical signal at the output which is then reflected back through the  device to assign a spatially distributed error to the internal layers. This  error is then used to modify the internal weighting values. Results from  several computer simulations of the training are presented, and a simple  optical table demonstration of the network is discussed. \n772 \nElizabeth C. Behrman \n1 KERR TYPE MATERIALS \nKerr-type optical networks utilize thin layers of Kerr-type nonlinear materials, in which the  index of refraction can vary within the material and depends on the amount of light striking  the material at a given location. The material index of refraction can be described by:  n(x)=no+nzI(x), where 110 is the linear index of refraction, ~ is the nonlinear coefficient, and  I(x) is the irradiance of a applied optical field as a function of position x across the material  layer (Armstrong, 1962). This means that a beam of light (a signal beam carrying  information perhaps) passing through a layer of Kerr-type material can be steered or  controlled by another beam of light which applies a spatially varying pattern of intensity  onto the Kerr-type material. Steering of light with a glass lens (having constance index of  refraction) is done by varying the thickness of the lens (the amount of material present) as  a function of position. Thus the Kerr effect can be loosely thought of as a glass lens whose  geometry and therefore focusing ability could be dynamically controlled as a function of  position across the lens. Steering in the Kerr material is accomplished by a gradient or  change in the material index of refraction which is created by a gradient in applied light  intensity. This is illustrated by the simple experiment in Figure 1 where a small weak probe  beam is steered away from a straight path by the intensity gradient of a more powerful pump  beam.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/d240e3d38a8882ecad8633c8f9c78c9b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/d240e3d38a8882ecad8633c8f9c78c9b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "115": {
        "TITLE": "Instance-Based State Identification for Reinforcement Learning",
        "AUTHORS": "R. Andrew McCallum",
        "ABSTRACT": "This paper presents instance-based state identification, an approach  to reinforcement learning and hidden state that builds disambiguat(cid:173) ing amounts of short-term memory on-line, and also learns with an  order of magnitude fewer training steps than several previous ap(cid:173) proaches. Inspired by a key similarity between learning with hidden  state and learning in continuous geometrical spaces, this approach  uses instance-based (or \"memory-based\") learning, a method that  has worked well in continuous spaces. \n1 BACKGROUND AND RELATED WORK \nWhen a robot's next course of action depends on information that is hidden from  the sensors because of problems such as occlusion, restricted range, bounded field  of view and limited attention, the robot suffers from hidden state. More formally,  we say a reinforcement learning agent suffers from the hidden state problem if the  agent's state representation is non-Markovian with respect to actions and utility. \nThe hidden state problem arises as a case of perceptual aliasing: the mapping be(cid:173) tween states of the world and sensations of the agent is not one-to-one [Whitehead,  1992]. If the agent's perceptual system produces the same outputs for two world  states in which different actions are required, and if the agent's state representation  consists only of its percepts, then the agent will fail to choose correct actions. Note  that even if an agent's state representation includes some internal state beyond its \n378 \nR. Andrew McCallum \nimmediate percepts, the agent can still suffer from hidden state if it does not keep  enough internal state to uncover the non-Markovian-ness of its environment. \nOne solution to the hidden state problem is simply to avoid passing through the  aliased states. This is the approach taken in Whitehead's Lion algorithm [White(cid:173) head, 1992]. Whenever the agent finds a state that delivers inconsistent reward, it  sets that state's utility so low that the policy will never visit it again. The success  of this algorithm depends on a deterministic world and on the existence of a path  to the goal that consists of only unaliased states. \nOther solutions do not avoid aliased states, but do as best they can given a non(cid:173) Markovian state representation [Littman, 1994; Singh et al., 1994; Jaakkola et al.,  1995]. They involve either learning deterministic policies that execute incorrect  actions in some aliased states, or learning stochastic policies with action choice  probabilities matching the proportions of the different underlying aliased world  states. These approaches do not depend on a path of unaliased states, but they  have other limitations: when faced with many aliased states, a stochastic policy  degenerates into random walk; when faced with potentially harmful results from  incorrect actions, deterministically incorrect or probabilistically incorrect action  choice may prove too dangerous; and when faced with performance-critical tasks,  inefficiency that is proportional to the amount of aliasing may be unacceptable. \nThe most robust solution to the hidden state problem is to augment the agent's  state representation on-line so as to disambiguate the aliased states. State identi(cid:173) fication techniques uncover the hidden state information-that is, they make the  agent's internal state space Markovian. This transformation from an imperfect state  information model to a perfect state information model has been formalized in the  decision and control literature, and involves adding previous percepts and actions to  the definition of agent internal state [Bertsekas and Shreve, 1978]. By augmenting  the agent's perception with history information-.short-term memory of past per(cid:173) cepts, actions and rewards-the agent can distinguish perceptually aliased states,  and can then reliably choose correct actions from them. \nPredefined, fixed memory representations such as order n Markov models (also  known as constant-sized perception windows, linear traces or tapped-delay lines)  are often undesirable. When the length of the window is more than needed, they  exponentially increase the number of internal states for which a policy must be  stored and learned; when the length of the memory is less than needed, the agent  reverts to the disadvantages of undistinguished hidden state. Even if the agent de(cid:173) signer understands the task well enough to know its maximal memory requirements,  the agent is at a disadvantage with constant-sized windows because, for most tasks,  different amounts of memory are needed at different steps of the task. \nThe on-line memory creation approach has been adopted in several reinforcement  learning algorithms. The Perceptual Distinctions Approach [Chrisman, 1992] and  Utile Distinction Memory [McCallum, 1993] are both based on splitting states of a  finite state machine by doing off-line analysis of statistics gathered over many steps.  Recurrent-Q [Lin, 1993] is based on training recurrent neural networks. Indexed  Memory [Teller, 1994] uses genetic programming to evolve agents that use load and  store instructions on a register bank. A chief disadvantage of all these techniques  is that they require a very large number of steps for training. \nInstance-Based State Identification for Reinforcement Learning",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/d2ed45a52bc0edfa11c2064e9edee8bf-Bibtex.bib",
            "SUPP": ""
        }
    },
    "116": {
        "TITLE": "Nonlinear Image Interpolation using Manifold Learning",
        "AUTHORS": "Christoph Bregler, Stephen M. Omohundro",
        "ABSTRACT": "The problem of interpolating between specified images in an image  sequence  is  a  simple,  but  important  task  in  model-based  vision.  We  describe  an  approach based  on  the abstract  task of  \"manifold  learning\"  and present  results  on  both synthetic and real image se(cid:173) quences.  This  problem  arose  in  the  development  of a  combined  lip-reading and speech  recognition system.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/d516b13671a4179d9b7b458a6ebdeb92-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/d516b13671a4179d9b7b458a6ebdeb92-Bibtex.bib",
            "SUPP": ""
        }
    },
    "117": {
        "TITLE": "A Growing Neural Gas Network Learns Topologies",
        "AUTHORS": "Bernd Fritzke",
        "ABSTRACT": "An incremental network model is  introduced which is  able to learn  the important topological relations in a given set of input vectors by  means of a  simple Hebb-like learning rule.  In contrast to previous  approaches like the \"neural gas\"  method of Martinetz and Schulten  (1991,  1994), this model has no parameters which change over time  and is able to continue learning, adding units and connections, until  a  performance criterion has been met.  Applications of the model  include vector quantization, clustering, and interpolation.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Bibtex.bib",
            "SUPP": ""
        }
    },
    "118": {
        "TITLE": "Transformation Invariant Autoassociation with Application to Handwritten Character Recognition",
        "AUTHORS": "Holger Schwenk, Maurice Milgram",
        "ABSTRACT": "When  training  neural  networks  by  the classical  backpropagation algo(cid:173) rithm the whole problem to learn must be expressed by a set of inputs and  desired  outputs.  However,  we often  have  high-level knowledge  about  the  learning problem.  In  optical character recognition  (OCR),  for  in(cid:173) stance, we know that the classification should be invariant under a set of  transformations like rotation or translation.  We propose a new modular  classification system based on several autoassociative multilayer percep(cid:173) trons which allows the efficient incorporation of such knowledge.  Results  are reported on the NIST database of upper case handwritten letters and  compared to other approaches to the invariance problem. \n1  INCORPORATION OF EXPLICIT KNOWLEDGE \nThe  aim  of supervised  learning is  to  learn  a  mapping  between  the  input and  the  output  space from  a  set of example pairs (input,  desired output).  The classical implementation  in the domain of neural networks is the backpropagation algorithm.  If this learning set is  sufficiently representative of the underlying data distributions, one hopes that after learning,  the system is able to generalize correctly to other inputs of the same distribution. \n992 \nHolger Schwenk,  Maurice  Milgram \nIt  would  be  better to  have  more powerful techniques  to  incorporate knowledge  into  the  learning process  than  the choice of a set of examples.  The use  of additional knowledge  is  often limited  to  the  feature  extraction  module.  Besides  simple  operations  like  (size)  normalization,  we  can  find  more sophisticated  approaches  like  zernike  moments  in  the  domain of optical character recognition (OCR). In  this paper we  will  not investigate this  possibility, all discussed classifiers work directly on almost non preprocessed data (pixels). \nIn the context of OCR interest focuses  on invariance of the classifier under a  number of  given transformations (translation, rotation, ... ) of the data to classify.  In general a neural  network could extract those properties of a large enough learning set, but it is very hard to  learn and will probably take a lot of time.  In the last years  two  main approaches for this  invariance problem have been proposed:  tangent-prop and tangent-distance.  An indirect  incorporation can be achieved by boosting (Drucker, Schapire and Simard, 1993). \nIn this paper we briefly discuss these approaches and will present a new classification system  which allows the efficient incorporation of transformation invariances. \n1.1  TANGENT PROPAGATION \nThe principle of tangent-prop is to specify besides desired outputs also desired changes jJJ.  of the output vector when transforming the net input x  by the transformations tJJ.  (Simard,  Victorri, LeCun and Denker, 1992).  For this, let us define a transformation of pattern p as t(p, a) : P  --t  P  where P  is the space  of all  patterns  and a  a parameter.  Such  transformations are in  general highly  nonlinear  operations in  the pixel space P  and their analytical expressions are  seldom known.  It is  therefore favorable to use a first order approximation:",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/d707329bece455a462b58ce00d1194c9-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/d707329bece455a462b58ce00d1194c9-Bibtex.bib",
            "SUPP": ""
        }
    },
    "119": {
        "TITLE": "Learning to Play the Game of Chess",
        "AUTHORS": "Sebastian Thrun",
        "ABSTRACT": "This paper presents NeuroChess,  a program which learns to play chess from the final  outcome of games.  NeuroChess learns chess  board  evaluation functions,  represented  by artificial neural  networks.  It integrates inductive neural  network learning, temporal  differencing, and a variant of explanation-based learning.  Performance results illustrate  some of the strengths and weaknesses of this approach.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Bibtex.bib",
            "SUPP": ""
        }
    },
    "120": {
        "TITLE": "Interior Point Implementations of Alternating Minimization Training",
        "AUTHORS": "Michael Lemmon, Peter T. Szymanski",
        "ABSTRACT": "This paper presents an alternating minimization (AM) algorithm  used in the training of radial basis function and linear regressor  networks. The algorithm is a modification of a small-step interior  point method used in solving primal linear programs. The algo(cid:173) rithm has a convergence rate of O( fo,L) iterations where n is a  measure of the network size and L is a measure of the resulting  solution's accuracy. Two results are presented that specify how  aggressively the two steps of the AM may be pursued to ensure  convergence of each step of the alternating minimization.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/d79aac075930c83c2f1e369a511148fe-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/d79aac075930c83c2f1e369a511148fe-Bibtex.bib",
            "SUPP": ""
        }
    },
    "121": {
        "TITLE": "A Convolutional Neural Network Hand Tracker",
        "AUTHORS": "Steven J. Nowlan, John C. Platt",
        "ABSTRACT": "We  describe  a system that can track  a hand in a sequence of video  frames  and recognize hand gestures  in  a user-independent  manner.  The system  locates  the  hand in  each  video  frame  and  determines  if the hand is open or closed.  The tracking system is  able to track  the  hand  to  within  ±10  pixels  of its  correct  location  in  99.7%  of  the frames  from a  test set  containing video sequences  from  18  dif(cid:173) ferent  individuals captured in 18  different  room environments.  The  gesture  recognition network correctly  determines if the hand being  tracked  is  open  or  closed  in  99.1 % of the  frames  in this  test  set .  The system has been designed to operate in real time with existing  hardware.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/d93ed5b6db83be78efb0d05ae420158e-Bibtex.bib",
            "SUPP": ""
        }
    },
    "122": {
        "TITLE": "Temporal Dynamics of Generalization in Neural Networks",
        "AUTHORS": "Changfeng Wang, Santosh S. Venkatesh",
        "ABSTRACT": "This  paper  presents  a  rigorous  characterization  of how  a  general  nonlinear learning machine generalizes  during  the training process  when  it  is  trained  on  a  random  sample  using  a  gradient  descent  algorithm  based  on  reduction  of  training  error.  It is  shown,  in  particular, that best generalization performance occurs,  in general,  before  the global  minimum of the  training error  is  achieved.  The  different  roles  played  by  the  complexity of the  machine  class  and  the  complexity of the  specific  machine in the class  during learning  are  also precisely  demarcated.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/da0d1111d2dc5d489242e60ebcbaf988-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/da0d1111d2dc5d489242e60ebcbaf988-Bibtex.bib",
            "SUPP": ""
        }
    },
    "123": {
        "TITLE": "Learning Stochastic Perceptrons Under k-Blocking Distributions",
        "AUTHORS": "Mario Marchand, Saeed Hadjifaradji",
        "ABSTRACT": "We  present  a  statistical  method  that  PAC  learns  the  class  of  stochastic  perceptrons  with  arbitrary  monotonic  activation  func(cid:173) tion and weights Wi  E  {-I, 0, + I} when the probability distribution  that  generates  the input examples  is  member of a  family  that we  call k-blocking distributions.  Such distributions represent an impor(cid:173) tant step beyond the case where each input variable is statistically  independent  since the 2k-blocking  family  contains  all  the  Markov  distributions of order k.  By stochastic  percept ron we  mean  a  per(cid:173) ceptron which,  upon presentation of input vector x, outputs 1 with  probability  fCLJi WiXi  - B).  Because the same algorithm works  for  any  monotonic  (nondecreasing  or  nonincreasing)  activation  func(cid:173) tion  f  on  Boolean  domain,  it  handles  the  well  studied  cases  of  sigmolds  and the  \"usual\"  radial basis functions.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/dd8eb9f23fbd362da0e3f4e70b878c16-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/dd8eb9f23fbd362da0e3f4e70b878c16-Bibtex.bib",
            "SUPP": ""
        }
    },
    "124": {
        "TITLE": "Recurrent Networks: Second Order Properties and Pruning",
        "AUTHORS": "Morten With Pedersen, Lars Kai Hansen",
        "ABSTRACT": "Second  order  properties  of cost  functions  for  recurrent  networks  are investigated.  We  analyze a layered fully recurrent  architecture,  the  virtue  of this  architecture  is  that  it features  the  conventional  feedforward architecture as a special case.  A detailed description of  recursive  computation of the full  Hessian of the network cost func(cid:173) tion is  provided.  We  discuss  the possibility of invoking simplifying  approximations of the Hessian and show how weight decays iron the  cost function and thereby greatly assist training. We present tenta(cid:173) tive  pruning results,  using  Hassibi  et  al.'s  Optimal Brain  Surgeon,  demonstrating  that  recurrent  networks  can  construct  an  efficient  internal memory. \n1  LEARNING IN RECURRENT NETWORKS \nTime  series  processing  is  an  important  application  area for  neural  networks  and  numerous architectures have been suggested, see e.g. (Weigend and Gershenfeld, 94).  The most general structure is a fully recurrent network and it may be adapted using  Real Time Recurrent Learning (RTRL) suggested by  (Williams and Zipser,  89).  By  invoking a  recurrent  network,  the  length of the network memory can  be  adapted to  the  given  time series,  while  it is  fixed  for  the  conventional  lag-space net  (Weigend  et  al.,  90).  In  forecasting,  however,  feedforward  architectures  remain  the  most  popular structures; only few  applications are reported based on the Williams&Zipser  approach.  The main difficulties experienced  using  RTRL are slow  convergence  and \n674 \nMorten  With Pedersen,  Lars Kai Hansen \nlack  of generalization.  Analogous  problems  in  feedforward  nets  are  solved  using  second  order  methods  for  training  and  pruning  (LeCun  et  al.,  90;  Hassibi  et  al.,  92;  Svarer  et  al.,  93).  Also,  regularization  by  weight  decay  significantly  improves  training and generalization.  In this work we initiate the investigation of second order  properties for  RTRL; a detailed  calculation scheme for  the cost function  Hessian  is  presented,  the importance of weight decay is demonstrated, and preliminary pruning  results  using Hassibi et al.'s Optimal Brain Surgeon  (OBS)  are presented.  We find  that the recurrent  network  discards  the  available lag space  and constructs  its own  efficient  internal memory. \n1.1  REAL  TIME RECURRENT  LEARNING \nThe fully  connected  feedback  nets studied  by  Williams&Zipser operate like a  state  machine, computing the outputs from the internal units according to a state vector  z(t)  containing previous external inputs and internal unit outputs.  Let  x(t) denote  a  vector  containing the external  inputs  to  the net  at time t,  and let y(t)  denote  a  vector  containing the outputs of the  units  in  the  net.  We now  arrange  the  indices  on  x  and y  so  that the elements of z(t)  can be defined  as \n,  k  E I  ,  k E U \nwhere I  denotes the set of indices for which  Zk  is  an input, and U denotes  the set of  indices for  which  Zk  is  the output of a unit in the net.  Thresholds are implemented  using  an  input  permanently  clamped  to  unity.  The  k'th  unit  in  the  net  is  now  updated according to \nwhere Wkj  denotes the weight to unit k from input/unit j  and \"'0 is the activation  function  of the k'th unit.  When  used  for  time  series  prediction,  the  input  vector  (excluding  threshold)  is  usually defined  as  x(t) =  [x(t), . .. , x(t - L + 1)]  where  L denotes  the dimension of  the lag  space.  One of the units in the net is  designated to be the output unit Yo,  and  its activating function 10  is  often chosen  to be linear in order  to allow for  arbitrary  dynamical range.  The prediction of x(t + 1)  is x(t + 1)  = lo[so(t»).  Also,  if the first  prediction  is  at t =  1,  the first  example is  presented  at t = 0  and we 'set y(O)  =  O.  We  analyse here  a modification of the standard Williams&Zipser construction  that  is  appropriate for forecasting purposes.  The studied architecture is  layered.  Firstly,  we  remove  the external  inputs from  the linear  output unit in  order  to prevent  the  network from getting  trapped  in a linear mode.  The output then reads \nx(t + 1) = Yo(t + 1) = L WojYj(t)  + Wthres,o",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/df6d2338b2b8fce1ec2f6dda0a630eb0-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/df6d2338b2b8fce1ec2f6dda0a630eb0-Bibtex.bib",
            "SUPP": ""
        }
    },
    "125": {
        "TITLE": "Unsupervised Classification of 3D Objects from 2D Views",
        "AUTHORS": "Satoshi Suzuki, Hiroshi Ando",
        "ABSTRACT": "This paper presents an unsupervised learning scheme for categorizing  3D  objects  from  their  2D  projected images.  The  scheme  exploits  an  auto-associative network's ability to encode each view of a single object  into a representation that indicates its view direction.  We propose two  models that employ different classification mechanisms; the first model  selects an auto-associative network whose recovered view best matches  the input view, and the second model is based on a modular architecture  whose  additional  network classifies  the  views  by  splitting  the  input  space  nonlinearly.  We demonstrate  the  effectiveness  of the  proposed  classification models through simulations using 3D wire-frame objects.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/e205ee2a5de471a70c1fd1b46033a75f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/e205ee2a5de471a70c1fd1b46033a75f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "126": {
        "TITLE": "Hyperparameters Evidence and Generalisation for an Unrealisable Rule",
        "AUTHORS": "Glenn Marion, David Saad",
        "ABSTRACT": "Using a statistical mechanical formalism we  calculate the evidence,  generalisation  error  and  consistency  measure  for  a  linear  percep(cid:173) tron  trained  and  tested  on  a  set  of examples  generated  by  a  non  linear  teacher.  The teacher  is  said  to be  unrealisable  because  the  student  can  never  model it without error.  Our model allows  us  to  interpolate between the known case of a linear teacher,  and an un(cid:173) realisable, nonlinear teacher.  A comparison of the hyperparameters  which  maximise the evidence  with  those  that optimise the perfor(cid:173) mance measures  reveals  that,  in  the  non-linear  case,  the  evidence  procedure is a misleading guide to optimising performance.  Finally,  we explore the extent to which the evidence procedure is  unreliable  and find  that, despite being sub-optimal, in  some circumstances  it  might be  a  useful  method for  fixing  the hyperparameters.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/e6cb2a3c14431b55aa50c06529eaa21b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/e6cb2a3c14431b55aa50c06529eaa21b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "127": {
        "TITLE": "Adaptive Elastic Input Field for Recognition Improvement",
        "AUTHORS": "Minoru Asogawa",
        "ABSTRACT": "For machines to perform classification tasks, such as speech and  character recognition, appropriately handling deformed patterns  is a key to achieving high performance. The authors presents a  new type of classification system, an Adaptive Input Field Neu(cid:173) ral Network (AIFNN), which includes a simple pre-trained neural  network and an elastic input field attached to an input layer. By  using an iterative method, AIFNN can determine an optimal affine  translation for an elastic input field to compensate for the original  deformations. The convergence of the AIFNN algorithm is shown.  AIFNN is applied for handwritten numerals recognition. Conse(cid:173) quently, 10.83% of originally misclassified patterns are correctly  categorized and total performance is improved, without modifying  the neural network.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/e744f91c29ec99f0e662c9177946c627-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/e744f91c29ec99f0e662c9177946c627-Bibtex.bib",
            "SUPP": ""
        }
    },
    "128": {
        "TITLE": "A Model for Chemosensory Reception",
        "AUTHORS": "Rainer Malaka, Thomas Ragg, Martin Hammer",
        "ABSTRACT": "A new  model for chemosensory reception is presented.  It models reacti(cid:173) ons between odor molecules and receptor proteins and the activation of  second  messenger  by receptor proteins.  The mathematical  formulation  of the  reaction  kinetics  is  transformed  into an  artificial  neural  network  (ANN). The resulting feed-forward network provides a powerful means  for parameter fitting by applying learning algorithms. The weights of the  network corresponding to chemical parameters can be trained by presen(cid:173) ting experimental data.  We demonstrate the simulation capabilities of the  model with experimental data from honey bee chemosensory neurons.  It  can be shown that our model is sufficient to rebuild the observed data and  that simpler models are not able to do this task.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/e820a45f1dfc7b95282d10b6087e11c0-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/e820a45f1dfc7b95282d10b6087e11c0-Bibtex.bib",
            "SUPP": ""
        }
    },
    "129": {
        "TITLE": "A Real Time Clustering CMOS Neural Engine",
        "AUTHORS": "Teresa Serrano-Gotarredona, Bernabé Linares-Barranco, José Luis Huertas",
        "ABSTRACT": "We  describe  an  analog  VLSI  implementation  of the ARTI  algorithm  (Carpenter, 1987). A prototype chip has been fabricated in a standard low  cost  1.5~m double-metal single-poly CMOS process. It has a die area of  lcm2  and  is  mounted in  a  12O-pins PGA package.  The chip realizes  a  modified version of the original ARTI  architecture.  Such modification  has been shown to preserve all computational properties of the original  algorithm  (Serrano,  1994a),  while  being  more  appropriate  for  VLSI  realizations. The chip implements an ARTI network with 100 F 1 nodes  and 18 F2 nodes. It can therefore cluster 100 binary pixels input patterns  into up to 18 different categories. Modular expansibility of the system is  possible  by  assembling  an  NxM  array  of  chips  without  any  extra  interfacing circuitry, resulting in an F 1 layer with  l00xN nodes, and an  F2  layer  with  18xM nodes.  Pattern  classification  is  performed  in  less  than  1.8~s, which  means  an  equivalent  computing  power of 2.2x109  connections and connection-updates per second. Although internally the  chip is analog in nature, it interfaces to the outside world through digital  signals, thus having a true asynchrounous digital behavior. Experimental  chip  test  results  are  available,  which  have  been  obtained  through  test  equipments for digital chips.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/ec5aa0b7846082a2415f0902f0da88f2-Bibtex.bib",
            "SUPP": ""
        }
    },
    "130": {
        "TITLE": "Learning from queries for maximum information gain in imperfectly learnable problems",
        "AUTHORS": "Peter Sollich, David Saad",
        "ABSTRACT": "In  supervised  learning,  learning  from  queries  rather  than  from  random  examples  can  improve  generalization  performance  signif(cid:173) icantly.  We  study  the  performance of query  learning for  problems  where  the  student  cannot learn  the  teacher  perfectly,  which  occur  frequently  in  practice.  As  a  prototypical scenario of this  kind,  we  consider  a  linear  perceptron  student  learning a  binary  perceptron  teacher.  Two kinds of queries for  maximum information gain,  i.e.,  minimum entropy,  are  investigated:  Minimum  student  space  en(cid:173) tropy  (MSSE)  queries,  which  are  appropriate  if the  teacher  space  is  unknown,  and  minimum teacher  space entropy  (MTSE)  queries,  which can be used  if the teacher space is  assumed to be known, but  a student of a simpler form  has deliberately  been  chosen.  We  find  that  for  MSSE  queries,  the  structure  of the  student  space  deter(cid:173) mines  the  efficacy  of query  learning,  whereas  MTSE  queries  lead  to  a  higher  generalization  error  than  random examples,  due  to  a  lack of feedback about the progress of the student in the way queries  are  selected.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/eeb69a3cb92300456b6a5f4162093851-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/eeb69a3cb92300456b6a5f4162093851-Bibtex.bib",
            "SUPP": ""
        }
    },
    "131": {
        "TITLE": "Factorial Learning by Clustering Features",
        "AUTHORS": "Joshua B. Tenenbaum, Emanuel V. Todorov",
        "ABSTRACT": "We introduce a novel algorithm for factorial learning, motivated  by segmentation problems in computational vision, in which the  underlying factors correspond to clusters of highly correlated input  features. The algorithm derives from a new kind of competitive  clustering model, in which the cluster generators compete to ex(cid:173) plain each feature of the data set and cooperate to explain each  input example, rather than competing for examples and cooper(cid:173) ating on features, as in traditional clustering algorithms. A natu(cid:173) ral extension of the algorithm recovers hierarchical models of data  generated from multiple unknown categories, each with a differ(cid:173) ent, multiple causal structure. Several simulations demonstrate  the power of this approach.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/ef4e3b775c934dada217712d76f3d51f-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/ef4e3b775c934dada217712d76f3d51f-Bibtex.bib",
            "SUPP": ""
        }
    },
    "132": {
        "TITLE": "Generalization in Reinforcement Learning: Safely Approximating the Value Function",
        "AUTHORS": "Justin A. Boyan, Andrew W. Moore",
        "ABSTRACT": "A  straightforward  approach  to  the  curse  of  dimensionality  in  re(cid:173) inforcement  learning  and  dynamic  programming  is  to  replace  the  lookup table with a generalizing function approximator such as a neu(cid:173) ral net.  Although this has been successful in the domain of backgam(cid:173) mon,  there  is  no  guarantee  of convergence.  In  this  paper,  we  show  that the combination of dynamic programming and function approx(cid:173) imation  is  not  robust,  and  in  even  very  benign  cases,  may produce  an  entirely  wrong  policy.  We  then  introduce  Grow-Support,  a  new  algorithm which is safe from divergence yet can still reap the benefits  of successful  generalization .",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/ef50c335cca9f340bde656363ebd02fd-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/ef50c335cca9f340bde656363ebd02fd-Bibtex.bib",
            "SUPP": ""
        }
    },
    "133": {
        "TITLE": "A Rapid Graph-based Method for Arbitrary Transformation-Invariant Pattern Classification",
        "AUTHORS": "Alessandro Sperduti, David G. Stork",
        "ABSTRACT": "We  present  a  graph-based  method  for  rapid,  accurate  search  through prototypes for  transformation-invariant pattern classifica(cid:173) tion.  Our method has in theory the same recognition accuracy as  other recent  methods  based  on  ''tangent  distance\"  [Simard et al.,  1994],  since it uses the same categorization rule.  Nevertheless ours  is  significantly  faster  during  classification  because  far  fewer  tan(cid:173) gent  distances  need  be  computed.  Crucial  to  the success  of our  system  are  1)  a  novel  graph  architecture in  which  transformation  constraints and geometric  relationships  among  prototypes are en(cid:173) coded  during learning,  and 2)  an improved graph search criterion,  used during classification.  These architectural insights are applica(cid:173) ble to a wide range of problem domains.  Here we demonstrate that  on  a  handwriting  recognition task,  a  basic implementation of our  system  requires  less  than  half the  computation  of the  Euclidean  sorting method.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/f33ba15effa5c10e873bf3842afb46a6-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/f33ba15effa5c10e873bf3842afb46a6-Bibtex.bib",
            "SUPP": ""
        }
    },
    "134": {
        "TITLE": "A solvable connectionist model of immediate recall of ordered lists",
        "AUTHORS": "Neil Burgess",
        "ABSTRACT": "A  model  of short-term  memory for  serially  ordered  lists  of verbal  stimuli is proposed as an implementation of the 'articulatory loop'  thought  to  mediate  this  type  of memory  (Baddeley,  1986).  The  model predicts the presence  of a  repeatable time-varying 'context'  signal  coding  the  timing  of items'  presentation  in  addition  to  a  store  of phonological information and a  process  of serial rehearsal.  Items are associated with context nodes and phonemes by Hebbian  connections showing both short and long term plasticity.  Items are  activated  by  phonemic input  during  presentation  and reactivated  by context and phonemic feedback  during output.  Serial selection  of items  occurs  via a  winner-take-all  interaction  amongst  items,  with  the  winner  subsequently  receiving  decaying  inhibition.  An  approximate analysis  of error  probabilities due  to  Gaussian  noise  during  output  is  presented.  The  model  provides  an  explanatory  account of the  probability of error as  a  function of serial  position,  list  length,  word  length,  phonemic similarity,  temporal  grouping,  item and list familiarity,  and is proposed  as the starting point for  a  model of rehearsal and vocabulary acquisition.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/f47d0ad31c4c49061b9e505593e3db98-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/f47d0ad31c4c49061b9e505593e3db98-Bibtex.bib",
            "SUPP": ""
        }
    },
    "135": {
        "TITLE": "H∞ Optimal Training Algorithms and their Relation to Backpropagation",
        "AUTHORS": "Babak Hassibi, Thomas Kailath",
        "ABSTRACT": "Abstract Unavailable",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/f57a2f557b098c43f11ab969efe1504b-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/f57a2f557b098c43f11ab969efe1504b-Bibtex.bib",
            "SUPP": ""
        }
    },
    "136": {
        "TITLE": "Using a neural net to instantiate a deformable model",
        "AUTHORS": "Christopher K. I. Williams, Michael Revow, Geoffrey E. Hinton",
        "ABSTRACT": "Deformable models are  an attractive approach to recognizing  non(cid:173) rigid objects which have considerable within class variability.  How(cid:173) ever,  there  are  severe  search  problems  associated  with  fitting  the  models to data.  We show that by using neural networks to provide  better starting points, the search time can be significantly reduced.  The method is  demonstrated on a  character  recognition task. \nIn previous work  we  have  developed  an approach to handwritten character recogni(cid:173) tion  based  on  the use  of deformable models  (Hinton,  Williams and  Revow,  1992a;  Revow,  Williams and Hinton,  1993).  We have obtained good performance with this  method, but a major problem is that the search  procedure for fitting each model to  an  image is  very  computationally intensive,  because  there  is  no efficient  algorithm  (like  dynamic programming) for  this  task.  In this paper  we  demonstrate that it is  possible  to  \"compile  down\"  some of the  knowledge  gained  while  fitting  models  to  data to obtain better  starting points that significantly reduce  the search  time. \n1  DEFORMABLE MODELS FOR DIGIT RECOGNITION \nThe basic idea in using deformable models for digit recognition is that each digit has  a  model, and  a  test  image is  classified  by finding  the model which  is  most likely to  have generated it.  The quality of the match between model and test image depends  on the  deformation of the model, the amount of ink that is  attributed to noise  and  the  distance of the  remaining ink from  the  deformed model. \n·Current  address:  Department of Computer  Science  and  Applied  Mathematics,  Aston \nUniversity,  Birmingham  B4  7ET,  UK. \n966 \nChristopher K.  T.  Williams,  Michael  D.  Revow,  Geoffrey  E.  Hinton \nMore  formally,  the two  important terms in  assessing  the fit  are  the prior  probabil(cid:173) ity  distribution  for  the  instantiation  parameters of a  model  (which  penalizes  very  distorted models),  and the imaging model that characterizes  the probability distri(cid:173) bution over  possible images given  the instantiated  model l .  Let  I  be an  image,  M  be a  model  and  z  be  its instantiation parameters.  Then the evidence  for  model M  is  given  by \nP(IIM) = J P(zIM)P(IIM, z)dz \n(1) \nThe first  term in the integrand is the prior on the instantiation parameters and the  second  is  the  imaging model  i.e.,  the  likelihood of the  data given  the  instantiated  model.  P(MII) is  directly  proportional to P(IIM), as  we  assume  a  uniform prior  on each  digit.  Equation 1 is  formally correct,  but if z has more than a few  dimensions the evalua(cid:173) tion of this integral is very  computationally intensive.  However,  it is often  possible  to make an  approximation based on  the  assumption  that the integrand is strongly  peaked around  a  (global)  maximum value  z*.  In this case,  the evidence  can  be  ap(cid:173) proximated by the highest  peak of the integrand times a  volume factor  ~(zII, M),  which  measures  the sharpness of the  peak2 . \nP(IIM) ~ P(zIM)P(Ilz, M)~(zII, M) \n(2)  By  Taylor expanding  around  z*  to second  order  it  can  be  shown  that the  volume  factor  depends  on  the  determinant of the  Hessian  of 10gP(z, 11M)  .  Taking logs  of equation  2,  defining  EdeJ  as  the  negative  log  of P(z*IM),  and  EJit  as  the  cor(cid:173) responding  term  for  the  imaging model,  then  the  aim of the  search  is  to  find  the  minimum of E tot  =  EdeJ  + EJit .  Of course  the  total energy  will  have  many local  minima;  for  the  character  recognition  task  we  aim  to find  the  global  minimum by  using  a  continuation method (see  section  1.2). \n1.1  SPLINES, AFFINE TRANSFORMS  AND  IMAGING  MODELS \nThis section  presents  a  brief overview  of our  work  on  using  deformable models for  digit recognition.  For a fuller  treatment, see  Revow,  Williams and  Hinton (1993) . \nEach  digit is  modelled by  a  cubic  B-spline  whose  shape  is  determined  by  the  posi(cid:173) tions of the control points in the object-based frame.  The models have eight control  points,  except  for  the  one  model which  has  three,  and  the  seven  model which  has  five.  To  generate  an  ideal  example of a  digit  the  control  points  are  positioned  at  their  \"home\"  locations.  Deformed  characters  are  produced  by  perturbing  the  con(cid:173) trol  points  away  from  their  home  locations.  The  home  locations  and  covariance  matrix for  each  model were  adapted in order to improve the  performance. \nThe deformation energy only penalizes shape  deformations.  Affine  transformations,  i.e., translation, rotation,  dilation, elongation, and shear,  do  not change the under(cid:173) lying shape of an  object so  we  want the  deformation energy  to be  invariant under  them .  We  achieve  this  by  giving  each  model  its  own  \"object-based  frame\"  and  computing the deformation energy  relative to this frame. \nlThis framework  has been  used  by many  authors,  e.g.  Grenander  et  al (1991) .  2The  Gaussian  approximation  has  been  popularized  in  the  neural  net  community  by",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/fba9d88164f3e2d9109ee770223212a0-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/fba9d88164f3e2d9109ee770223212a0-Bibtex.bib",
            "SUPP": ""
        }
    },
    "137": {
        "TITLE": "Grammar Learning by a Self-Organizing Network",
        "AUTHORS": "Michiro Negishi",
        "ABSTRACT": "This  paper presents the design and simulation results of a  self(cid:173) organizing neural network which induces a grammar from exam(cid:173) ple sentences. Input sentences are generated from a simple phrase  structure grammar including number agreement,  verb  transitiv(cid:173) ity,  and recursive  noun phrase construction rules.  The  network  induces a grammar explicitly in the form of symbol categorization  rules and phrase structure rules. \n1  Purpose and related works \nThe purpose of this research is to show that a self-organizing network with a certain  structure can acquire syntactic knowledge from only positive (i.e.  grammatical)  data,  without requiring  any initial  knowledge  or  external  teachers  that correct  errors.  There has been research on supervised neural network models of language acquisi(cid:173) tion tasks [Elman, 1991, Miikkulainen and Dyer, 1988, John and McClelland, 1988].  Unlike these supervised models, the current model self-organizes word and phrasal  categories and phrase construction rules through mere exposure to input sentences,  without any artificially defined  task goals.  There  also have been self-organizing  models of language  acquisition  tasks [Ritter and Kohonen, 1990,  Scholtes, 1991].  Compared to these models, the current model acquires phrase structure rules in  more explicit forms, and it learns wider and more structured contexts, as will be  explained below. \n2  Network Structure and Algorithm \nThe design of the current network is motivated by the observation that humans  have  the  ability  to  handle  a  frequently  occurring  sequence of symbols (chunk)  as an unit of information [Grossberg, 1978, Mannes, 1993].  The network consists  of two parts:  classification networks and production networks (Figure 1).  The  classification networks categorize words and phrases, and the production networks \n28",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/fe7ee8fc1959cc7214fa21c4840dff0a-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/fe7ee8fc1959cc7214fa21c4840dff0a-Bibtex.bib",
            "SUPP": ""
        }
    },
    "138": {
        "TITLE": "Coarse-to-Fine Image Search Using Neural Networks",
        "AUTHORS": "Clay Spence, John C. Pearson, Jim Bergen",
        "ABSTRACT": "The  efficiency  of image  search  can  be  greatly  improved  by  using  a  coarse-to-fine search strategy with a  multi-resolution image representa(cid:173) tion. However,  if the resolution is so low that the objects have few  dis(cid:173) tinguishing  features,  search  becomes  difficult.  We  show  that  the  performance of search at such low resolutions can be improved by using  context information, i.e., objects visible at low-resolution which are not  the objects of interest but are associated with them. The networks can be  given explicit context information as inputs, or they can learn to detect  the context objects, in which case the user does not have to be aware of  their existence. We also use Integrated Feature Pyramids, which repre(cid:173) sent high-frequency  information  at low  resolutions.  The use of multi(cid:173) resolution search techniques allows us to combine information about the  appearance of the objects on many scales in an efficient way.  A natural  fOlm  of exemplar selection also arises from these  techniques.  We illus(cid:173) trate these ideas by training hierarchical systems of neural  networks to  find clusters of buildings in aerial photographs of farmland.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/fec8d47d412bcbeece3d9128ae855a7a-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/fec8d47d412bcbeece3d9128ae855a7a-Bibtex.bib",
            "SUPP": ""
        }
    },
    "139": {
        "TITLE": "ICEG Morphology Classification using an Analogue VLSI Neural Network",
        "AUTHORS": "Richard Coggins, Marwan A. Jabri, Barry Flower, Stephen Pickard",
        "ABSTRACT": "An  analogue  VLSI  neural  network  has  been  designed  and  tested  to perform cardiac morphology classification tasks.  Analogue tech(cid:173) niques were chosen to meet the strict power and area requirements  of an Implantable Cardioverter Defibrillator (ICD) system.  The ro(cid:173) bustness  of the neural network  architecture  reduces  the impact of  noise,  drift  and offsets  inherent  in  analogue approaches.  The net(cid:173) work  is a  10:6:3 multi-layer percept ron  with on  chip  digital weight  storage,  a  bucket  brigade input  to feed  the  Intracardiac  Electro(cid:173) gram  (ICEG)  to  the  network  and  has  a  winner  take  all  circuit  at  the  output.  The  network  was  trained  in  loop  and  included  a  commercial ICD in the signal processing path.  The system has suc(cid:173) cessfully distinguished arrhythmia for different patients with better  than 90%  true positive and true negative detections for  dangerous  rhythms which cannot be detected  by present  ICDs.  The chip  was  implemented in 1.2um CMOS and consumes less than 200n W max(cid:173) imum average power in an area of 2.2  x 2.2mm2.",
        "CONFERENCE": "Advances in Neural Information Processing Systems 7  (NIPS 1994)",
        "ABBR": "NeurIPS",
        "TRACK": "Main Conference Track",
        "LINKS": {
            "PAPER": "https://papers.nips.cc/paper_files/paper/1994/file/fed33392d3a48aa149a87a38b875ba4a-Paper.pdf",
            "BIBTEX": "https://papers.nips.cc/paper_files/paper/1994/file/fed33392d3a48aa149a87a38b875ba4a-Bibtex.bib",
            "SUPP": ""
        }
    }
}