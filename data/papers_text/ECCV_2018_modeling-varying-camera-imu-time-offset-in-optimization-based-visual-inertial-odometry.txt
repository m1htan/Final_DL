Modeling Varying Camera-IMU Time Oﬀset in
Optimization-Based Visual-Inertial Odometry
Yonggen Ling, Linchao Bao, Zequn Jie, Fengming Zhu, Ziyang Li,
Shanmin Tang, Yongsheng Liu, Wei Liu, and Tong Zhang
Tencent AI Lab, China
ylingaa@connect.ust.hk,{linchaobao,zequn.nus,fridazhu}@gmail.com,
{tzeyangli,mickeytang,kakarliu}@tencent.com, wl2223@columbia.edu,
tongzhang@tongzhang-ml.org
Abstract. Combining cameras and inertial measurement units (IMUs)
has been proven eﬀective in motion tracking, as these two sensing modal-
ities oﬀer complementary characteristics that are suitable for fusion.
While most works focus on global-shutter cameras and synchronized
sensor measurements, consumer-grade devices are mostly equipped with
rolling-shutter cameras and suﬀer from imperfect sensor synchronization.
In this work, we propose a nonlinear optimization-based monocular vi-
sual inertial odometry (VIO) with varying camera-IMU time oﬀset mod-
eled as an unknown variable. Our approach is able to handle the rolling-
shutter eﬀects and imperfect sensor synchronization in a uniﬁed way.
Additionally, we introduce an eﬃcient algorithm based on dynamic pro-
gramming and red-black tree to speed up IMU integration over variable-
length time intervals during the optimization. An uncertainty-aware ini-
tialization is also presented to launch the VIO robustly. Comparisons
with state-of-the-art methods on the Euroc dataset and mobile phone
data are shown to validate the eﬀectiveness of our approach.
Keywords: Visual-Inertial Odometry, Online Temporal Camera-IMU
Calibration, Rolling Shutter Cameras.
1
Introduction
Online, robust, and accurate localization is the foremost important component
for many applications, such as autonomous navigation of mobile robots, on-
line augmented reality, and real-time localization-based service. A monocular
VIO that consists of one IMU and one camera is particularly suitable for this
task as these two sensors are cheap, ubiquitous, and complementary. However,
a VIO works only if both visual and inertial measurements are aligned spatially
and temporally. This requires that both sensor measurements are synchronized
and sensor extrinsics between sensors are known. While online sensor extrinsic
calibration has gained lots of discussions in recent works, VIO with inperfect
synchronization is less explored. Historically, some works [12, 14] calibrate the
sensor time oﬀsets oﬄine, and assume that these parameters are not changed
2
Ling et al.
Sparse Features
Keyframe Pose & Velocity & Bias
Inertial Constraint
Dynamic Visual Constraint
t
Keyframe Window
Camera-IMU Extrinsic
Camera-IMU Time Offset
Non-keyframe Pose & Velocity & Bias
Non-KeyframeWindow
Fig. 1: The graph representation of our model. All variables (circles) and con-
straints (squares) in both the keyframe window and non-keyframe window are
involved in the optimization. Note that modeling the camera-IMU time oﬀset for
each frame raises computational diﬃculties during the optimization, since the
computation of visual constraints depends on the estimated time oﬀset. In other
words, the visual constraints in our model are “dynamic” due to the varying
camera-IMU time oﬀset.
in next runs. In real cases, time oﬀsets change over time due to the variation
of the system processing payload and sensor jitter. Other works [18, 10, 21] cali-
brate sensor time oﬀsets online in an extended Kalman ﬁlter (EKF) framework.
However, these methods suﬀer from the inherent drawbacks of ﬁltering based
approaches. They require a good prior about the initial system state (such as
poses, velocities, biases, the camera-IMU extrinsic/time oﬀsets) such that the
estimation at each update step converges close to the global minimum. In con-
trast to ﬁltering based approaches, nonlinear optimization-based methods [13,
17, 40, 4] iteratively re-linearize all nonlinear error costs from visual and iner-
tial constraints to better treat the underlying nonlinearity, leading to increased
tracking robustness and accuracy. However, introducing time oﬀsets in a nonlin-
ear optimization framework is non-trivial since the visual constraints are varying
as they depend on the estimated time oﬀsets that are varied between itera-
tions. Another critical problem of VIO is the use of rolling-shutter cameras. Un-
like global-shutter cameras that capture all rows of pixels at one time instant,
rolling-shutter cameras capture each row of pixels at a diﬀerent time instant.
The rolling-shutter eﬀect on captured images causes a signiﬁcant geometry dis-
tortion if the system movement is fast. Without taking the rolling-shutter eﬀect
into account, the estimation performance degrades rapidly. Unfortunately, most
consumer-grade cameras (such as cameras on mobile phones) are rolling-shutter
cameras. If we optimize camera poses at every readout time of rolling-shutter
cameras, the computational complexity will be intractable.
To the best of our knowledge, we are the ﬁrst to propose a nonlinear optimiza-
tion-based VIO to overcome the diﬃculties mentioned above. The graph rep-
resentation of our model is shown in Fig. 1. Diﬀerent from prior VIO algo-
rithms based on nonlinear optimization, we incorporate an unknown, dynami-
cally changing time oﬀset for each camera image (shown as yellow circle in Fig.
Modeling Varying Camera-IMU Time Oﬀset in Optimization-Based VIO
3
1). The time oﬀset is jointly optimized together with other variables like poses,
velocities, biases, and camera-IMU extrinsics. We show that by modeling the
time oﬀset as a time-varying variable, imperfect camera-IMU synchronization
and rolling-shutter eﬀects can be handled in a uniﬁed formulation (Sect. 4.1
and Sect. 4.2). We derive the Jacobians involved in the optimization after in-
troducing the new variable (Sect. 4.3), and show that the varying time oﬀset
brings computational challenges of pose computation over variable-length time
intervals. An eﬃcient algorithm based on dynamic programming and red-black
tree is proposed to ease these diﬃculties (Sect. 4.4). Finally, since the nonlinear
optimization is based on linearization, an initial guess is required for optimiza-
tion bootstrap. A poor initialization may lead to a decrease of VIO robustness
and accuracy. To improve the robustness of the system bootstrap, we present an
initialization scheme, which takes the uncertainty of sensor measurements into
account and better models the underlying sensor noises. Main contributions of
this paper are as follows:
– We propose a nonlinear optimization-based VIO with varying camera-IMU
time oﬀset modeled as an unknown variable, to deal with the rolling-shutter
eﬀects and online temporal camera-IMU calibration in a uniﬁed framework.
– We design an eﬃcient algorithm based on dynamic programming and red-
black tree to speed up the IMU integration over variable-length time inter-
vals, which is needed during optimization.
– We introduce an uncertainty-aware initialization scheme to improve the ro-
bustness of the VIO bootstrap.
Qualitative and quantitative results on the Euroc dataset with simulated camera-
IMU time oﬀsets and real-world mobile phone data are shown to demonstrate
the eﬀectiveness of the proposed method.
2
Related Work
The idea of VIO goes back at least to the work [35] proposed by Roumeliotis et
al. based on ﬁltering and the work [13] proposed by Jung and Taylor based on
nonlinear optimization. Subsequently, lots of work have been published based on
an exemplar implementation of ﬁltering based approaches, called EKF [33, 19,
11, 27]. EKFs predict latest motions using IMU measurements and perform up-
dates according to the reprojection errors from visual measurements. To bound
the algorithmic complexity, many works follow a loosely coupled fashion [38, 26,
33, 24, 25, 23]. Relative poses are ﬁrstly estimated by IMU propagations as well as
vision-only structure from motion algorithms separately. They are then fused to-
gether for motion tracking. Alternatively, approaches in a tightly coupled fashion
optimize for estimator states using raw measurements from IMUs and cameras
[27, 19]. They take the relations among internal states of diﬀerent sensors into
account, thus achieving higher estimation accuracy than loosely coupled meth-
ods at the cost of a higher computational complexity. Additionally, to beneﬁt
from increased accuracy oﬀered by relinearization, nonlinear optimization-based
4
Ling et al.
methods iteratively minimize errors from both inertial measurements and visual
measurements [17, 40, 4]. The main drawback of nonlinear optimization-based
methods is their high computational complexity due to repeated linearizations,
which can be lessened by limiting the variables to optimize and utilizing struc-
tural sparsity of the visual-inertial problem [17].
Recent approaches on VIOs consider the problem of spatial or temporal
camera-IMU calibration. The camera-IMU relative transformation is calibrated
oﬄine [6] using batch optimization, or online by including it into the system
state for optimization [19, 38, 39]. The temporal calibration between the camera
and the IMU is a less-explored topic [12, 14, 18, 10]. Jacovitti et al. [12] estimate
the time-oﬀset by searching the peak that maximizes the correlation of diﬀerent
sensor measurements. Kelly et al. [14] ﬁrstly estimated rotations from diﬀerent
sensors independently, and then temporally aligned these rotations in the rota-
tion space. However, both [12, 14] cannot estimate time-varying time oﬀsets. Li
et al. [18] adopted a diﬀerent approach. They assume constant velocities around
local trajectories. Time oﬀsets are included in the estimator state vector, and
optimized together with other state variables within an EKF framework. Instead
of explicitly optimizing the time oﬀset, Guo et al. [10] proposed an interpolation
model to account for the pose displacements caused by time oﬀsets.
While most works on VIOs use global-shutter cameras, deployments on con-
sumer devices drive the need for using rolling-shutter cameras. A straightforward
way to deal with rolling-shutter eﬀects is to rectify images as if they are captured
by global-shutter cameras, such as the work [15] proposed by Klein and Murray
that assumes a constant velocity model and corrects distorted image measure-
ments in an independent thread. For more accurate modeling, some approaches
extend the camera projection function to take rolling-shutter eﬀects into ac-
count. They represent local trajectories using zero order parameterizations [18,
20] or higher order parameterizations [36]. Instead of modeling the trajectories,
[22] predicts the trajectories using IMU propagation and models the prediction
errors of the estimated trajectories. These errors are represented as a weighted
sum of temporal basis functions.
3
Preliminaries
In this section we brieﬂy review the preliminaries of the nonlinear optimization
framework used in our model. For detailed derivations, please refer to [32, 17].
We begin by giving notations. We consider (·)w as the earth’s inertial frame,
and (·)bk and (·)ck as the IMU body frame and camera frame while taking the
kth image, respectively. We use pX
Y , vX
Y , and RX
Y to denote the 3D position,
velocity, and rotation of frame Y w.r.t frame X, respectively. The corresponding
quaternion (qX
Y = [qx, qy, qz, qw]) for rotation is in Hamilton notation in our
formulation. We assume that the intrinsic of the monocular camera is calibrated
beforehand with known focal length and principle point. The relative translation
and rotation between the monocular camera and the IMU are pc
b and qc
b. The
system-recorded time instant for taking the kth image is tk, while the image
Modeling Varying Camera-IMU Time Oﬀset in Optimization-Based VIO
5
is actually captured at ˜tk = tk + ∆to
k, with an unknown time oﬀset ∆to
k due
to inaccurate timestamps. Note that the time oﬀset ∆to
k is generally treated
as a known constant in other optimization-based VIO algorithms, whereas it is
modeled as an unknown variable for each image in our model.
In a sliding-window nonlinear optimization framework, the full state is usu-
ally encoded as X = [xb0 ... xbk ... xbn f w
0 ... f w
j ... f w
m pb
c qb
c], where a sub-state
xbk = [pw
bk, vw
bk, qw
bk, bbk
a , bbk
ω ] consists of the position, velocity, rotation, linear
acceleration bias, and angular velocity bias at tk, f w
j is the 3D Euclidean position
of feature j in the world coordinate, and pb
c as well as qb
c are the camera-IMU
extrinsics. Finding the MAP estimate of state parameters is equivalent to mini-
mizing the sum of the Mahalanobis norm of all measurement errors:
min
X
||bp −HpX||2 +
X
ˆzk
k+1∈Si
||ri(ˆzk
k+1, X)||2
Σk
k+1 +
X
ˆzik∈Sc
||rc(ˆzik, X)||2
Σc,
(1)
where bp and Hp are priors obtained via marginalization [17], Si and Sc are
the sets of IMU and camera measurements, with the corresponding inertial and
visual constraints modeled by residual functions ri(ˆzk
k+1, X) and rc(ˆzik, X), re-
spectively. The corresponding covariance matrices are denoted as Σk
k+1 and Σc.
To derive the inertial residual term ri(ˆzk
k+1, X) in Eq. (1), the IMU propa-
gation model needs to be derived from the kinematics equation ﬁrst, that is
pw
bk+1 = pw
bk + vw
bk∆tk −1
2gw∆t2
k + Rw
bk ˆαk
k+1,
vw
bk+1 = vw
bk −gw∆tk + Rw
bk ˆβ
k
k+1,
(2)
qw
k+1 = qw
k ⊗ˆqk
k+1,
where ∆tk = tk+1 −tk, gw = [0, 0, 9.8]T is the gravity vector in the earth’s
inertial frame, and ˆzk
k+1 = {ˆαk
k+1, ˆβ
k
k+1, ˆqk
k+1} as well as its covariance Σk
k+1
can be obtained by integrating linear accelerations abt and angular velocities
ωbt [5]. Then the inertial residual term can be derived as:
ri(ˆzk
k+1, X) =


Rbk
w (pw
bk+1 −pw
bk −vw
bk∆tk + 1
2gw∆t2
k) −ˆαk
k+1
Rbk
w (vw
bk+1 −vw
bk + gw∆tk) −ˆβ
k
k+1
(ˆqk
k+1)−1(qw
bk)−1qw
bk+1

.
(3)
The visual residual term rc(ˆzik, X) in Eq. (1) is deﬁned by the projection
errors of tracked sparse features, which can be obtained using ST corner detector
[34] and tracked across sequential images using sparse optical ﬂow [1]. Note that,
to handle the rolling-shutter eﬀect, the generalized epipolar geometry [3] can be
adopted as the ﬁtted model in the RANSAC outlier removal procedure during
correspondences establishment. Suppose a feature f w
i
in the world coordinate,
following the pinhole model, its projection uk
i on the k frame is:
uk
i =
xck
i /zck
i
yck
i /zck
i

, where f ck
i
=


xck
i
yck
i
zck
i

= Rc
b(Rbk
w (f w
i −pw
bk) −pb
c).
(4)
6
Ling et al.
t
…
…
…
…
∆��
�
��
̃��
t
…
…
…
…
∆��
��
̃��
�
IMU read-out time
Camera read-out time
Read-out time of each rolling shutter row
row of a rolling shutter
��
Ground Truth
Constant Velocity
Ours
First/Last-row Read-out Time
Trajectory Variance
Fig. 2: The illustration of a camera-IMU sensor suite with a rolling-shutter cam-
era and imperfect camera-IMU synchronization. We ‘average’ the rolling shutter
readout time instants and approximate the rolling-shutter images (top-left) with
global-shutter images captured at the ‘middle-row’ readout time of the rolling-
shutter cameras (top-right). The position of this ‘middle-row’ is optimized to be
the expected position of the local ground truth trajectory (bottom).
Then the projection error is rc(ˆzik, X) = uk
i −ˆuk
i , where ˆuk
i is the tracked feature
location. The covariance matrix Σc is set according to the tracking accuracy of
the feature tracker. By linearizing the cost function in Eq. (1) at the current
best estimation ˆ
X with respect to error state δX, the nonlinear optimization
problem is solved via iteratively minimizing the following linear system over δX
and updating the state estimation ˆ
X by ˆ
X ←ˆ
X + δX until convergence:
min
δX ||bp −Hp( ˆ
X + δX)||2 +
X
ˆzk
k+1∈Si
||ri(ˆzk
k+1, ˆ
X) + HkδX||2
Σk
k+1
+
X
ˆzik∈Sc
||rc(ˆzik, ˆ
X) + Hi
kδX||2
Σc,
(5)
where Hk and Hi
k are Jacobians of the inertial and visual residual functions.
4
Modeling Varying Camera-IMU Time Oﬀset
In this section, we ﬁrst show that rolling-shutter eﬀects can be approximately
compensated by modeling a camera-IMU time oﬀset (Sect. 4.1). Then we present
our time-varying model for the oﬀset (Sect. 4.2) and the derivation of the Ja-
cobian for optimization after introducing the time oﬀset variable (Sect. 4.3).
Finally, an eﬃcient algorithm is described to accelerate the IMU integration
over variable-length time intervals (Sect. 4.4), required by the optimization.
4.1
Approximate Compensation for Rolling-Shutter Eﬀects
Fig. 2.(a) shows a camera-IMU suite with a rolling-shutter camera and imperfect
camera-IMU synchronization. The system-recorded time instant for taking the
Modeling Varying Camera-IMU Time Oﬀset in Optimization-Based VIO
7
kth image is tk, which serves as our time reference for the retrieval of IMU data.
Due to imperfect sensor synchronization (or inaccurate timestamps), the image
is actually captured at ˜tk = tk + ∆to
k, with an unknown time oﬀset ∆to
k. With a
rolling-shutter camera, this means that ˜tk is the time instant when the camera
starts to read out pixels row by row. Instead of modeling local trajectories using
constant velocities [10, 15, 20], we model them with constant poses, which are
expected poses of local trajectories (Fig. 2). With this approximation, a rolling-
shutter image captured at ˜tk with time oﬀset ∆to
k can be viewed as an global-
shutter image captured at ˜t′
k with time oﬀset ∆to
k
′. In the following, we slightly
abuse notation by replacing ∆to
k
′ with ∆to
k and ˜t′
k with ˜tk. We also replace pw
bk
and Rbk
w in Eq. (4) by ˜pw
bk and ˜Rbk
w as they are now evaluated at time instant
˜tk. To calculate the pose at ˜tk, we use IMU propagation from the pose at tk:
˜pw
bk = pw
bk + vw
bk∆to
k −1
2gw(∆to
k)2 + Rw
bk ˆαbk
ck,
˜qw
bk = qw
bk ⊗ˆqbk
ck,
(6)
ˆαbk
ck =
ZZ
t∈[tk,˜tk]
Rbk
t (abt −bbk
a )dt2,
ˆqbk
ck =
Z
t∈[tk,tck ]
1
2
−

ωbt −bbk
ω

× ωbt −bbk
ω
−(ωbt −bbk
ω )T
0

qbk
bt dt
(7)
where abt/ωbt is the instant linear acceleration/angular velocity. Since only dis-
crete IMU measurements are available on IMUs, ˆαbk
ck and ˆqbk
ck in (7) are approx-
imately computed using numerical integration (i.e. mid-point integration).
The beneﬁt of our constant-pose approximation is that additional variables,
i.e. velocities and the rolling-shutter row time, are not needed for estimation,
which leads to a large reduction of computational complexity.
4.2
Modeling Camera-IMU Time Oﬀset
From the previous subsection, we see that the time oﬀset ∆to
k is the addition of
two parts. The ﬁrst part is the camera-IMU time oﬀset, which varies smoothly
because of the system payload variation and sensor jitter. The second part is
the compensated time oﬀset caused by the rolling-shutter eﬀect approximation,
which varies smoothly according to the change of local trajectories. We see ∆to
k
as a slowly time-varying quantity and model it as a Gaussian random walk:
˙∆t
o
k = no, where no is zero-mean Gaussian noise with covariance Σo. Since
time oﬀsets we optimize are at discrete time instants, we integrate this noise
over the time interval between two consecutive frames in the sliding window
[tk, tk+1]: ∆to
k+1 = ∆to
k + no
k, Σo
k = ∆tkΣo, where no
k and Σo
k are discrete noise
and covariance, respectively. Thus we add ∥∆to
k+1 −∆to
k∥Σo
k into Eq. (1) for all
consecutive frames. By including constraints between consecutive time oﬀsets,
we avoid “oﬀset jumping” between consecutive frames.
8
Ling et al.
4.3
Optimization with Unknown Camera-IMU Time Oﬀset
Our state vector at time instant tk reads as xbk = [pw
bk, vw
bk, qw
bk, bbk
a , bbk
ω , ∆to
k],
where ∆to
k is the dynamically changing camera-IMU time oﬀset modeling both
the approximate compensation of rolling-shutter eﬀects and imperfect sensor
synchronization. With ∆to
k, the error state δX for linearization becomes δX =
[δpw
bk δvw
bk δθw
bk δbbk
a δbbk
ω δpc
b δθc
b δf w
i
δ∆to
k], where we adopt a minimal error
representation for rotations (δθbk, δθc
b ∈R3): qw
bk = ˆqw
bk ⊗

δθw
bk
1

,
qc
b = ˆqc
b ⊗
δθc
b
1

. Other error-state variables δpw
bk, δvw
bk, δbbk
a , δbbk
ω , δpc
b, δf w
i , and δ∆to
k are
standard additive errors. After introducing δ∆to
k, the Jacobian Hk of the inertial
residual function in Eq. (5) remains the same as before, while the Jacobian Hi
k
of the visual residual function needs to be reformulated. Denoting ˆ(·) the states
obtained from the last iteration of the nonlinear optimization, the Jacobian Hi
k
can be written as
Hi
k = ∂rc
∂δX = ∂rc
∂f ck
i
∂f ck
i
∂δX =


1
ˆz
ck
i
0 −ˆx
ck
i
ˆz
ck
i
0
1
ˆz
ck
i
−ˆy
ck
i
ˆz
ck
i

J,
J = [−ˆRc
b ˜Rbk
w
−ˆRc
b ˜Rbk
w ∆ˆto
k
ˆRc
b ˜Rbk
w
j
ˆf w
i −˜pw
bk
k
× 03×6
ˆRc
b
j
ˆf ck
i
k
×
ˆRc
b ˜Rbk
w Jδ∆t],
Jδ∆t = ˆRc
b(

ωbk
× ˜Rbk
w (ˆf w
i −˜pw
bk) + ˜Rbk
w vw
bk + gw∆ˆto
k),
where ⌊·⌋× denotes the skew symmetric matrix of a vector. Recall that the posi-
tion ˜pw
bk and rotation ˜Rbk
w in the Jacobian is computed at time instant ˜tk instead
of tk, which depends on variable ∆ˆto
k and varies in each iteration of the opti-
mization. Besides, the computation of the feature position ˆf ck
i
projected on the
k-th frame depends on ˜pw
bk and ˜Rbk
w as shown in Eq. (4), which also needs to be
recomputed when ∆ˆto
k changes. We in the next section present a novel algorithm
based on dynamic programming and red-black tree to eﬃciently compute ˜pw
bk
and ˜Rbk
w as the estimated time oﬀset ∆ˆto
k varies during the iterations.
4.4
Eﬃcient IMU Integration over Variable-Length Time Intervals
With a naive implementation, the computation of ˆαbk
ck and ˆqbk
ck in Eq. (7) be-
tween tk and ˜tk needs to be recomputed each time the oﬀset ∆to
k changes during
the optimization. In order to reuse the intermediate integration results, we de-
compose the integration into two steps (Fig. 3): ﬁrstly, compute the integration
between tk and ti; secondly, compute the integration between ti and ˜tk. Here,
without loss of generality, we assume ti is the closest IMU read-out time instant
before ˜tk. The decomposition makes the results in the ﬁrst step reusable since
the integration is computed over variable yet regular time intervals. We design
an algorithm based on discrete dynamic programming [16] and red-black tree
[16] to perform eﬃcient integration over the variable-length time intervals.
Modeling Varying Camera-IMU Time Oﬀset in Optimization-Based VIO
9
t
∆��
�
��
IMU read-out time
Estimated camera read-out time
��
��′
…
…
��
��′
��
tree-1
tree-2
tree-3
��+1
̃��
Fig. 3: Computing the pose at ˜tk from the pose at tk is decomposed into two steps
(left): ﬁrstly, compute the pose at ti, where ti is the closest IMU measurement
time instant to ˜tk and smaller than ˜tk; secondly, compute the pose at ˜tk based
on the pose at ti. We design an algorithm based on dynamic programming and
red-black tree (right) for eﬃcient indexing to accelerate the ﬁrst step.
Speciﬁcally, we build a table (implemented as a red-black tree [16]) for each
time instant tk to store the intermediate results of IMU integration starting from
tk (right part of Fig. 3). Each node in the tree stores a key-value pair, where
the key is an IMU read-out time ti and the value is the integration from tk to
ti computed using Eq. (7). Note that this integration is independent of the pose
and velocity at tk, so the stored results do not need to be updated when the
pose and velocity in Eq. (6) as tk change. During the optimization, each time
when we need to compute the integration from tk to ti (recall that ti is variable
according to ˜tk), we ﬁrst try to search in the tree at tk to query the integration
results for ti. If the query failed, we then instead search if there exists a record
for another IMU read-out time ti′ such that ti′ < ti. If multiple records exist,
we select the maximum ti′ (which is closest to ti), and compute the integration
from tk to ti based on the retrieved record at ti′ and IMU measurements from
ti′ to ti. During the process, each time a new integration is computed, the result
is stored into the tree for future retrieval.
5
Uncertainty-Aware Initialization
We initialize our VIO system by ﬁrst running a vision-only bundle adjustment on
K consecutive frames. Suppose that the K consecutive rotations and positions
obtained from vision-only bundle adjustment are [pc0
c0 Rc0
c0 ... pc0
cK−1 Rc0
cK−1]. The
goal of the initialization is to solve initial velocities [vb0
bk ... vb0
bK−1], gravity vector
gc0, and metric scale s. The initializations in [31, 28] solve for the unknowns by
minimizing the least squares of the L2 errors between the poses obtained from
vision-only bundle adjustment and the poses obtained from IMU integration.
These methods do not take into account the uncertainties introduced by IMU
noise during the IMU integration, which can cause failures of the initialization
when the camera-IMU time oﬀset is unknown (see Sect. 6.2).
We employ a diﬀerent method to perform uncertainty-aware initialization by
incorporating the covariances obtained during the IMU integration. The IMU
10
Ling et al.
propagation model for the K initialization images can be written as
pc0
bk+1 = pc0
bk + Rc0
bkvbk
bk∆tk −1
2gc0∆t2
k + Rc0
bk ˆαk
k+1,
Rc0
bk+1vbk+1
bk+1 = Rc0
bkvbk
bk −gc0∆tk + Rc0
bk ˆβ
k
k+1,
(8)
where pc0
bk = spc0
ck −Rc0
bkpb
c and Rc0
bk = Rc0
ckRc
b. We set the camera-IMU extrinsics
pb
c and Rb
c to a zero vector and an identity matrix, respectively. Instead of min-
imizing the L2 norm of the measurement errors, we minimize the Mahalanobis
norm of the measurement errors as:
min
X
X
ˆzk
k+1∈Si



R
bk
c0 (spc0
ck+1 −Rc0
bk+1pb
c −spc0
ck + Rc0
bk pb
c + 1
2 gc0∆t2
k) −v
bk
bk ∆tk −ˆαk
k+1
R
bk
c0 (Rc0
bk+1v
bk+1
bk+1 + gc0∆tk) −v
bk
bk −ˆβ
k
k+1



2
Σk
k+1
,
where ˆzk
k+1 = {ˆαk
k+1, ˆβ
k
k+1} and Σk
k+1 is the covariance matrix computed during
IMU integration [5]. Note that the covariance Σk
k+1 in our formulation models
the uncertainty of the IMU measurements. The resulting problem is a weighted
least squares problem and can be solved eﬃciently. After solving the problem,
we project the solved variables into the world coordinate. Other variables like
accelerator biases, gyrocope biases, and time oﬀsets are initialized as zeros.
6
Experiments
We compare our approach to state-of-the-art monocular VIO systems: OKVIS
[17] and VINS-Mono [30]. Loop closure in VINS-Mono is disabled for a fair
comparison. The number of keyframes and non-keyframes in the optimization
window of OKVIS and our approach are set to be 8 and 3 respectively. The
sliding window size in VINS-Mono is set to be 11.
6.1
Performance on The Euroc Dataset
Sequences of the Euroc dataset consist of synchronized global-shutter stereo im-
ages (only mono/left images are used) and IMU measurements. The complexity
of these sequences varies regarding trajectory length, ﬂight dynamics, and illu-
mination conditions. Ground truth poses are obtained by Vicon motion capture
system. For presentation, we use numbers to denote sequence names: 1 ∼5 for
MH 01 easy ∼MH 05 diﬃcult, 6 ∼8 for V1 01 easy ∼V1 03 diﬃcult, 9 ∼11
for V2 01 easy ∼V2 03 diﬃcult.
Signiﬁcance of Online Temporal Camera-IMU Calibration Two error
metrics are used for tracking accuracy evaluation: the average relative rota-
tion error (deg) and the average relative translation error (m) [8]. Approaches
are compared under diﬀerent simulated time oﬀsets between visual and inertial
Modeling Varying Camera-IMU Time Oﬀset in Optimization-Based VIO
11
1
2
3
4
5
6
7
8
9
10
11
Sequence Number
0
0.05
0.1
0.15
0.2
Ave. Relative Angel Error (deg)
OKVIS
VINS-Mono
Ours
(a) Rot. errors without oﬀsets
1
2
3
4
5
6
7
8
9
10
11
Sequence Number
0
2
4
6
8
10
12
14
Ave. Relative Translation Error (mm)
OKVIS
VINS-Mono
Ours
(b) Trans. errors without oﬀsets
1
2
3
4
5
6
7
8
9
10
11
Sequence Number
0
0.1
0.2
0.3
0.4
0.5
0.6
Ave. Relative Angel Error (deg)
OKVIS
VINS-Mono
Ours
(c) Rot. errors with 30 ms oﬀset
1
2
3
4
5
6
7
8
9
10
11
Sequence Number
0
10
20
30
40
50
Ave. Relative Translation Error (mm)
OKVIS
VINS-Mono
Ours
(d) Trans. errors with 30 ms oﬀset
1
2
3
4
5
6
7
8
9
10
11
Sequence Number
0
0.2
0.4
0.6
0.8
1
1.2
Ave. Relative Angel Error (deg)
OKVIS
VINS-Mono
Ours
(e) Rot. errors with 60 ms oﬀset
1
2
3
4
5
6
7
8
9
10
11
Sequence Number
0
20
40
60
80
Ave. Relative Translation Error (mm)
OKVIS
VINS-Mono
Ours
(f) Trans. errors with 60 ms oﬀset
Fig. 4: The relative rot. and trans. errors with diﬀerent time oﬀsets on Euroc.
1
2
3
4
5
6
7
8
9
10
11
Sequence Number
0
0.02
0.04
0.06
0.08
0.1
0.12
Ave. Relative Angel Error (deg)
No offset
30 ms offset
60 ms offset
(a) Avg. relative rot. error (deg)
1
2
3
4
5
6
7
8
9
10
11
Sequence Number
0
1
2
3
4
5
6
7
Ave. Relative Translation Error (mm)
No offset
30 ms offset
60 ms offset
(b) Avg. relative trans. error (m)
Fig. 5: The average relative rot. and trans. errors of our approach with diﬀerent
time oﬀsets on Euroc.
measurements: no time oﬀsets; 30 ms oﬀset; 60 ms oﬀset. Fig. 4 shows the com-
parison results. When visual measurements and inertial measurements are syn-
chronized, the average relative rotation/translation error between VINS-Mono
and our approach are similar, while OKVIS performs the worst. As the time
oﬀset increases, VINS-Mono and OKVIS perform worse and worse due to the
lack of camera-IMU time oﬀset modeling. Our estimator achieves the smallest
tracking error when there exists a time oﬀset. OKVIS fails to track in sequence 2
∼5 when the time oﬀset is set to be 60 ms. We have tested diﬀerent approaches
under larger time oﬀsets, such as 90 ms. However, neither OKVIS or VINS-Mono
provides reasonable estimates. We thus omit comparisons for larger time oﬀsets.
We also illustrate the tracking performance of our approach under diﬀerent time
oﬀsets in Fig. 5. We see that, by modeling the time oﬀset properly, there is no
signiﬁcant performance decrease as time oﬀset increases. Fig. 6 gives a visual
12
Ling et al.
-4
-3
-2
-1
0
1
2
x(m)
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
2.5
3
y (m)
VINS_Mono
Ours
GT
Starting Point
Fig. 6:
Compari-
son on the Euroc
V1 03 diﬃcult with
a 60 ms time oﬀset.
10
20
30
40
50
60
time (s)
-0.1
0
0.1
gyr bias (rad/s)
gyr_bias_x
gyr_bias_y
gyr_bias_z
10
20
30
40
50
60
time (s)
-0.2
0
0.2
acc bias (m/s2)
acc_bias_x
acc_bias_y
acc_bias_z
10
20
30
40
50
60
time (s)
0
0.05
0.1
time offset (s)
time offset
5
10
15
20
25
30
35
time (s)
-0.1
0
0.1
gyr bias (rad/s)
gyr_bias_x
gyr_bias_y
gyr_bias_z
5
10
15
20
25
30
35
time (s)
-0.2
0
0.2
acc bias (m/s2)
acc_bias_x
acc_bias_y
acc_bias_z
5
10
15
20
25
30
35
time (s)
0
0.05
0.1
time offset (s)
time offset
Fig. 7: The convergence study of variables in our
estimator on the Euroc MH 03 medium (left) and
V1 02 medium (right) with a 60 ms time oﬀset.
comparison on trajectories estimated by our approach and VINS-Mono on the
Euroc sequence V1 03 diﬃcult with a 60 ms time oﬀset. Noticeable ‘jaggies’ are
found on the trajectory estimated by VINS-Mono.
Parameter Convergence Study Another aspect we pay attention to is whether
estimator parameters converge or not. We analyze our results on the sequence
MH 03 medium and sequence V1 02 medium with a simulated 60 ms time oﬀ-
set. Estimated biases and time oﬀsets w.r.t. time are plotted in Fig. 7. Both
gyroscope biases and time oﬀsets converge quickly. On the one hand, relative
rotations are well constrained by visual measurements. They are not related to
metric scale estimation. On the other hand, relative rotations are the ﬁrst-order
integration of angular velocities and gyroscope biases, thus they are easy to esti-
mate. Another interesting thing we found is that, the convergence of time oﬀsets
is the same as the convergence of gyroscope biases. This means that, time oﬀsets
are calibrated mainly by aligning relative rotations from visual constraints and
from gyroscope integrations, which is consistent with ideas of oﬄine time oﬀset
calibration algorithms [12, 14]. Compared to gyroscope biases, acceleration bi-
ases converge much more slowly. They are hard to estimate as positions are the
second-order integration of accelerations and acceleration biases. Additionally,
acceleration measurements obtained from IMUs are coupled with gravity mea-
surements. Abundant rotations collected across time are required to distinguish
actual accelerations and gravity vector from measurements.
6.2
Performance on Mobile Phone Data
One of the applications of VIO is the motion tracking on mobile phones. We col-
lect visual and inertial measurements using Samsung Galaxy S8 with a rolling-
shutter camera and unsynchronized sensor measurements. Google developed AR-
Core [9] is also used for comparison. After recording the data, we run our system
on the Samsung Galaxy S8. It takes 70 ± 10ms for each nonlinear optimization.
Qualitative Comparison of Tracking Accuracy We use AprilTag [37] to ob-
tain time-synchronized and drift-free ground truths (Fig. 8 (d)). We detect tags
Modeling Varying Camera-IMU Time Oﬀset in Optimization-Based VIO
13
0
5
10
15
20
25
30
time (s)
-0.5
0
0.5
x (m)
0
5
10
15
20
25
30
time (s)
-0.5
0
0.5
y (m)
Ground truth
VINS-Mono
ARCore
Ours_fix
Ours
0
5
10
15
20
25
30
time (s)
0
0.5
z (m)
(a) Slow translation and rotation.
0
2
4
6
8
10
12
14
16
18
20
time (s)
-0.5
0
0.5
x (m)
0
2
4
6
8
10
12
14
16
18
20
time (s)
-0.4
-0.2
0
y (m)
0
2
4
6
8
10
12
14
16
18
20
time (s)
0
0.5
1
z (m)
Ground truth
VINS-Mono
ARCore
Ours_fix
Ours
(b) Medium translation and rotation.
0
5
10
15
20
25
30
time (s)
-1
0
1
x (m)
0
5
10
15
20
25
30
time (s)
-2
0
2
y (m)
Ground truth
ARCore
Ours_fix
Ours
0
5
10
15
20
25
30
time (s)
0
0.5
1
x (m)
(1)
(1)
(1)
(2)
(2)
(2)
(3)
(3)
(3)
(c) Fast translation and rotation.
(d) Image
Fig. 8: The qualitative comparison of tracking accuracy. VINS-Mono fails to track
on the fast translation and rotation sequence.
on captured images. If tags are detected, we calculate ground truth camera poses
via P3P [7] using tag corners. Since VINS-Mono performs better than OKVIS,
we only include tracking results from VINS-Mono for the following comparison.
We test our approach with two conditions: 1)‘Ours-ﬁx’: the variable time-delay
estimation is switched oﬀ, i.e. is replaced with a single time oﬀset, to account for
the camera-IMU synchronization; 2)‘Ours’: the variable time-delay is enabled
to account for both the camera-IMU synchronization and the rolling-shutter ef-
fect approximation. Three sequences are recorded around an oﬃce desk. These
datasets are increasingly diﬃcult to process in terms of motion dynamics: slow,
medium, and fast translation and rotation (Fig. 8). To align coordinates of dif-
ferent approaches for comparison, we regularly move the mobile phone at the
beginning of recorded sequences (see 0-5s in (a), 0-4s in (b), and 0-6s in (c)).
VINS-Mono performs worst among all datasets, as it does not model time oﬀsets
between visual and inertial measurements. Our approach with condition ‘Ours-
ﬁx’ performs worse than the one with condition ‘Ours’. The tracking accuracy
of our approach is comparable to that of ARCore in small and medium motion
settings. While for the fast translation and rotation sequence, where part of
captured images are blurred, our approach exhibits better tracking robustness
14
Ling et al.
compared to ARCore. ARCore loses tracks in time periods within dashed boxes
(1)(2)(3). This is because ARCore is based on EKF [11], which requires good
initial guesses about predicted states. If pose displacements are large and feature
correspondences are not abundant because of image blur, ARCore may fail. Con-
versely, our nonlinear optimization approach iteratively minimizes errors from
sensor measurements, which treats the underlying non-linearity better and are
less sensitive to initial guesses. Note that, there is a loop closing module in AR-
Core for pose recovery and drift correction. The ﬁnal position drift of ARCore
is thus smallest. However, loop closing is not the focus of this work.
Qualitative Comparison of Estimator Initialization To evaluate the sig-
niﬁcance of our initialization method, we compare it with the state-of-the-art
visual-inertial initialization method [28]. We record 20 testing sequences on Sam-
sung Galaxy S8 with medium translation and rotation. Each testing sequence
lasts for about 30 seconds. We use the ﬁrst 2-second sensor measurements to
do the initialization. After the initialization is done, we use the visual-inertial
estimator proposed in this work estimate camera poses. We then calculate an
optimal scale factor by aligning the estimated trajectory with the trajectory
reported by ARCore by a similarity transformation [2]. Although there is in-
evitably pose drift in ARCore estimates, this drift is not signiﬁcant compared
to the scale error caused by improper initializations. If the estimator fails to
track in any time intervals of the testing sequences or the calculated scale error
is larger than 5%, we declare the initialization as failures. For a fair comparison,
we use the same relative poses obtained by visual sfm [29] as the initialization
input of [28] and that of our approach. We ﬁnd that 14 successful trials out of
20 (70%) using initialization proposed in [28], while 18 successful trials out of
20 (90%) using our initialization method. The successful initialization rate using
our approach is higher than using initialization in [28]. We study the two testing
sequences where our initialization fails. We ﬁnd that time oﬀsets between visual
and inertial measurements in these two sequences are larger than 100 ms (this
can be obtained by enumerating oﬀsets and testing whether the following visual-
inertial estimator outputs are close to that of ARCore or not after initialization),
causing a big inconsistency when aligning the visual and inertial measurements.
Since the main focus of this work is on the estimator, we are going to handle
this failure case thoroughly in the future work.
7
Conclusions
In this work, we proposed a nonlinear optimization-based VIO that can deal with
rolling-shutter eﬀects and imperfect camera-IMU synchronization. We modeled
the camera-IMU time oﬀset as a time-varying variable to be estimated. An ef-
ﬁcient algorithm for IMU integration over variable-length time intervals, which
is required during the optimization, was also introduced. The VIO can be ro-
bustly launched with our uncertainty-aware initialization scheme. The experi-
ments demonstrated the eﬀectiveness of the proposed approach.
Modeling Varying Camera-IMU Time Oﬀset in Optimization-Based VIO
15
References
1. Baker, S., Matthews, I.: Lucas-Kanade 20 years on: A unifying framework. Inter-
national Journal of Computer Vision 56(3), 221–255 (2004)
2. Berthold K. P. Horn: Closed-form solution of absolute orientation using unit
quaternions. Optical Society of America 4 (1987)
3. Dai, Y., Li, H., Kneip, L.: Rolling shutter camera relative pose: Generalized epipo-
lar geometry. In: Proc. of the IEEE Intl. Conf. on Comput. Vis. and Pattern
Recognition (2016)
4. Dong-Si, T., Mourikis, A.I.: Estimator initialization in vision-aided inertial naviga-
tion with unknown camera-imu calibration. In: Proc. of the IEEE/RSJ Intl. Conf.
on Intell. Robots and Syst. (2012)
5. Forster, C., Carlone, L., Dellaert, F., Scaramuzza, D.: IMU preintegration on
manifold for eﬃcient visual-inertial maximum-a-posteriori estimation. In: Proc.
of Robot.: Sci. and Syst. (2015)
6. Furgale, P., Rehder, J., Siegwart, R.: Uniﬁed temporal and spatial calibration for
multi-sensor systems. In: Proc. of the IEEE/RSJ Intl. Conf. on Intell. Robots and
Syst. (2013)
7. Gao, X.S., Hou, X.R., Tang, J., Cheng, H.F.: Complete solution classiﬁcation for
the perspective-three-point problem. IEEE Transactions on Pattern Analysis and
Machine Intelligence (2003)
8. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti
vision benchmark suite. In: Conference on Computer Vision and Pattern Recogni-
tion (CVPR) (2012)
9. Google: ARCore: https://developers.google.com/ar/
10. Guo, C., Kottas, D., DuToit, R., Ahmed, A., Li, R., Roumeliotis, S.: Eﬃcient
visual-inertial navigation using a rolling-shutter camera with inaccurate times-
tamps. In: Proceedings of Robotics: Science and Systems (2014)
11. Hesch, J.A., Kottas, D.G., Bowman, S.L., Roumeliotis, S.I.: Consistency analysis
and improvement of vision-aided inertial navigation. IEEE Trans. Robot. 30(1),
158–176 (Feb 2014)
12. Jacovitti, G., Scarano, G.: Discrete time techniques for time delay estimation. IEEE
Transactions on Signal Processing 41 (1993)
13. Jung, S.H., Taylor, C.J.: Camera trajectory estimation using inertial sensor mea-
surements and structure from motion results. In: Proc. of the IEEE Intl. Conf. on
Comput. Vis. and Pattern Recognition (2001)
14. Kelly, J., Sukhatme, G.: A general framework for temporal calibration of multiple
proprioceptive and exteroceptive sensors. In: Proc. of the Intl. Sym. on Exp. Robot.
(2010)
15. Klein G., Murray D.: Parallel tracking and mapping on a camera phone. In: Proc.
Sixth IEEE and ACM International Symposium on Mixed and Augmented Reality
(2009)
16. Knuth, D.: The Art of Computer Programming, vol. 1-3. Addison-Wesley Longman
Publishing Co., Inc., Boston, MA, USA (1998)
17. Leutenegger, S., Furgale, P., Rabaud, V., Chli, M., Konolige, K., Siegwart, R.:
Keyframe-based visual-inertial SLAM using nonlinear optimization. In: Proc. of
Robot.: Sci. and Syst. (2013)
18. Li, M., Mourikis, A.I.: 3D motion estimation and online temporal calibration for
camera- IMU systems. In: Proc. of the IEEE Intl. Conf. on Robot. and Autom.
(2013)
16
Ling et al.
19. Li, M., Mourikis, A.I.: High-precision, consistent EKF-based visual-inertial odom-
etry. Intl. J. Robot. Research 32(6), 690–711 (May 2013)
20. Li, M., Mourikis, A.I.: Real-time motion tracking on a cellphone using inertial
sensing and a rolling shutter camera. In: Proc. of the IEEE Intl. Conf. on Robot.
and Autom. (2013)
21. Li, M., Mourikis, A.I.: Online temporal calibration for camera-imu systems: Theory
and algorithms. Intl. J. Robot. Research 33 (2014)
22. Li, M., Mourikis, A.I.: Vision-aided Inertial Navigation with Rolling-Shutter Cam-
eras. Intl. J. Robot. Research 33 (2014)
23. Ling, Y., Kuse, M., Shen, S.: Edge alignment-based visual-inertial fusion for track-
ing of aggressive motions. Autonomous Robots (2017)
24. Ling, Y., Shen, S.: Dense visual-inertial odometry for tracking of aggressive mo-
tions. In: Proc. of the IEEE Intl. Conf. on Robot. and Bio. (2015)
25. Ling, Y., Shen, S.: Aggressive quadrotor ﬂight using dense visual-inertial fusion.
In: Proc. of the IEEE Intl. Conf. on Robot. and Autom. (2016)
26. Lynen, S., Achtelik, M., Weiss, S., Chli, M., Siegwart, R.: A robust and mod-
ular multi-sensor fusion approach applied to MAV navigation. In: Proc. of the
IEEE/RSJ Intl. Conf. on Intell. Robots and Syst. (2013)
27. Mourikis, A.I., Roumeliotis, S.I.: A Multi-State Constraint Kalman Filter for
Vision-aided Inertial Navigation. In: Proc. of the IEEE Intl. Conf. on Robot. and
Autom. (2007)
28. Mur-Artal, R., D.Tards, J.: Visual-inertial monocular slam with map reuse. IEEE
Robotics and Automation Letters 2 (2017)
29. Mur-Artal, R., Montiel, J.M.M., Tard´os, J.D.: ORB-SLAM: a versatile and accu-
rate monocular SLAM system. IEEE Transactions on Robotics 31(5), 1147–1163
(2015). https://doi.org/10.1109/TRO.2015.2463671
30. Qin, T., Li, P., Shen, S.: VINS-Mono: A robust and versatile monocular visual-
inertial state estimator. arXiv preprint arXiv:1708.03852 (2017)
31. Qin, T., Shen, S.: Robust initialization of monocular visual-inertial estimation on
aerial robots. In: Proc. of the IEEE/RSJ Intl. Conf. on Intell. Robots and Syst.
(2017)
32. Shen, S., Michael, N., Kumar, V.: Tightly-coupled monocular visual-inertial fusion
for autonomous ﬂight of rotorcraft MAVs. In: Proc. of the IEEE Intl. Conf. on
Robot. and Autom. Seattle, WA (May 2015)
33. Shen, S., Mulgaonkar, Y., Michael, N., Kumar, V.: Vision-based state estimation
for autonomous rotorcraft MAVs in complex environments. In: Proc. of the IEEE
Intl. Conf. on Robot. and Autom. (2013)
34. Shi, J., Tomasi, C.: Good features to track. In: Proc. of IEEE Conf. on Computer
Vision and Pattern Recognition (1994)
35. S.I. Roumeliotis, A.E. Johnson, J.F. Montgomery: Augmenting inertial navigation
with image-based motion estimation. In: Proc. of the IEEE Intl. Conf. on Robot.
and Autom. (2002)
36. Steven, L., Alonso, P.P., Gabe, S.: Spline fusion: A continuous-time representation
for visual-inertial fusion with application to rolling shutter cameras. In: British
Machine Vision Conference (2013)
37. Wang, J., Olson, E.: AprilTag 2: Eﬃcient and robust ﬁducial detection. In: Proc.
of the IEEE/RSJ Intl. Conf. on Intell. Robots and Syst. (2016)
38. Weiss, S., Achtelik, M.W., Lynen, S., Chi, M., Siegwart, R.: Real-time onboard
visual-inertial state estimation and self-calibration of mavs in unknown environ-
ments. In: Proc. of the IEEE Intl. Conf. on Robot. and Autom. (2012)
Modeling Varying Camera-IMU Time Oﬀset in Optimization-Based VIO
17
39. Yang, Z., Shen, S.: Monocular visual-inertial fusion with online initialization and
camera-IMU calibration. In: Proc. of the IEEE/RSJ Intl. Conf. on Intell. Robots
and Syst. (2015)
40. Yang, Z., Shen, S.: Tightly-coupled visual-inertial sensor fusion based on IMU pre-
integration. Tech. rep., Hong Kong University of Science and Technology (2016),
URL: http://www.ece.ust.hk/ eeshaojie/vins2016zhenfei.pdf