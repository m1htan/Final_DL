Gray-box Adversarial Training
Vivek B.S.[0000−0003−1933−9920], Konda Reddy Mopuri[0000−0001−8894−7212], and
R. Venkatesh Babu[0000−0002−1926−1804]
Indian Institute of Science, Bangalore, India
{svivek,kondamopuri,venky}@iisc.ac.in
Abstract. Adversarial samples are perturbed inputs crafted to mislead
the machine learning systems. A training mechanism, called adversarial
training, which presents adversarial samples along with clean samples
has been introduced to learn robust models. In order to scale adversarial
training for large datasets, these perturbations can only be crafted using
fast and simple methods (e.g., gradient ascent). However, it is shown
that adversarial training converges to a degenerate minimum, where the
model appears to be robust by generating weaker adversaries. As a result,
the models are vulnerable to simple black-box attacks.
In this paper we, (i) demonstrate the shortcomings of existing evalua-
tion policy, (ii) introduce novel variants of white-box and black-box at-
tacks, dubbed “gray-box adversarial attacks” based on which we propose
novel evaluation method to assess the robustness of the learned models,
and (iii) propose a novel variant of adversarial training, named “Gray-
box Adversarial Training” that uses intermediate versions of the models
to seed the adversaries. Experimental evaluation demonstrates that the
models trained using our method exhibit better robustness compared to
both undefended and adversarially trained models.
Keywords: adversarial perturbations, attacks on machine learning mod-
els, adversarial training, robust machine learning models
1
Introduction
Machine learning models are observed ([1, 2, 4, 7, 17, 20]) to be susceptible
to adversarial examples: samples perturbed with mild but structured noise to
manipulate model’s output. Further, Szegedy et al. [20] demonstrated that ad-
versarial samples are transferable across multiple models, i.e., samples crafted
to mislead one model often fool other models also. This will enable to launch
simple black-box attacks [12, 18] on the models deployed in real world. These
methods to generate adversarial samples, generally known as adversaries, range
from simple gradient ascent [4] to complex optimization procedures (e.g., [14]).
Augmenting the training data with adversarial samples, known as Adversar-
ial Training (AT) [4, 20] has been introduced as a simple defense mechanism
against these attacks. In the adversarial training regime, models are trained
with mini-batches comprising of both clean and adversarial samples, typically
2
Vivek B.S., K.R. Mopuri, and R.V. Babu
obtained from the same model. It is shown by Madry et al. [13] that adver-
sarial training helps to learn models robust to white-box attacks, provided the
perturbations computed during the training closely maximize the model’s loss.
However, in order to scale adversarial training for large datasets, the pertur-
bations can only be crafted with fast and simple methods such as single-step
FGSM [4, 9], an attack based on linearization of the model’s loss. Tram`er et
al. [21] demonstrated that adversarial training with single-step attacks leads to
a degenerate minimum where linear approximation of model’s loss is not reliable.
They revealed that the model’s decision surface exhibits sharp curvature near
the data points which leads to overﬁtting in adversarially trained models. Thus,
(i) adversarially trained models using single-step attacks remain susceptible to
simple attacks, and (ii) perturbations crafted on undefended models transfer and
form black-box attacks.
Tram`er et al. [21] proposed to decouple the adversary generation process
from the model parameters and to increase the diversity of the perturbations
shown to the model during training. Their training mechanism, called Ensemble
Adversarial Training (EAT), incorporates perturbations from multiple (e.g., N
diﬀerent) pre-trained models. They showed that EAT enables to learn models
with increased robustness against black-box attacks.
However, EAT has severe drawbacks in presenting diverse perturbations dur-
ing the training. Since they augment the white-box perturbations (from the
model being learned) with black-box perturbations from an ensemble of dif-
ferent pre-trained models, it is required to train those models before we start
learning a robust model. Therefore, the computational cost increases linearly
with the population of the ensemble. Because of this, the experiments presented
in [21] have a maximum of 4 members in the ensemble. Though it is argued that
diverse set of perturbations is important to learn robust models, EAT fails to
eﬃciently bring diversity to the table.
Unlike EAT, we demonstrate that it is feasible to eﬃciently generate diverse
set of perturbations and augment the white-box perturbations. Further, utilizing
these additional perturbations we learn models that are signiﬁcantly robust com-
pared to those learned with vanilla and ensemble adversarial training (EAT).
The major contributions of this work can be listed as follows:
– We bring out an important observation that the pseudo robustness of an
adversarially trained model is due to the limitations in the existing evaluation
procedure.
– We introduce a novel evaluation procedure via robustness plots (3.2) and a
derived metric “Worst-case Performance (Aw)” that can assess the suscep-
tibility of the learned models. For that, we present variants of the white-box
and black-box attacks, termed “Gray-box adversarial attacks” that can be
launched by temporally evolving intermediate models. Given the eﬃciency to
generate and the ability to examine the robustness, we strongly recommend
the community to consider robustness plots and “Worst-case Performance”
as standard bench-marking for evaluating the models.
GAT
3
 
Existing 
Evaluation
 
Proposed 
Evaluation
Ground 
Truth Label
Bird
MBest
Model used for generating 
Adversarial sample
Input:
Adversarial sample
Output:
Predicted Class
Evaluating an Adversarially trained Neural Network
Bird
Cat
Dog
Deer
Bird
Bird
Neural Network 
M1
M10
M11
M12
MBest
MMax.Epoch
Fig. 1: Overview of existing and proposed evaluation methods for testing the
robustness of adversarially trained network against adversarial attacks. For ex-
isting evaluation, best model’s robustness against adversarial attack is tested by
obtaining it’s prediction on adversarial samples generated by itself. Whereas, for
the proposed evaluation method adversarial samples are not only generated by
best model but also by the intermediate models obtained while training.
– Harnessing the above observations, we propose a novel variant of adversarial
training, termed “Gray-box Adversarial Training” that uses our gray-box
perturbations in order to learn robust models.
The paper is organized as follows: section 2 introduces the notation followed in
the subsequent sections of the paper, section 3 presents the drawbacks in the
current robustness evaluation methods for deep networks and proposes improved
procedure, section 4 hosts the experiments and results, section 5 discusses exist-
ing works that are relevant, and section 6 concludes the paper.
2
Notations and terminology
In this section we deﬁne the notations followed throughout this paper:
– x : clean image from the dataset.
– x∗: a potential adversarial image corresponding to the image x.
– ytrue : ground truth label corresponding to the image x.
– ypred : prediction of the neural network for the input image x.
– ǫ : deﬁnes the strength of perturbation added to the clean image.
– θ : parameters of the neural network.
– J : loss function used to train the neural network.
4
Vivek B.S., K.R. Mopuri, and R.V. Babu
– ∇xJ : gradient of the loss J with respect to image x.
– Best Model (M Best): Model corresponding to least validation loss, typically
obtained at the end of the training.
– M i
t : represents model at ith epoch of training, obtained when network M is
trained using method ‘t’.
3
Gray-box Adversarial Attacks
3.1
Limitations of existing evaluation method
Existing ways of evaluating an adversarially trained network consists of eval-
uating the best model’s accuracy on adversarial samples generated by itself.
This way of evaluating the networks gives false inference about their robustness
against adversarial attacks. This assumes (though explicitly not mentioned) that
robustness to the adversarial samples generated by the best model extends to
the adversarial samples generated by the intermediate models evolved during the
training, crafted via linear approximation of the loss (e.g., FGSM [4]). Clearly,
this is not true, as shown by our robustness plots (see Figure 2). We show this
by obtaining robustness plot which captures accuracy of best-model not only
on adversarial samples generated by itself but also on adversarial samples gen-
erated by the intermediate models, which are obtained during training. Based
on the above facts we propose two new ways of evaluating adversarially trained
network, shown in Table 1.
3.2
Robustness plot and Worst-case performance
We propose a new way of evaluating the robustness of a network, which is a plot
of recognition accuracy of the best model M Best on adversarial samples of dif-
ferent perturbation strengths ǫ, generated by multiple intermediate models that
are obtained during training. That is, performance of the model under investi-
gation is evaluated against the adversaries of diﬀerent perturbation strengths ǫ,
generated by checkpoints or models saved during training. Based on the source
of these saved models which seed adversarial samples, we diﬀerentiate robustness
plot into two categories.
– If the saved models and the best model are obtained from the same network
and also they both have the same training procedure, then we name such
robustness plot as Extended White-box robustness plot. Further, we name the
attacks as “Extended White-box adversarial attacks”.
– Else, if the network trained or the training procedure used are diﬀerent, then
we call such robustness plot as Extended Black-box robustness plot and such
attacks as “Extended Black-box adversarial attacks”.
In general, we call these attacks as Gray-box adversarial attacks. We believe
it is intuitive to call the proposed attacks as “Extensions” to existing white and
black box attacks. White-box attack means the attacker has full access to the
GAT
5
Table 1: List of the adversarial attacks. Note that the subscript denotes the
training procedure how the model is trained, superscript denotes the training
epoch at which the model is considered. M i denotes intermediate model and
M Best denotes the best model.
Source
Target Name of the attack
M Best
Adv.
M Best
Adv.
White-box attack
XBest
normal
M Best
Adv.
Black-box attack
M i
Adv. i = 1, . . . , MaxEpoch
M Best
Adv.
Extended White-box attack
Xi
Normal/Adv. i = 1, . . . , MaxEpoch M Best
Adv.
Extended Black-box attack
target model: architecture, parameters, training data and procedure. Typically,
source model that creates adversaries is same as the target model under attack.
Whereas, “extended white-box” attack means, some aspects of the setup are
known while some are not. Speciﬁcally, the architecture and the training proce-
dure of the source and target model are same while the model parameters diﬀer.
Similarly, black-box attack means the scenario where the attacker has no infor-
mation about the target such as architecture, parameters, training procedure,
etc. Generally, the source model would be a fully trained model that has diﬀerent
architecture (and hence parameters) compared to the target model. However, the
“extended black-box” attack mean, the source model can be a partially trained
model having diﬀerent network architecture. Note that it is not very diﬀerent
compared to the existing black-box attack except the source model can now be a
partially trained model. We jointly call these two extended attacks as “Gray-box
Adversarial Attacks”. Table 1 lists the deﬁnitions of both the existing and our
extended attacks with the notation we introduced earlier in the paper.
Worst-case performance (Aw): We introduce a metric derived from the
proposed robustness plot to infer the susceptibility of the trained model quantita-
tively in terms of its weakest performance. We name it “Worst-case Performance
(Aw)” of the model, which is the least recognition accuracy achieved for a given
attack strength (ǫ). Ideally, for a robust model the value of Aw should be high
(close to its performance on clean samples).
3.3
Which attack to use for adversarial training, FGSM, FGSM-LL
or FGSM-Rand?
Kurakin et al. [9] suggested to use FGSM-LL or FGSM-Rand variants for ad-
versarial training, in order to reduce the eﬀect of label leaking [9]. Label leaking
eﬀect is observed in adversarially trained networks, where the accuracy of the
model on the adversarial samples of higher perturbation is greater than that
on the adversarial samples of lower perturbation. It is necessary for adversarial
training to include stronger attacks in the training process in order to make the
model robust. We empirically show (in sec. 4.2 and Fig. 3) that FGSM is stronger
attack compared to FGSM-LL and FGSM-Rand through robustness plots with
the three diﬀerent attacks for a normally trained network.
6
Vivek B.S., K.R. Mopuri, and R.V. Babu
In addition, for models (of same architecture) adversarially trained using
FGSM, FGSM-LL and FGSM-Rand attacks respectively, FGSM attack causes
more damage compared to other two attacks. We show (in sec. 4.2 and Fig. 4)
this by obtaining robustness plots with FGSM, FGSM-LL, and FGSM-Rand
attacks respectively, for all three variants of adversarial training methods. Based
on these observations we use FGSM for all our experiments.
3.4
Gray-box Adversarial Training
Based on the observations presented in sec. 3.1, we propose Gray-box Adversarial
Training in Algorithm 1 to alleviate the drawbacks of existing adversarial train-
ing. During training, for every iteration, we replace a portion of clean samples in
the mini-batch with its corresponding adversarial samples which are generated
not only by the current state of the network but also by one of the saved inter-
mediate models. We use “drop in training loss” as a criterion for saving these
intermediate models during training i.e., for every D drop in the training loss
we save the model and this process of saving the models continues until training
loss reaches minimum preﬁxed value EL (End Loss).
The intuition behind using “drop in training loss” as a criterion for sav-
ing the intermediate models is that, models substantially apart in the training
(evolution) process can source diﬀerent set of adversaries. Having variety of ad-
versarial samples to participate in the adversarial training procedure makes the
model robust [21]. As training progresses, network representations evolve and
loss decreases from the initial value. Thus, we use the “drop in training loss”
as a useful index to pick source models that can potentially generate diﬀerent
adversaries. We represent this quantity as D in the proposed algorithm.
Ideally we would like to have an ensemble of as many diﬀerent source models
as possible. However, bigger ensembles would pose additional challenges such as
computational, memory overheads and slow down the training process. There-
fore, D has to be picked depending on the trade-oﬀbetween performance and
overhead. Note that too small value of D will include highly correlated models
in the ensemble. Also, towards later stages of the training, model evolves very
slowly and representations would not change signiﬁcantly. So, after some time
into the training, we stop augmenting the ensemble of source models. For this we
deﬁne a parameter denoted as EL (End Loss) which is a threshold on the loss
that deﬁnes when to stop saving the intermediate models. During the training
process, once the loss falls below EL, we stop saving of intermediate models and
prevent augmenting the ensemble with redundant models.
Further, in the best case we would like to pick diﬀerent saved model for
every iteration during our Gray-box Adversarial Training. However, this creates
bottleneck because loading a saved model at each iteration is time consuming.
In order to reduce this additional overhead, we pick a saved model and use this
for T consecutive iterations after which we pick another saved model in a round-
robin fashion. In total, we have three hyper-parameters namely, D, EL, and T,
and we show the eﬀect of these hyper-parameters in sec. 4.3.
GAT
7
Algorithm 1: Gray-box adversarial training of network N
Input:
m = Size of the training minibatch
k = No. of adversarial images in minibatch generated using current
state of the network N
p = No. of adversarial images in minibatch generated using ith state of
the network N
MaxItertion = Maximum training iterations
Number of clean samples in minibatch = m −k −p
Hyper-parameters EL, D and T
1 Initialization
Randomly initialize network N
/*Set containing the iterations at which seed models are saved*/
AdvBag ={}
AdvPtr=0 /*Pointer to elements in set ‘AdvBag’*/
i=0 /*Refers to initial state of the network*/
Initialize LossSetPoint with initial training loss
iteration = 0
2 while iteration ̸= MaxItertion do
3
Read minibatch B = {x1, ..., xm} from training set
4
Generate ‘k’ adversarial examples {x1
adv, ..., xk
adv} from corresponding clean
samples {x1, ..., xk} using current state of the network N
5
Generate ‘p’ adversarial examples {xk+1
adv , ..., xk+p
adv } from corresponding clean
samples {xk+1, ..., xk+p} using ith state of the network N
6
Make new minibatch B∗= {x1
adv, ..., xk
adv, xk+1
adv , ..., xk+p
adv , xk+p+1, ..., xm}
7
/*forward pass, compute loss, backward pass, and update parameters*/
8
Do one training step of Network N using minibatch B∗
9
/*moving average loss computed over 10 iterations*/
LossCurrentV alue = MovingAverage(loss)
10
/*Logic for saving seed model */
11
if (LossSetPoint −LossCurrentV alue) ≥D and LossSetPoint ≥EL then
12
AdvBag.add(iteration)
13
SaveModel(N iteration)
14
LossSetPoint = LossCurrentV alue
15
end
16
/*Logic for picking saved seed model*/
17
if (iteration%T) == 0 and len(AdvBank) ̸= 0 then
18
i = AdvBank[AdvPtr]
19
AdvPtr = (AdvPtr + 1)%len(AdvBank)
20
end
21
iteration = iteration + 1
22 end
4
Experiments
In our experiments we show results on CIFAR-100, CIFAR-10 [8], and MNIST
[10] dataset. We work with WideResNet-28-10 [22] for CIFAR-100, ResNet-18 [6]
8
Vivek B.S., K.R. Mopuri, and R.V. Babu
for CIFAR-10 and LeNet [11] for MNIST dataset, all these networks achieve
near state of the art performance on the respective dataset. These networks are
trained for 100 epochs (25 epochs for LeNet) using SGD with momentum, and
models are saved at the end of each epoch. For learning rate scheduling, step-
policy is used. We pre-process images to be in [0, 1] range, and random crop and
horizontal ﬂip are performed for data-augmentation (except for MNIST). Exper-
iments and results on MNIST dataset are shown in supplementary document.
4.1
Limitations of existing evaluation method
In this subsection, we present the relevant experiments to understand the issues
present in the existing evaluation method as discussed in Section. 3.1. We adver-
sarially train, WideResNet-28-10 and ResNet-18 on CIFAR-100 and CIFAR-10
datasets respectively and while training, FGSM is used for adversarial sam-
ple generation process. After training, we obtain their corresponding Extended
White-box robustness plot using FGSM attack. Figure 2 shows the obtained Ex-
M1
Adv.
M20
Adv.
M40
Adv.
M60
Adv.
M80
Adv.
M100
Adv.
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.
Result of Existing Evaluation
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
(a) CIFAR-100
M1
Adv.
M20
Adv.
M40
Adv.
M60
Adv.
M80
Adv.
M100
Adv.
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.
Result of Existing Evaluation
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
(b) CIFAR-10
Fig. 2: Extended White-box robustness plots with FGSM attack, obtained for (a)
WideResNet-28-10 adversarially trained on CIFAR-100 dataset, (b) ResNet-18
adversarially trained on CIFAR-10 dataset. Note that the classiﬁcation accuracy
of the best model (M Best
Adv ) is poor for the attacks generated by early models
(towards origin) as opposed to that by the later models.
tended White-box robustness plot. It can be observed that adversarially trained
networks are not robust to the adversaries generated by the intermediate mod-
els, which the existing way of evaluation fails to capture. It also infers that the
implicit assumption of best model’s robustness to adversaries generated by the
intermediate models is false. We also reiterate the fact that existing adversar-
ial training formulation does not make the network robust but makes them to
generate weaker adversaries.
GAT
9
M1
Nor.
M20
Nor.
M40
Nor.
M60
Nor.
M80
Nor.
M100
Nor.
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Nor.
Attack Method: FGSM
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Nor.
M20
Nor.
M40
Nor.
M60
Nor.
M80
Nor.
M100
Nor.
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Nor.
Attack Method: FGSM-LL
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Nor.
M20
Nor.
M40
Nor.
M60
Nor.
M80
Nor.
M100
Nor.
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Nor.
Attack Method: FGSM-Rand
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
Fig. 3: Extended White-box robustness plots of WideResNet-28-10 normally
trained on CIFAR-100 dataset, obtained using FGSM (column-1), FGSM-LL
(column-2) and FGSM-Rand (column-3) attacks. Note that for a wide range of
perturbations, FGSM attack causes more dip in the accuracy
4.2
Which attack to use for adversarial training, FGSM, FGSM-LL
or FGSM-Rand?
We perform normal training of WideResNet-28-10 on CIFAR-100 dataset and
obtain its corresponding Extended White-box robustness plots using FGSM,
FGSM-LL, and FGSM-Rand attacks respectively. Figure 3 shows the obtained
plots. It is clear that FGSM attack (column-1) produces stronger attacks com-
pared to FGSM-LL (column-2) and FGSM-Rand (column-3) attacks. That is,
drop in the model’s accuracy is more for FGSM attack.
Additionally, we adversarially train the above network using FGSM, FGSM-
LL and FGSM-Rand respectively for adversarial sample generation process. Af-
ter training, we obtain robustness plots with FGSM, FGSM-LL and FGSM-Rand
attacks respectively. Figure 4 shows the obtained Extended White-box robust-
ness plots for all the three versions of adversarial training. It is observed that
the model is more susceptible to FGSM attack (column-1) compared to other
two attacks. Similar trends are observed for networks trained on CIFAR-10 and
MNIST datasets and corresponding results are shown in supplementary docu-
ment.
4.3
Gray-box adversarial training
We train WideResNet-28-10 and Resent-18 on CIFAR-100 and CIFAR-10 datasets
respectively, using the proposed Gray-box Adversarial Training (GAT) algo-
rithm. In order to create strong adversaries, we chose FGSM to generate ad-
versarial samples. We use the same set of hyper-parameters D= 0.2, EL= 0.5,
and T= (1/4th of an epoch), for all the networks trained. Speciﬁcally, we save
intermediate models for every 0.2 drop (D) in the training loss till the loss falls
below 0.5 (EL). Also, each of the saved models are sampled from the ensemble,
generates adversarial samples for 1/4th of an epoch. After training, we obtain
Extended White-box robustness plots and Extended Black-box robustness plots
for networks trained with the Gray-box adversarial training (Algorithm 1) and
for adversarially trained networks. Figure 5 shows the obtained robustness plots,
it is observed from the plots that in the proposed gray-box adversarial training
10
Vivek B.S., K.R. Mopuri, and R.V. Babu
M1
Adv.FGSM
M40
Adv.FGSM
M80
Adv.FGSM
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.FGSM
Attack Method: FGSM
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Adv.FGSM
M40
Adv.FGSM
M80
Adv.FGSM
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.FGSM
Attack Method: FGSM-LL
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Adv.FGSM
M40
Adv.FGSM
M80
Adv.FGSM
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.FGSM
Attack Method: FGSM-Rand
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Adv.FGSM−LL
M40
Adv.FGSM−LL
M80
Adv.FGSM−LL
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.FGSM−LL
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Adv.FGSM−LL
M40
Adv.FGSM−LL
M80
Adv.FGSM−LL
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.FGSM−LL
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Adv.FGSM−LL
M40
Adv.FGSM−LL
M80
Adv.FGSM−LL
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.FGSM−LL
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Adv.FGSM−Rand
M40
Adv.FGSM−Rand
M80
Adv.FGSM−Rand
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.FGSM−Rand
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Adv.FGSM−Rand
M40
Adv.FGSM−Rand
M80
Adv.FGSM−Rand
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.FGSM−Rand
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Adv.FGSM−Rand
M40
Adv.FGSM−Rand
M80
Adv.FGSM−Rand
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.FGSM−Rand
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
Fig. 4: Extended White-box robustness plots of WideResNet-28-10 trained on
CIFAR-100 dataset using diﬀerent adversarial training methods, obtained us-
ing FGSM (column-1), FGSM-LL (column-2) and FGSM-Rand (column-3) at-
tacks. Rows represents training method used, Row-1 : Adversarial training using
FGSM, Row-2 : Adversarial training using FGSM-LL and Row-3 : Adversarial
training using FGSM-Rand.
(row-2) there are no deep valleys in the robustness plots, whereas for the net-
works trained using existing adversarially training method (row-1) exhibits deep
valley in the robustness plots. Table 2 presents the worst-case accuracy Aw of the
models trained using diﬀerent training methods. Note that Aw is signiﬁcantly
better for the proposed GAT compared to the model trained with AT and EAT
for a wide range of attack strength (ǫ).
Eﬀect of hyper-parameters: In order to study the eﬀect of hyper pa-
rameters, we train ResNet-18 on CIFAR-10 dataset using Gray-box adversarial
training for diﬀerent hyper-parameter settings. Extended White-box robustness
plots are obtained for each setting with two of them ﬁxed and the other being
varied. The hyper-parameter D deﬁnes the value of “drop in training loss” for
which intermediate models are saved to generate adversarial samples. Figure 6(a)
shows the eﬀect of varying D from 0.2 to 0.4. It is observed that for higher values
of D, the depth and the width of the valley increases. This is because choosing
higher values of D might miss saving of models that are potential sources for
generating stronger adversarial samples, and also choosing very low values of D
will results in saving large number of models that are redundant and may not
be useful. The hyper-parameter EL decides when to stop saving of intermediate
seed models. Figure 6(b) shows the eﬀect of EL for ﬁxed values of D and T. We
GAT
11
M1
Nor
M20
Nor
M40
Nor
M60
Nor
M80
Nor
M100
Nor
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.
Source Model: Normally trained
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Adv.
M20
Adv.
M40
Adv.
M60
Adv.
M80
Adv.
M100
Adv.
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.
Source Model: Adv. trained
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Gray.Adv.
M20
Gray.Adv.
M40
Gray.Adv.
M60
Gray.Adv.
M80
Gray.Adv.
M100
Gray.Adv.
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.
Source Model: Gray.Adv. trained
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Nor
M20
Nor
M40
Nor
M60
Nor
M80
Nor
M100
Nor
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Gray.Adv.
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Adv.
M20
Adv.
M40
Adv.
M60
Adv.
M80
Adv.
M100
Adv.
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Gray.Adv.
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Gray.Adv.
M20
Gray.Adv.
M40
Gray.Adv.
M60
Gray.Adv.
M80
Gray.Adv.
M100
Gray.Adv.
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Gray.Adv.
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
Source of intermediate models
(a) CIFAR-100
M1
Nor
M20
Nor
M40
Nor
M60
Nor
M80
Nor
M100
Nor
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.
Source Model: Normally trained
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Adv.
M20
Adv.
M40
Adv.
M60
Adv.
M80
Adv.
M100
Adv.
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.
Source Model: Adv. trained
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Gray.Adv.
M20
Gray.Adv.
M40
Gray.Adv.
M60
Gray.Adv.
M80
Gray.Adv.
M100
Gray.Adv.
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.
Source Model: Gray.Adv. trained
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Nor
M20
Nor
M40
Nor
M60
Nor
M80
Nor
M100
Nor
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Gray.Adv.
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Adv.
M20
Adv.
M40
Adv.
M60
Adv.
M80
Adv.
M100
Adv.
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Gray.Adv.
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Gray.Adv.
M20
Gray.Adv.
M40
Gray.Adv.
M60
Gray.Adv.
M80
Gray.Adv.
M100
Gray.Adv.
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Gray.Adv.
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
Source of intermediate models
(b) CIFAR-10
Fig. 5: Robustness plots obtained using FGSM attack for (a)WideResNet-28-10
trained on CIFAR-100 and (b) ResNet-18 trained on CIFAR-10. Rows repre-
sents the training method, Row-1:Model trained adversarially using FGSM and
Row-2:Model trained using Gray-box adversarial training. Adversarial samples
are generated by intermediate models of Column-1:Normal training, Column-2:
Adversarial training using FGSM, Column-3:Gray-box adversarial training
observe that as EL increases the width of the valley increases since higher values
of EL prevents saving of potential models. Finally, the hyper-parameter T de-
cides the duration for which a member of ensemble is used after getting sampled
from the ensemble to generate adversarial samples. Figure 6(c) shows the eﬀect
of varying T from 1/4th epoch to 3/4th epoch. Note that T has minimal eﬀect
on the robustness plot within that range.
12
Vivek B.S., K.R. Mopuri, and R.V. Babu
M1
Gray.Adv
M20
Gray.Adv
M40
Gray.Adv
M60
Gray.Adv
M80
Gray.Adv
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Gray.Adv
D : 0.2
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Gray.Adv
M20
Gray.Adv
M40
Gray.Adv
M60
Gray.Adv
M80
Gray.Adv
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Gray.Adv
D : 0.3
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Gray.Adv
M20
Gray.Adv
M40
Gray.Adv
M60
Gray.Adv
M80
Gray.Adv
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Gray.Adv
D : 0.4
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
(a) Eﬀect of D: With EL = 0.5, T = 0.25 epoch
M1
Gray.Adv
M20
Gray.Adv
M40
Gray.Adv
M60
Gray.Adv
M80
Gray.Adv
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Gray.Adv
EL : 0.5
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Gray.Adv
M20
Gray.Adv
M40
Gray.Adv
M60
Gray.Adv
M80
Gray.Adv
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Gray.Adv
EL : 0.75
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Gray.Adv
M20
Gray.Adv
M40
Gray.Adv
M60
Gray.Adv
M80
Gray.Adv
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Gray.Adv
EL : 1.0
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
(b) Eﬀect of EL: With D = 0.2, T = 0.25 epoch
M1
Gray.Adv
M20
Gray.Adv
M40
Gray.Adv
M60
Gray.Adv
M80
Gray.Adv
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Gray.Adv
T : 0.25
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Gray.Adv
M20
Gray.Adv
M40
Gray.Adv
M60
Gray.Adv
M80
Gray.Adv
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Gray.Adv
T : 0.5
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Gray.Adv
M20
Gray.Adv
M40
Gray.Adv
M60
Gray.Adv
M80
Gray.Adv
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Gray.Adv
T : 0.75
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
(c) Eﬀect of T: With D = 0.2, EL = 0.5
Fig. 6: Extended White-box robustness plots of ResNet-18 trained on CIFAR-10
dataset using Gray-box adversarial training (algorithm 1), for diﬀerent hyper-
parameters settings. Eﬀects of hyper-parameters are shown in (a) Eﬀect of D,
(b) Eﬀect of EL and (c) Eﬀect of T (measured in fraction of an epoch).
Table 2: Worst case accuracy of models trained on (a) CIFAR-100 and (b)
CIFAR-10, using diﬀerent training methods. For ensemble adversarial training
(EAT) refer to section 4.4. A, B and C refers to the ensemble used in EAT.
(a) CIFAR-100
Training
Aw for various ǫ
Method
2/255 4/255 8/255 16/255
Normal
20.81 11.58
7.09
4.8
AT
70.04 62.34 44.04
18.58
EAT
A
65.39 55.55
36.8
15.2
B
65.45 54.63 35.65
14.26
C
65.71 56.05 37.43
15.98
GAT (ours) 70.84 66.5 65.43 67.41
(b) CIFAR-10
Training
Aw for various ǫ
Method
2/255 4/255 8/255 16/255
Normal
38.15 20.84
12.2
9.34
Adversarial 87.08 79.25 56.09
22.87
EAT
A
83.27
73.4
53.96
31.01
B
82.56 72.35 56.11
34.84
C
82.64 73.84 55.19
32.33
GAT (ours) 89.46 85.89 79.28 60.81
GAT
13
Table 3: Setup for ensemble adversarial training
Network to be trained
Pre-trained Models
Held-out Model
WideResNet-28-10(Ensemble-A)
Resnet-50, ResNet-34
WideResNet-28-10
CIFAR-100
WideResNet-28-10(Ensemble-B)
WideResNet-28-10, ResNet-50
ResNet-34
WideResNet-28-10(Ensemble-C)
WideResNet-28-10, ResNet-34
ResNet-50
ResNet-34(Ensemble-A)
ResNet-34, ResNet-18
VGG-16
CIFAR-10
ResNet-34(Ensemble-B)
ResNet-34, VGG-16
ResNet-18
ResNet-34(Ensemble-C)
ResNet-18, VGG-16
ResNet-34
4.4
Ensemble adversarial training
In this subsection, we compare our Gray-box Adversarial Training against En-
semble Adversarial Training [21]. We work with the networks on CIFAR-100 and
CIFAR-10 datasets. Ensemble adversarial training uses ﬁxed set of pre-trained
models for generating adversarial samples along with the current state of the
network. For each iteration during training, the source model for generating ad-
versaries is picked at random among the current and ensemble models. Table 3
shows the setup used for ensemble adversarial training, which contains network
to be trained, pre-trained source models used for generating adversarial samples
and the held-out model used for black-box attack. Figure 7 shows the Extended
White-box robustness plots for the networks trained using ensemble adversar-
ial training algorithm. Note the presence of deep and wide valleys in the plot.
Whereas, the Extended White-box robustness plots for the models trained using
the Gray-box adversarial training shown in ﬁgure 5 (row:2,column:3), do not
have deep and wide valley. Also, because of the space restrictions, Extended
Black-box robustness plots for the above trained networks using ensemble ad-
versarial training algorithm are shown in supplementary document.
5
Related Works
Following the ﬁndings of Szegedy et al. [20], various attack methods (e.g. [3, 4,
14, 15, 16]) and various defense techniques (e.g. [4, 5, 13, 21, 19]) have been
proposed. On the defense side, adversarial training shows promising results. In
order to scale adversarial training [9] to large datasets, single-step attack meth-
ods (use 1st order approximation of model’s loss to generate attack ) are used
while training. Goodfellow et al. [4] observed that adversarially trained mod-
els, incurs higher loss on transferred samples than on the white-box single-step
attacks. Further, Kurakin [9] observed that adversarially trained models were
susceptible to adversarial samples generated using multi-step methods under
white-box setting. This paradoxical behaviour is explained by Tram`er et al. [21]
through the inaccuracy of linear approximation of the model’s loss function in the
vicinity of the data samples. Madry et al. [13] showed that adversarially trained
model can become robust against white-box attacks, if adversaries added during
training closely maximizes the model’s loss. However, such methods are hard to
scale to diﬃcult tasks such as ILSVRC [9]. Tram`er et al. [21] showed that having
diversity in the adversarial samples presented during the training can alleviate
14
Vivek B.S., K.R. Mopuri, and R.V. Babu
M1
Adv.Ens
M20
Adv.Ens
M40
Adv.Ens
M60
Adv.Ens
M80
Adv.Ens
M100
Adv.Ens
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.Ens
Ensemble-A
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Adv.Ens
M20
Adv.Ens
M40
Adv.Ens
M60
Adv.Ens
M80
Adv.Ens
M100
Adv.Ens
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.Ens
Ensemble-B
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Adv.Ens
M20
Adv.Ens
M40
Adv.Ens
M60
Adv.Ens
M80
Adv.Ens
M100
Adv.Ens
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.Ens
Ensemble-C
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
(a) CIFAR-100
M1
Adv.Ens
M20
Adv.Ens
M40
Adv.Ens
M60
Adv.Ens
M80
Adv.Ens
M100
Adv.Ens
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.Ens
Ensemble-A
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Adv.Ens
M20
Adv.Ens
M40
Adv.Ens
M60
Adv.Ens
M80
Adv.Ens
M100
Adv.Ens
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.Ens
Ensemble-B
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
M1
Adv.Ens
M20
Adv.Ens
M40
Adv.Ens
M60
Adv.Ens
M80
Adv.Ens
M100
Adv.Ens
Model from which adversarial samples
are generated
0
10
20
30
40
50
60
70
80
90
100
Accuracy of MBest
Adv.Ens
Ensemble-C
ǫ = 2/255
ǫ = 4/255
ǫ = 6/255
ǫ = 8/255
ǫ = 10/255
ǫ = 12/255
ǫ = 14/255
ǫ = 16/255
(b) CIFAR-10
Fig. 7: Extended White-box robustness plots of models trained using ensemble
adversarial training algorithm. (a)models trained on CIFAR-100 dataset and
(b)models trained on CIFAR-10 dataset
the eﬀect of gradient masking. However, it is ineﬃcient to have an ensemble of
diﬀerent pre-trained models to seed the adversarial samples. Apart from high-
lighting the ﬂaws in the existing evaluation and proposing better evaluation
method, we propose eﬃcient way of generating diverse adversarial samples for
learning robust models.
6
Conclusions
Presence of adversarial samples indicates the vulnerability of the machine learn-
ing models. Learning robust models and measuring their susceptibility against
adversarial attacks are need of the day. In this paper, we demonstrated impor-
tant gaps in the existing evaluation method for testing the robustness of a model
against adversarial attacks. Also, we proposed a novel evaluation method called
“Robustness plots” and a derived metric “Worst-case performance (Aw)”. From
the proposed evaluation method, it is observed that the existing adversarial train-
ing methods which use ﬁrst order approximation of loss function for generating
samples, do not make the model robust. Instead, they make the model to gen-
erate weaker adversaries. Finally, the proposed “Gray-box Adversarial Training
(GAT)” which harnesses the presence of stronger adversaries during training,
achieves better robustness against adversarial attacks compared to the exist-
ing adversarial training methods (that follow single step adversarial generation
process).
GAT
15
References
1. Biggio, B., Corona, I., Maiorca, D., Nelson, B., ˇSrndi´c, N., Laskov, P., Giacinto, G.,
Roli, F.: Evasion attacks against machine learning at test time. In: Joint European
Conference on Machine Learning and Knowledge Discovery in Databases. pp. 387–
402 (2013) 1
2. Biggio, B., Fumera, G., Roli, F.: Pattern recognition systems under attack: Design
issues and research challenges. International Journal of Pattern Recognition and
Artiﬁcial Intelligence 28(07) (2014) 1
3. Carlini, N., Wagner, D.A.: Towards evaluating the robustness of neural networks.
In: 2017 IEEE Symposium on Security and Privacy, SP 2017, San Jose, CA, USA,
May 22-26, 2017. pp. 39–57 (2017) 13
4. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial
examples. In: International Conference on Learning Representations (ICLR) (2015)
1, 2, 4, 13
5. Guo, C., Rana, M., Cisse, M., van der Maaten, L.: Countering adversarial images
using input transformations. In: International Conference on Learning Represen-
tations (ICLR) (2018) 13
6. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR).
pp. 770–778 (2016) 7
7. Huang, L., Joseph, A.D., Nelson, B., Rubinstein, B.I., Tygar, J.D.: Adversarial
machine learning. In: Proceedings of the 4th ACM Workshop on Security and
Artiﬁcial Intelligence. AISec ’11 (2011) 1
8. Krizhevsky, A.: Learning multiple layers of features from tiny images. Tech. rep.,
University of Toronto (2009) 7
9. Kurakin, A., Goodfellow, I.J., Bengio, S.: Adversarial machine learning at scale.
In: International Conference on Learning Representations (ICLR) (2017) 2, 5, 13
10. LeCun, Y.: The mnist database of handwritten digits http://yann.lecun.com/
exdb/mnist/ 7
11. LeCun, Y., et al.: Lenet-5, convolutional neural networks http://yann.lecun.com/
exdb/lenet/ 8
12. Liu, Y., Chen, X., Liu, C., Song, D.: Delving into transferable adversarial examples
and black-box attacks. In: International Conference on Learning Representations
(ICLR) (2017) 1
13. Madry, A., Makelov, A., Schmidt, L., Dimitris, T., Vladu, A.: Towards deep learn-
ing models resistant to adversarial attacks. In: International Conference on Learn-
ing Representations (ICLR) (2018) 2, 13
14. Moosavi-Dezfooli, S.M., Fawzi, A., Frossard, P.: Deepfool: A simple and accurate
method to fool deep neural networks. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR). pp. 2574–2582 (2016) 1, 13
15. Mopuri, K.R., Garg, U., Babu, R.V.: Fast feature fool: A data independent ap-
proach to universal adversarial perturbations. In: Proceedings of the British Ma-
chine Vision Conference (BMVC) (2017) 13
16. Mopuri, K.R., Ojha, U., Garg, U., Babu, R.V.: NAG: Network for adversary gen-
eration. In: Proceedings of the IEEE Computer Vision and Pattern Recognition
(CVPR) (2018) 13
17. Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z.B., Swami, A.: The
limitations of deep learning in adversarial settings. In: Security and Privacy (Eu-
roS&P), 2016 IEEE European Symposium on. pp. 372–387. IEEE (2016) 1
16
Vivek B.S., K.R. Mopuri, and R.V. Babu
18. Papernot, N., McDaniel, P.D., Goodfellow, I.J., Jha, S., Celik, Z.B., Swami, A.:
Practical black-box attacks against deep learning systems using adversarial exam-
ples. In: Asia Conference on Computer and Communications Security (ASIACCS)
(2017) 1
19. Samangouei, P., Kabkab, M., Chellappa, R.: Defense-GAN: Protecting classiﬁers
against adversarial attacks using generative models. In: International Conference
on Learning Representations (ICLR) (2018) 13
20. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.J.,
Fergus, R.: Intriguing properties of neural networks. In: International Conference
on Learning Representations (ICLR) (2014) 1, 13
21. Tram`er, F., Kurakin, A., Papernot, N., Boneh, D., McDaniel, P.: Ensemble ad-
versarial training: Attacks and defenses. In: International Conference on Learning
Representations (ICLR) (2018) 2, 6, 13
22. Zagoruyko, S., Komodakis, N.: Wide residual networks. In: Richard C. Wilson,
E.R.H., Smith, W.A.P. (eds.) Proceedings of the British Machine Vision Conference
(BMVC). pp. 87.1–87.12. BMVA Press (September 2016) 7