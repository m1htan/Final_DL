Context Reﬁnement for Object Detection
Zhe Chen, Shaoli Huang, and Dacheng Tao
UBTECH Sydney AI Centre, SIT, FEIT, University of Sydney, Australia
{zche4307}@uni.sydney.edu.au
{shaoli.huang, dacheng.tao}@sydney.edu.au
Abstract. Current two-stage object detectors, which consists of a re-
gion proposal stage and a reﬁnement stage, may produce unreliable re-
sults due to ill-localized proposed regions. To address this problem, we
propose a context reﬁnement algorithm that explores rich contextual in-
formation to better reﬁne each proposed region. In particular, we ﬁrst
identify neighboring regions that may contain useful contexts and then
perform reﬁnement based on the extracted and uniﬁed contextual in-
formation. In practice, our method eﬀectively improves the quality of
the ﬁnal detection results as well as region proposals. Empirical studies
show that context reﬁnement yields substantial and consistent improve-
ments over diﬀerent baseline detectors. Moreover, the proposed algorithm
brings around 3% performance gain on PASCAL VOC benchmark and
around 6% gain on MS COCO benchmark respectively.
Keywords: Object Detection · Context Analysis · Deep Convolutional
Neural Network
1
Introduction
Recent top-performing object detectors, such as Faster RCNN [29] and Mask
RCNN [16], are mostly based on a two-stage paradigm which ﬁrst generates
a sparse set of object proposals and then reﬁnes the proposals by adjusting
their coordinates and predicting their categories. Despite great success, these
methods tend to produce inaccurate bounding boxes and false labels after the
reﬁnement because of the poor-quality proposals generated in the ﬁrst stage. As
illustrated in Figure 1, if a proposed region has a partial overlap with a true
object, existing methods would suﬀer reﬁnement failures since this region does
not contain suﬃcient information for holistic object perception. Although much
eﬀort such as [21] has been dedicated to enhance the quality of object proposals,
it still cannot guarantee that the proposed regions can have a satisfactory overlap
for each ground truth.
To tackle the aforementioned issue, we augment the representation for each
proposed region by leveraging its surrounding regions. This is motivated by the
fact that surrounding regions usually contain complementary information on
object appearance and high-level characteristics, e.g., semantics and geometric
relationships, for a proposed region. Diﬀerent from related approaches [36, 12, 37,
26] that mainly include additional visual features from manually picked regions
2
Zhe Chen, Shaoli Huang, and Dacheng Tao
dog
dog
cat
dog
visual features &
contexts
visual features
Proposed Regions
Context 
Refinement
Refinement
Existing Refinement
(b)
(a)
(c)
Proposed Refinement
Fig. 1. Overview of the pipeline for the proposed context reﬁnement algorithm compar-
ing to existing reﬁnement pipeline. Existing pipeline (b) reﬁnes each proposed region
by performing classiﬁcation and regression only based on visual features, while the
proposed algorithm (c) can achieve a more reliable reﬁnement by making use of both
visual cues and contexts brought by surrounding regions.
to help reﬁnement, our method is based on oﬀ-the-shelf proposals that are more
natural and more reliable than hand-designed regions. Furthermore, by using
a weighting strategy, our method can also take better advantage of contextual
information comparing to other existing methods.
In this paper, we propose a learning-based context reﬁnement algorithm
to augment the existing reﬁnement procedure. More speciﬁcally, our proposed
method follows an iterative procedure which consists of three processing steps
in each iteration. In the ﬁrst processing step, we select a candidate region from
the proposed regions and identify its surrounding regions. Next, we gather the
contextual information from the surrounding regions and then aggregate these
collected contexts into a uniﬁed contextual representation based on an adaptive
weighting strategy. Lastly, we perform context reﬁnement for the selected re-
gion based on both the visual features and the corresponding uniﬁed contextual
representation. In practice, since the proposed method requires minor modiﬁ-
cation in detection pipeline, we can implement our algorithm by introducing
additional small networks that can be directly embedded in existing two-stage
detectors. With such simplicity of design and ease of implementation, our method
can further improve the region proposal stage for two-stage detectors. Extensive
experimental results show that the proposed method consistently boosts the per-
formance for diﬀerent baseline detectors, such as Faster RCNN [29], Deformable
R-FCN [9], and Mask RCNN [16], with diversiﬁed backbone networks, such as
VGG [32] and ResNet [17]. The proposed algorithm also achieves around 3% im-
provement on PASCAL VOC benchmark and around 6% improvement on MS
COCO benchmark over baseline detectors.
Context Reﬁnement for Object Detection
3
2
Related Work
Object detection is the key task in many computer vision problems [7, 5, 18, 6].
Recently, researchers mainly adopt single-stage detectors or two-stage detectors
to tackle detection problems. Compared with single-stage detectors [27, 24, 30],
two-stage detectors are usually slower but with better detection performance
[20]. With a reﬁnement stage, two-stage detectors are shown to be powerful on
COCO detection benchmark [23] that contains many small-sized objects and
deformed objects. Over recent years, several typical algorithms [15, 29, 8, 9] have
been proposed to improve the two-stage detectors. For example, [22] developed
the feature pyramid network to address the challenge of small object detection.
[16] proposes a novel feature warping method to improve the performance of the
ﬁnal reﬁnement procedure. However, these methods are highly sensitive to the
quality of object proposals and thereby may produce false labels and inaccurate
bounding boxes on poor-quality object proposals.
To relieve this issue, post-processing methods have been widely used in two-
stage detection systems. One of the most popular among them is the iterative
bounding box reﬁnement method [12, 37, 17]. This method repeatedly reﬁnes the
proposed regions and performs a voting and suppressing procedure to obtain
the ﬁnal results. Meanwhile, rather than using a manually designed iterative
reﬁnement method, some studies [13, 14] recursively perform regression to the
proposed regions so that they can learn to gradually adapt the ground-truth
boxes. Although better performance could be achieved with more iterations of
processing, these methods are commonly computational costly. In addition, some
other studies adopt a re-scoring strategy. For example, the paper [2] tends to
progressively decrease the detection score of overlapped bounding boxes, lowering
the risk of keeping false positive results rather than more reliable ones, while
Hosang et al. [19] re-scores detection with a learning-based algorithm. However,
the re-scoring methods do not consider contexts, thus only oﬀering limited help
in improving the performance.
More related studies refer visual contexts to improve object detection. Even
without the powerful deep convolutional neural networks (DCNNs), the advan-
tages of using contexts for object detection have already been demonstrated in
[10, 35, 25]. In recent years, many studies [36, 12, 37, 26] attempt to further incor-
porate contexts in DCNN. In general, they propose to utilize additional visual
features from context windows to facilitate detection. A context window is com-
monly selected based on a slightly larger or smaller region comparing to the
corresponding proposed regions. The visual features inside each context window
will be extracted and used as contextual information for the ﬁnal reﬁnement of
each region. However, since context windows are commonly selected by hand,
the considered regions still have a limited range and surrounding contexts may
not be fully exploited. Instead of using context windows, some studies [1, 28, 4]
attempt to employ recurrent neural networks to encode contextual information.
For example, the ION detector [1] attempts to collect contexts by introducing
multi-directional recurrent neural network, but the resulting network becomes
much more complicated and it requires careful initialization for stable training.
4
Zhe Chen, Shaoli Huang, and Dacheng Tao
!(#$,#&)> )
*+$
,
*-$
,
!(#$,#+) >)
!(#$,#-) >)
#$
#&
#+
*&$
,
,.&$
,.+$
,.-$
⋯
bird
similarity 
estimation
context regions
context features & 
their weights
*0$, = 2
*3$
, ,.3$
context aggregation
*0$,
*$
context refinement
cls
reg
fc1 fc2
Context Refined Result
DCNN
visual feature extraction
cls
reg
fc1 fc2
original refinement
Results
Input
*$
*&
*+
*-
*4
⋯
ASelected Region
bird
bird
bird
!(#$,#4)< )
#4
ignore
bird
other regions
proposals
Fig. 2. The detailed working ﬂow of the proposed context reﬁnement algorithm for im-
proving the original reﬁnement (best view in color). Regarding each selected region, our
algorithm ﬁrst identiﬁes its surrounding regions that may carry useful context based
on a correlation estimation procedure. Afterwards, all the contextual information is
aggregated to form a uniﬁed representation based on an adaptive weighting strategy.
Using both the aggregated contexts and visual features extracted from DCNN, the pro-
posed context reﬁnement algorithm is able to improve the quality of detection results.
The detailed deﬁnitions of the math symbols can be found in Section 3.
Nevertheless, most of the prevailing context-aware object detectors only con-
sider contextual features extracted from DCNNs, lacking the consideration of
higher-level surrounding contexts such as semantic information and geometric
relationship.
3
Context Reﬁnement for Object Detection
Diﬀerent from existing studies that mainly extract visual contexts from manu-
ally picked regions or RNNs, we propose to extensively incorporate contextual
information brought by surrounding regions to improve the original reﬁnement.
Mathematically, we deﬁne that the status of a region r is described by its four
coordinates b = (x1, y1, x2, y2) and a conﬁdence score s. Suppose vi represents
visual features extracted from the region ri bounded by bi, then original reﬁne-
ment procedure of existing two-stage detectors commonly perform the following
Context Reﬁnement for Object Detection
5
operations to reﬁne the region ri:
si = fcls(vi)
bi = freg(b0
i , vi)
(1)
where b0
i is the original coordinates of the proposed region, fcls and freg re-
spectively represent the classiﬁcation and regression operations. In a two-stage
detector, fcls is usually a soft-max operation and freg is generally a linear
regression operation. Both operations perform reﬁnement based on the inner
product between the input vector and the weight vector. The classiﬁcation op-
eration actually assigns a pre-deﬁned box (namely anchor box) with a fore-
ground/background label and assigns a proposal with a category-aware label;
the regression operation estimates the adjustment of the coordinates for the
region. As mentioned previously, two-stage detectors which reﬁne proposed re-
gions based on Eq. 1 suﬀer from the issue that ill-localized proposed regions
would result in unreliable reﬁnement, if not considering context. Based on the
observation that surrounding regions can deliver informative clues for describing
the accurate status of an object, we introduce context reﬁnement algorithm to
tackle the partial detection issue and thus improve the original reﬁnement.
The processing ﬂow of the proposed algorithm can be described as an iter-
ative three-stage procedure. In particular, the three processing stages for each
iteration include: 1) selecting candidate region and identifying its context re-
gions; 2) aggregating contextual features; and 3) conducting context reﬁnement.
Formally, we make ri represent the selected region in current iteration and fur-
ther deﬁne the surrounding regions of ri that may carry useful contexts as its
context region. In the ﬁrst stage, we select a candidate region ri and then the
context regions of ri can be properly obtained by collecting other regions that
are in the neighbourhood and closely related to the selected region. We use the
symbol Rc
i to represent the set of the obtained context regions for ri. Afterwards,
in the second stage, we extract contextual features from Rc
i and fuse these con-
texts into a uniﬁed representation, ˆvc
i , based on an adaptive weighting strategy.
For the last stage, based on both vi and ˆvc
i , we perform context reﬁnement using
the following operations:
s′
i = f c
cls(si, vi, ˆvc
i )
b′
i = f c
reg(bi, vi, ˆvc
i )
(2)
where b′
i and s′
i are the results of context reﬁnement, and f c
cls and f c
reg are the
context reﬁnement functions for classiﬁcation and regression respectively. The
detailed workﬂow of context reﬁnement for improving the detection is illustrated
in Eq. 2.
3.1
Selecting Regions
In our proposed algorithm, the ﬁrst step is to select a candidate region and iden-
tify its context regions for reﬁnement. According to Eq. 2, we perform original
reﬁnement before the ﬁrst step of our algorithm so that the regions can be ﬁrst
6
Zhe Chen, Shaoli Huang, and Dacheng Tao
enriched with semantics and meaningful geometry information. This can also
make the regions tend to cluster themselves around true objects and thus can
convey helpful context.
After the original reﬁnement, the estimated conﬁdence score can indicate the
quality of a region to some extents. In this study, we adopt a greedy strategy
to select regions in each iteration, which means that regions of higher scores
will be reﬁned with contexts earlier. When a region is selected, we then identify
its context regions for extracting contextual information. In our algorithm, the
context regions represent closely related regions, considering that these regions
could cover the same object with the selected region. In order to obtain an
adequate set of context regions, we estimate the closeness between the selected
region and the other regions. Therefore, the regions that are closer to the selected
region can form an adequate set of context regions Rc
i.
We introduce the concept of correlation level to deﬁne the closeness between
a selected region and other regions. The correlation level represents the strength
of the relationship between any two regions. We use ρ(ri, rj) to describe the
correlation level between ri and rj. Using this notation, we describe the set of
context regions for ri as:
Rc
i = {rj|ρ(ri, rj) > τ}
(3)
where τ is a threshold. In our implementation, we measure the correlation
level between two regions based on their Intersect-over-Union (IoU) score, thus
ρ(ri, rj) = IoU(bi, bj). The detailed setting for τ is deﬁned in Section 5.
3.2
Fusing Context
Context extracted from Rc
i can provide complementary information that could
be beneﬁcial for rectifying the coordinates and improving the estimated class
probabilities for the selected candidate regions. However, a major issue of using
the collected contextual information is that the number of context regions is not
ﬁxed and can range from zero to hundreds. Using an arbitrary amount of con-
textual information, it will be diﬃcult for an algorithm to conduct appropriate
reﬁnement for ri. To tackle this issue, we introduce the aggregation function g to
fuse all the collected contextual information into a uniﬁed representation based
on an adaptive weighting strategy, thus facilitating the context reﬁnement.
We use vc
ji to denote the contextual information carried by rj w.r.t ri. Then
we can build a set of contextual representation V c
i by collecting all the vc
ji from
Rc
i:
V c
i = {vc
ji|vc
ji for rj ∈Rc
i}.
(4)
Since the size of V c
i will vary according to diﬀerent selected regions, we at-
tempt to aggregate all the contexts in V c
i into a uniﬁed representation. In order
to properly realize the aggregation operation, we propose that the more related
context regions should make major contributions to the uniﬁed contextual rep-
resentation. This can further reduce the risk of distracting the reﬁnement if
Context Reﬁnement for Object Detection
7
surrounding regions are scattered. In particular, we adopt the use of an adaptive
weighting strategy to help deﬁne the aggregation function g.
Mathematically, we refer ωji as the weight of vc
ji ∈V c
i that can be adaptively
computed according to diﬀerent selected regions. Since we are assigning larger
weights to more related context regions, we attempt to estimate the relation score
between rj and ri and make ωji depend on the estimated score. Considering that
we are using semantics (i.e. classiﬁcation results) and geometry information to
deﬁne regions, it is appropriate to describe ωji as a combination of semantic
relation score ωs
ji and geometric relation score ωg
ji:
ωji = ωs
ji · ωg
ji.
(5)
We instantiate the semantic relation score ωs
ji and geometry relation score ωg
ji
using the following settings:
ωs
ji = ✶(lj = li) · sj
ωg
ji = IoU(bi, bj)
(6)
where ✶(·) is a bool function and li, lj represent the predicted labels for corre-
sponding regions. Using this setting, the context regions with lower conﬁdence
and lower overlap scores w.r.t the selected region will make minor contributions
to the uniﬁed contextual representation.
By denoting Ωi as the set of estimated ωji for vc
ji ∈V c
i , we introduce an av-
eraging operation to consolidate all the weighted contextual information brought
by a variable number of context regions. Recall that the uniﬁed contextual repre-
sentation is ˆvc
i , we implement the aggregation operation g based on the following
equation:
ˆvc
i = g({vc
ji, ωji|vc
ji ∈V c
i , ωji ∈Ωi})
(7)
where:
g({vc
ji, ωji}) =
P
j ωji · vc
ji
P
j ωji
.
(8)
3.3
Learning-based Reﬁnement
After ˆvc
i is computed by Eq. 7, we are then able to perform context reﬁnement
for each selected regions based on Eq. 2. In this paper, we introduce a learning-
based scheme to fulﬁll the context reﬁnement. More speciﬁcally, we employ fully
connected neural network layers to realize the functions f c
cls and f c
reg. By con-
catenating together the vi and ˆvc
i , the employed fully connected layers will learn
to estimate a context reﬁned classiﬁcation score s′
i and coordinates b′
i. These
fully connected layers can be trained together with original reﬁnement network.
Overall, Algorithm 1 describes the detailed processing ﬂow of the proposed
context reﬁnement algorithm over an original reﬁnement procedure. The pro-
posed algorithm is further visualized by Figure. 2.
8
Zhe Chen, Shaoli Huang, and Dacheng Tao
Algorithm 1 Context Reﬁnement
Require: A set of regions, R = {ri = (li, bi)}, that has been ﬁrst reﬁned by Eq. 1;
Ensure: A set of context reﬁned regions R′ = {r′
i = (l′
i, b′
i)};
1: R′ ←{}
2: for each selected originally reﬁned region ri ∈R do
3:
ﬁnd the set of context regions Rc
i based on Eq. 3;
4:
collect contextual representation V c
i based on Eq. 4;
5:
aggregate contexts and obtain the uniﬁed contextual representation ˆvc
i based on
Eq. 5 - Eq. 8;
6:
perform learning-based context reﬁnement for ri based on Eq. 2, obtaining l′
i
and b′
i;
7:
R′ ←R′ ∪(l′
i, b′
i);
8: end for
9: return R′
4
Embedded Architecture
Since the proposed method only alters reﬁnement operations, such as classiﬁ-
cation and regression, of current two-stage detectors, it is straightforward to
implement the proposed method by introducing an additional network that can
be directly embedded into existing two-stage object detection pipelines. Such
design is lightweight and can enable us to perform context reﬁnement for both
the ﬁnal detection results and the region proposals because the proposals can be
considered as the reﬁned results of pre-deﬁned anchor boxes.
As shown in Figure 3, we can directly attach the context reﬁnement module
to both the proposal generation stage and ﬁnal reﬁnement stage compatibly.
As mentioned previously, we attach networks for context reﬁnement after the
original reﬁnement operations. It is especially necessary to perform original re-
ﬁnement prior to our context reﬁnement for proposal generation stage because
pre-deﬁned anchor map does not contain semantic or geometric information that
can indicate the existence of objects. Moreover, such embedding design does not
revise the form of a detection result, which means that it is still possible to use
post-processing algorithms.
5
Implementation Details and Discussions
To embed context reﬁnement network in diﬀerent phases of a two-stage ob-
ject detector, we apply the following implementation. In the ﬁrst stage that
produces region proposals, we attach the network of context reﬁnement to the
top-6k proposals without performing NMS. We re-use original visual features as
useful information and also include relative geometry information (i.e. coordi-
nates oﬀsets) and semantics to enrich the instance-level contextual information
for context reﬁnement. The resulting context feature vector then has a length
of (C + 4 + K) where C is the channel dimension of visual feature and K is the
number of categories. The threshold for deﬁning context regions for proposals is
Context Reﬁnement for Object Detection
9
CNN
Proposals
CNN
Detection 
Results
Collect & Aggregate 
Contexts
NMS
Context Refinement for Proposals
Context Refinement for Detection Results
Original 
Refinement
refined regions
Context 
Refinement
Proposals
Original 
Second-stage 
Refinement
refined regions
Context 
Refinement
Collect & Aggregate 
Contexts
Anchor 
Map
Fig. 3. Embedded architecture of the proposed algorithm. This design makes the con-
text reﬁnement algorithm compatible for both region proposal generation stage and
ﬁnal reﬁnement stage in existing two-stage detection pipeline.
set as 0.5. In addition, f c
cls and f c
reg are conducted on the output of two consecu-
tive fully connected layers with ReLU non-linear activation for the ﬁrst layer. In
the second reﬁnement stage, we additionally involve the semantics estimated in
the ﬁrst context reﬁnement stage. The f c
cls and f c
reg of this stage are performed
with one fully connected layer. Other settings are kept the same. When training
the context reﬁnement network, since we are using an embedded architecture, it
is possible to ﬁx the weights of other parts of a detector to achieve much higher
training speed, which would not sacriﬁce much accuracy. The loss functions used
for training are cross entropy loss for classiﬁcation and smooth L1 loss for regres-
sion. Except that in the second stage, we additionally penalize the redundant
detection results following the strategy proposed by [19] and thus can relieve the
impacts of unnecessary detection results.
Model Complexity With the embedded design, the increase in model com-
plexity brought by context reﬁnement mainly comes from extracting and unifying
contexts brought by context regions. Therefore, the required extra complexity
would be at most O(M 2D) for using M candidate regions with the uniﬁed con-
textual feature of length D. In practice, our method will only use a small portion
of proposals for context reﬁnement. More speciﬁcally, based on Eq. 3, we can
ignore a large number of proposals with a low correlation level when performing
context reﬁnement for each candidate region. In addition, we further conduct
a thresholding procedure to eliminate the proposals with low conﬁdence scores.
As a result, our method only costs around 0.11s extra processing time when
processing 2000 proposals.
10
Zhe Chen, Shaoli Huang, and Dacheng Tao
Context Reﬁnement for Single-stage Detectors Although it is possi-
ble to realize context reﬁnement for single-stage detectors, we ﬁnd that these
detectors (e.g. SSD [24]) usually perform reﬁnement on a smaller number of re-
gions, meaning that there would not be suﬃcient surrounding contexts to access
considerable improvements.
Failure Cases In general, our method brings limited improvements in two
cases. The ﬁrst one is that the context regions are inaccurate. In this case, the
extracted contexts are not helping improve the performance. The second one is
that the number of context regions is too small to provide suﬃcient contextual
information for improvement.
6
Experiments
To evaluate the eﬀectiveness of the proposed context reﬁnement method for two-
stage object detectors, we perform comprehensive evaluations on the well-known
object detection benchmarks, including PASCAL VOC[11] and MS COCO[23].
We estimate the eﬀects of our method on ﬁnal detection results as well as region
proposals, comparing to original reﬁnement method and other state-of-the-art
detection algorithms.
6.1
PASCAL VOC
PASCAL VOC benchmark [11] is a commonly used detection benchmark which
contains 20 categories of objects for evaluating detectors. For all the following
evaluation, we train models on both VOC 07 + 12 trainval datasets and perform
the evaluation on VOC 07 test set, where mean Average Precision (mAP) will
be majorly reported as detection performance.
In this section, we apply context reﬁnement for both regional proposals and
the second reﬁnement stage in the Faster RCNN (FRCNN) detector [29]. Consid-
ering that this detector adopts region proposal network (RPN) to reﬁne anchors,
we abbreviate the context reﬁned RPN as C-RPN. We further use C-FRCNN
to represent the FRCNN whose RPN and the second reﬁnement stage are both
reﬁned with contexts. We re-implement the FRCNN following the protocol of [3]
and use † to represent this re-implemented FRCNN in following experiments.
Eﬀects on Region Proposals We ﬁrst evaluate the eﬀectiveness of the pro-
posed algorithm for region proposal network (RPN). The improvements in recall
rates w.r.t the ground-truth objects will illustrate the eﬃcacy of our method.
In this part, recall rates will be reported based on diﬀerent IoU thresholds and
diﬀerent number of proposals. It is worth noting that our method is not a novel
region proposal algorithm, thus we do not compare with SelectiveSearch [34] and
EdgeBox[38]. We only report the performance gain with respect to the original
reﬁnement performed by region proposal network.
Using diﬀerent IoU thresholds as the criteria, we report the recall rates by
ﬁxing the number of proposals in each plot, as illustrated in Figure 4. From the
Context Reﬁnement for Object Detection
11
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
RPN
C-RPN
RPN
C-RPN
RPN
C-RPN
RPN
C-RPN
300 proposals
500 proposals
1000 proposals
2000 proposals
Fig. 4. Curves for the recall rates against IoU threshold on the PASCAL VOC07 test
set for the original reﬁned region proposal network and the context reﬁned results.
C-RPN refers to the region proposal network improved with contexts.
0
500
1000
1500
2000
0
0.2
0.4
0.6
0.8
1
0
500
1000
1500
2000
0
0.2
0.4
0.6
0.8
1
0
500
1000
1500
2000
0
0.2
0.4
0.6
0.8
1
0
500
1000
1500
2000
0
0.2
0.4
0.6
0.8
1
IoU = 0.7
IoU = 0.8
IoU = 0.9
IoU = 0.6
RPN
C-RPN
RPN
C-RPN
RPN
C-RPN
RPN
C-RPN
Fig. 5. Curves for the recall rates against the number of proposals on the PASCAL
VOC07 test set for the original reﬁned region proposal network and the context reﬁned
results. C-RPN refers to the region proposal network improved with contexts.
presented plots, we can ﬁnd that although all the curves change slightly with
the number of proposals increases, the proposed context reﬁnement procedure
can consistently boost recalls rates of original reﬁnement. Especially, context
reﬁnement is able to improve the recall rates of original RPN with around 45% at
an IoU threshold of 0.8 in each plot, which validates that the proposed algorithm
is advantageous for improving the quality of region proposals.
In addition, we report the recall rates for adopting diﬀerent numbers of pro-
posals in Figure 5. In these plots, we can observe that the context reﬁnement
bring more improvements when using higher IoU thresholds as criteria. Starting
from the IoU threshold of 0.8 for computing the recall rates, the improvements
of the proposed method becomes obvious, out-performing the original reﬁne-
ment method in RPN with around 2 points for using more than 100 proposals.
With a more strict IoU threshold (i.e. 0.9), the proposals reﬁned with surround-
ing contexts can still capture 20% to 30% of ground-truth boxes, while original
reﬁnement only facilitates RPN to cover only around 7% ground-truth.
Eﬀects on Detection With the help of context reﬁnement, we not only can
boost recall rates of proposals but also can promisingly promote the ﬁnal de-
tection performance. Table 1 brieﬂy shows the ablation results of the context
12
Zhe Chen, Shaoli Huang, and Dacheng Tao
Method
AP@0.5 AP@0.7 AP@0.8
FRCNN† with RPN
0.796
0.633
0.442
FRCNN† with C-RPN
0.804
0.650
0.469
C-FRCNN†
0.822
0.685
0.485
Table 1. Ablation study on VOC 07 test set for using context reﬁnement to improve
diﬀerent reﬁnement stages in Faster RCNN (FRCNN) detector. “C-RPN” refers to
the context reﬁnement improved region proposal network (RPN). “C-FRCNN” means
the FRCNN whose both reﬁnement stages are improved with contexts. †: the FRCNN
implemented following the protocol suggested by [3].
Method
Network aero bike bird boat bottle bus
car
cat chair cow table dog horse motor person plant sheep sofa train
tv
mAP
HyperNet [21]
VGG
84.2 78.5 73.6 55.6
53.7
78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7
83.3
81.8
48.6
73.5 59.4 79.9 65.7
71.4
ION [1]
VGG
79.2 83.1 77.6 65.6
54.9
85.4 85.1 87.0 54.4 80.6 73.8 85.3 82.2
82.2
74.4
47.1
75.8 72.7 84.2 80.4
75.6
CC [26]
BN-incep 80.9 84.8 83.0 75.9 72.3 88.9 88.4 90.3 66.2 87.6 74.0 89.5 89.3
83.6
79.6
55.2
83.4 81.0 87.8 80.7
81.1
R-FCN [8]
Res101
79.9 87.2 81.5 72.0
69.8
86.8 88.5 89.8 67.0 88.1 74.5 89.8 90.6
79.9
81.2
53.7
81.8 81.5 85.9 79.9
80.5
FRCNN† [3, 29]
VGG
76.1 82.5 75.3 65.3
65.6
84.8 87.5 87.5 57.7 82.4 67.7 83.3 85.3
77.1
78.4
44.1
76.9 70.1 82.6 77.0
75.3
C-FRCNN† (ours)
VGG
79.5 83.7 77.6 69.3
67.2
84.9 87.5 87.6 61.3 83.9 72.3 85.3 85.7
80.8
83.5
49.9
79.2 73.4 83.2 76.7
77.6
FRCNN† [3, 29]
Res101
83.1 86.0 79.7 74.2
68.3
87.7 88.0 88.4 62.3 86.8 70.4 88.5 87.3
82.9
82.9
52.8
81.0 77.7 84.5 79.3
79.6
C-FRCNN† (ours)
Res101
84.7 88.2 83.1 76.2 71.1
87.9 88.7 89.5 68.7 88.6 78.2 89.5 88.7
84.8
86.2
55.4 84.7 82.0 86.0 81.7 82.2
Table 2. Performance of context reﬁnement improved Faster RCNN (C-FRCNN) de-
tector compared to other cutting-edge detectors on VOC 07 test set. †: the Faster
RCNN implemented following the protocol suggested by [3].
reﬁnement algorithm for improving performance in diﬀerent reﬁnement stages.
In particular, by improving the recall rates of generated proposals, context re-
ﬁnement brings 0.8 point’s gain in ﬁnal mAP using 0.5 as IoU threshold. When
further employing the proposed reﬁnement to the ﬁnal reﬁnement stage of FR-
CNN, there is another 1.6 points’ improvement using the same metric. The
presented statistics reveal that the proposed context reﬁnement is eﬀective in
improving detection performance, especially for the ﬁnal reﬁnement stage in
two-stage detectors.
Moreover, by well-considering the contextual information carried with sur-
rounding regions, the proposed method is supposed to greatly improve the detec-
tion results comparing to original detectors no matter what backbone network
is used. To verify this, we evaluate the enhancement in detection performance
of adopting the use of context reﬁnement for using diﬀerent backbone networks
such as VGG and ResNet in FRCNN detector, comparing to other state-of-
the-art two-stage detectors. All the other compared algorithms are processed as
described in original papers, using VOC 07+12 dataset as training set.
Table 2 presents the detailed results of C-FRCNN based on diﬀerent back-
bone networks, comparing to other state-of-the-art two-stage detectors based
on similar backbone networks. According to the results, context reﬁnement re-
spectively achieves 2.3 points higher mAP for VGG-based FRCNN detector and
2.6 points higher mAP for ResNet101-based FRCNN detector. ResNet101-based
C-FRCNN helps FRCNN surpass other state-of-the-art detectors, including the
context-aware algorithms such as [1] and [26].
Context Reﬁnement for Object Detection
13
Method
Network
AP mAP@0.5 mAP@0.7 mAP(small) mAP(medium) mAP(large)
TDM[31]
Inception-ResNet-v2 36.8
57.7
39.2
16.2
39.8
52.1
GRMI [20]
Inception-ResNet-v2 34.8
55.5
36.7
13.5
38.1
52.0
FPN [22]
Res101
36.2
59.1
39.0
18.2
39.0
48.2
FRCNN† [3, 29]
Res101
37.5
58.7
40.5
18.8
41.0
51.1
DRFCN [9]
Res101
37.1
58.9
39.8
17.1
40.3
51.3
Mask RCNN∗[16]
Res101
40.2
62.0
43.9
22.8
43.0
51.1
C-FRCNN† (ours)
Res101
39.0
59.7
42.8
19.4
42.4
53.0
C-DRFCN (ours)
Res101
39.1
60.9
42.5
19.0
42.4
53.2
C-Mask RCNN ∗(ours) [16]
Res101
42.0
62.9
46.4
23.4
44.7
53.8
Table 3. Performance of context reﬁnement improved Faster RCNN (C-FRCNN), De-
formable RFCN (C-DRFCN), and Mask RCNN (C-MaskRCNN) detectors compared
to other cutting-edge detectors on MS COCO test-dev results. †: the FRCNN im-
plemented following the protocol suggested by [3]. ∗: Mask RCNN trained with an
end-to-end scheme.
original refinement
context refinement
original refinement
context refinement
Fig. 6. Qualitative Results. Context reﬁnement has shown to improve the coordinates
as well as the labels of originally reﬁned results. Best illustrated in color.
6.2
MS COCO
We further evaluate our approach on MS COCO benchmark. The MS COCO
benchmark contains 80 objects of various sizes and is more challenging than the
14
Zhe Chen, Shaoli Huang, and Dacheng Tao
PASCAL VOC benchmark. This dataset has 80k images as train set. We report
the performance gain brought by context reﬁnement on the test-dev set with
20k images. In this part, besides FRCNN detector, we also embed the context
reﬁnement module to the compelling deformable RFCN (DRFCN) detector and
Mask RCNN detector and report enhancement in their detection performance.
We use C-DRFCN and C-Mask RCNN to respectively represent the relating
detectors reﬁned by our algorithm.
Table 3 illustrates the detailed performance of AP in diﬀerent conditions for
the evaluated methods. From it, we can ﬁnd that the context reﬁnement gener-
ally brings 1.5 to 2.0 points improvement over original detectors. It shows that
the performance for detecting objects of all the scales can be boosted to a bet-
ter score using our algorithm, proving the eﬀectiveness of the proposed method.
Furthermore, the C-FRCNN and C-DRFCN detectors have outperformed FPN,
by around 3 points. By improving the state-of-the-art detector, Mask RCNN,
C-Mask RCNN detector achieves the highest AP among all the evaluated meth-
ods even compared to the models with a more powerful backbone network, i.e.
InceptionResNetv2 [33]. This result also suggests that the proposed context re-
ﬁnement is insensitive to diﬀerent two-stage detection pipelines.
6.3
Qualitative Evaluation
Figure 6 presents qualitative results of the proposed context reﬁnement algo-
rithm. The illustrated images show that our algorithm is eﬀective in reducing
the false positive predictions based on the contexts carried by surrounding re-
gions. The context reﬁned results also provide better coverage about the objects.
7
Conclusion
In this study, we investigate the eﬀects of contextual information brought by
surrounding regions to improve the reﬁnement of a speciﬁc region. In order to
properly exploit the informative surrounding context, we propose the context re-
ﬁnement algorithm which attempts to identify context regions, extract and fuse
context based on adaptive weighting strategy, and perform reﬁnement. We im-
plement the proposed algorithm with an embedded architecture in both proposal
generation stage and ﬁnal reﬁnement stage of the two-stage detectors. Experi-
ments illustrate the eﬀectiveness of the proposed method. Notably, the two-stage
detectors improved by context reﬁnement achieve compelling performance on
well-known detection benchmarks against other state-of-the-art detectors.
8
Acknowledgement
This work was supported by Australian Research Council Projects FL-170100117,
DP-180103424, and LP-150100671. We would like to thank Dr. Wanli Ouyang
for his constructive suggestions on the response to review.
Context Reﬁnement for Object Detection
15
References
1. Bell, S., Lawrence Zitnick, C., Bala, K., Girshick, R.: Inside-outside net: Detecting
objects in context with skip pooling and recurrent neural networks. In: CVPR. pp.
2874–2883 (2016)
2. Bodla, N., Singh, B., Chellappa, R., Davis, L.S.: Soft-nms–improving object detec-
tion with one line of code. In: ICCV (2017)
3. Chen, X., Gupta, A.: An implementation of faster rcnn with study for region
sampling. arXiv preprint arXiv:1702.02138 (2017)
4. Chen, X., Gupta, A.: Spatial memory for context reasoning in object detection.
ICCV (2017)
5. Chen, Z., Chen, Z.: Rbnet: A deep neural network for uniﬁed road and road bound-
ary detection. In: ICONIP. pp. 677–687. Springer (2017)
6. Chen, Z., Hong, Z., Tao, D.: An experimental survey on correlation ﬁlter-based
tracking. arXiv preprint arXiv:1509.05520 (2015)
7. Chen, Z., You, X., Zhong, B., Li, J., Tao, D.: Dynamically modulated mask sparse
tracking. IEEE transactions on cybernetics 47(11), 3706–3718 (2017)
8. Dai, J., Li, Y., He, K., Sun, J.: R-fcn: Object detection via region-based fully
convolutional networks. In: NIPS. pp. 379–387 (2016)
9. Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convo-
lutional networks (2017)
10. Divvala, S.K., Hoiem, D., Hays, J.H., Efros, A.A., Hebert, M.: An empirical study
of context in object detection. In: CVPR. pp. 1271–1278. IEEE (2009)
11. Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal
visual object classes (voc) challenge. IJCV 88(2), 303–338 (2010)
12. Gidaris, S., Komodakis, N.: Object detection via a multi-region and semantic
segmentation-aware cnn model. In: ICCV. pp. 1134–1142 (2015)
13. Gidaris, S., Komodakis, N.: Attend reﬁne repeat: Active box proposal generation
via in-out localization. BMVC (2016)
14. Gidaris, S., Komodakis, N.: Locnet: Improving localization accuracy for object
detection. In: CVPR. pp. 789–798 (2016)
15. Girshick, R.: Fast r-cnn. In: ICCV. pp. 1440–1448 (2015)
16. He, K., Gkioxari, G., Doll´ar, P., Girshick, R.: Mask r-cnn. ICCV (2017)
17. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR. pp. 770–778 (2016)
18. Hong, Z., Chen, Z., Wang, C., Mei, X., Prokhorov, D., Tao, D.: Multi-store tracker
(muster): A cognitive psychology inspired approach to object tracking. In: CVPR.
pp. 749–758 (2015)
19. Hosang, J., Benenson, R., Schiele, B.: Learning non-maximum suppression. In:
CVPR (2017)
20. Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I.,
Wojna, Z., Song, Y., Guadarrama, S., et al.: Speed/accuracy trade-oﬀs for modern
convolutional object detectors. CVPR (2017)
21. Kong, T., Yao, A., Chen, Y., Sun, F.: Hypernet: Towards accurate region proposal
generation and joint object detection. In: CVPR. pp. 845–853 (2016)
22. Lin, T.Y., Doll´ar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature
pyramid networks for object detection. In: CVPR (2017)
23. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV. pp. 740–755.
Springer (2014)
16
Zhe Chen, Shaoli Huang, and Dacheng Tao
24. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.: Ssd:
Single shot multibox detector. In: ECCV. pp. 21–37. Springer (2016)
25. Mottaghi, R., Chen, X., Liu, X., Cho, N.G., Lee, S.W., Fidler, S., Urtasun, R.,
Yuille, A.: The role of context for object detection and semantic segmentation in
the wild. In: CVPR. pp. 891–898 (2014)
26. Ouyang, W., Wang, K., Zhu, X., Wang, X.: Learning chained deep features and
classiﬁers for cascade in object detection. ICCV (2017)
27. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Uniﬁed,
real-time object detection. In: CVPR. pp. 779–788 (2016)
28. Ren, J., Chen, X., Liu, J., Sun, W., Pang, J., Yan, Q., Tai, Y.W., Xu, L.: Accurate
single stage detector using recurrent rolling convolution. CVPR (2017)
29. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object de-
tection with region proposal networks. In: NIPS. pp. 91–99 (2015)
30. Shen, Z., Liu, Z., Li, J., Jiang, Y.G., Chen, Y., Xue, X.: Dsod: Learning deeply
supervised object detectors from scratch. In: CVPR. pp. 1919–1927 (2017)
31. Shrivastava, A., Sukthankar, R., Malik, J., Gupta, A.: Beyond skip connections:
Top-down modulation for object detection. arXiv preprint arXiv:1612.06851 (2016)
32. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition (2015)
33. Szegedy, C., Ioﬀe, S., Vanhoucke, V., Alemi, A.A.: Inception-v4, inception-resnet
and the impact of residual connections on learning. In: AAAI. pp. 4278–4284 (2017)
34. Uijlings, J.R., Van De Sande, K.E., Gevers, T., Smeulders, A.W.: Selective search
for object recognition. IJCV 104(2), 154–171 (2013)
35. Yu, R.R., Chen, X.S., Morariu, V.I., Davis, L.S., Redmond, W.: The role of context
selection in object detection. T-PAMI 32(9), 1627–1645 (2010)
36. Zagoruyko, S., Lerer, A., Lin, T.Y., Pinheiro, P.O., Gross, S., Chintala, S., Doll´ar,
P.: A multipath network for object detection. arXiv preprint arXiv:1604.02135
(2016)
37. Zeng, X., Ouyang, W., Yan, J., Li, H., Xiao, T., Wang, K., Liu, Y., Zhou, Y., Yang,
B., Wang, Z., et al.: Crafting gbd-net for object detection. T-PAMI (2017)
38. Zitnick, C.L., Doll´ar, P.: Edge boxes: Locating object proposals from edges. In:
ECCV. pp. 391–405. Springer (2014)