Pose Partition Networks for Multi-Person
Pose Estimation
Xuecheng Nie1[0000‚àí0003‚àí2433‚àí5983], Jiashi Feng1, Junliang Xing2, and
Shuicheng Yan1,3
1 ECE Department, National University of Singapore, Singapore
niexuecheng@u.nus.edu, elefjia@nus.edu.sg
2 Institute of Automation, Chinese Academy of Sciences, Beijing, China
jlxing@nlpr.ia.ac.cn
3 Qihoo 360 AI Institute, Beijing, China
yanshuicheng@360.cn
Abstract. This paper proposes a novel Pose Partition Network (PPN)
to address the challenging multi-person pose estimation problem. The
proposed PPN is favorably featured by low complexity and high ac-
curacy of joint detection and partition. In particular, PPN performs
dense regressions from global joint candidates within a speciÔ¨Åc embed-
ding space, which is parameterized by centroids of persons, to eÔ¨Éciently
generate robust person detection and joint partition. Then, PPN infers
body joint conÔ¨Ågurations through conducting graph partition for each
person detection locally, utilizing reliable global aÔ¨Énity cues. In this
way, PPN reduces computation complexity and improves multi-person
pose estimation signiÔ¨Åcantly. We implement PPN with the Hourglass
architecture as the backbone network to simultaneously learn joint de-
tector and dense regressor. Extensive experiments on benchmarks MPII
Human Pose Multi-Person, extended PASCAL-Person-Part, and WAF
show the eÔ¨Éciency of PPN with new state-of-the-art performance.
Keywords: Multi-Person Pose Estimation ¬∑ Pose Partition ¬∑ Dense Re-
gression
1
Introduction
Multi-person pose estimation aims to localize body joints of multiple persons
captured in a 2D monocular image [7,22]. Despite extensive prior research, this
problem remains very challenging due to the highly complex joint conÔ¨Åguration,
partial or even complete joint occlusion, signiÔ¨Åcant overlap between neighbor-
ing persons, unknown number of persons and more critically the diÔ¨Éculties in
allocating joints to multiple persons. These challenges feature the uniqueness
of multi-person pose estimation compared with the simpler single-person set-
ting [18,27]. To tackle these challenges, existing multi-person pose estimation
approaches usually perform joint detection and partition separately, mainly fol-
lowing two diÔ¨Äerent strategies. The top-down strategy [7,8,13,20,23] Ô¨Årst detects
2
X. Nie, J. Feng, J. Xing and S. Yan
(a) Input Image
(b) Pose Partition
(c) Local Inference
Fig. 1. Pose Partition Networks for multi-person pose estimation. (a) Input image. (b)
Pose partition. PPN models person detection and joint partition as a regression process
inferred from joint candidates. (c) Local inference. PPN performs local inference for
joint conÔ¨Ågurations conditioned on generated person detections with joint partitions
persons and then performs pose estimation for each single person individually.
The bottom-up strategy [3,12,11,15,16,22], in contrast, generates all joint candi-
dates at Ô¨Årst, and then tries to partition them to corresponding person instances.
The top-down approaches directly leverage existing person detection mod-
els [17,24] and single-person pose estimation methods [18,27]. Thus they eÔ¨Äec-
tively avoid complex joint partitions. However, their performance is critically
limited by the quality of person detections. If the employed person detector fails
to detect a person instance accurately (due to occlusion, overlapping or other dis-
tracting factors), the introduced errors cannot be remedied and would severely
harm performance of the following pose estimation. Moreover, they suÔ¨Äer from
high joint detection complexity, which linearly increases with the number of per-
sons in the image, because they need to run the single-person joint detector for
each person detection sequentially.
In contrast, the bottom-up approaches detect all joint candidates at Ô¨Årst
by globally applying a joint detector for only once and then partition them
to corresponding persons according to joint aÔ¨Énities. Hence, they enjoy lower
joint detection complexity than the top-down ones and better robustness to
errors from early commitment. However, they suÔ¨Äer from very high complexity
of partitioning joints to corresponding persons, which usually involves solving
NP-hard graph partition problems [11,22] on densely connected graphs covering
the whole image.
In this paper, we propose a novel solution, termed the Pose Partition Network
(PPN), to overcome essential limitations of the above two types of approaches
and meanwhile inherit their strengths within a uniÔ¨Åed model for eÔ¨Éciently and
eÔ¨Äectively estimating poses of multiple persons in a given image. As shown in
Fig. 1, PPN solves multi-person pose estimation problem by simultaneously 1)
modeling person detection and joint partition as a regression process over all joint
candidates and 2) performing local inference for obtaining joint categorization
and association conditioned on the generated person detections.
PPN for Multi-Person Pose Estimation
3
In particular, PPN introduces a dense regression module to generate person
detections with partitioned joints via votes from joint candidates in a carefully
designed embedding space, which is eÔ¨Éciently parameterized by person cen-
troids. This pose partition model produces joint candidates and partitions by
running a joint detector for only one feed-forward pass, oÔ¨Äering much higher ef-
Ô¨Åciency than top-down approaches. In addition, the produced person detections
from PPN are robust to various distracting factors, e.g., occlusion, overlapping,
deformation, and large pose variation, beneÔ¨Åting the following pose estimation.
PPN also introduces a local greedy inference algorithm by assuming indepen-
dence among person detections for producing optimal multi-person joint conÔ¨Åg-
urations. This local optimization strategy reduces the search space of the graph
partition problem for Ô¨Ånding optimal poses, avoiding high joint partition com-
plexity challenging the bottom-up strategy. Moreover, the local greedy inference
algorithm exploits reliable global aÔ¨Énity cues from the embedding space for
inferring joint conÔ¨Ågurations within robust person detections, leading to perfor-
mance improvement.
We implement PPN based on the Hourglass network [18] for learning joint
detector and dense regressor, simultaneously. Extensive experiments on MPII
Human Pose Multi-Person [1], extended PASCAL-Person-Part [28] and WAF [7]
benchmarks evidently show the eÔ¨Éciency and eÔ¨Äectiveness of the proposed PPN.
Moreover, PPN achieves new state-of-the-art on all these benchmarks.
We make following contributions. 1) We propose a new one feed-forward
pass solution to multi-person pose estimation, totally diÔ¨Äerent from previous
top-down and bottom-up ones. 2) We propose a novel dense regression module
to eÔ¨Éciently and robustly partition body joints into multiple persons, which
is the key to speeding up multi-person pose estimation. 3) In addition to high
eÔ¨Éciency, PPN is also superior in terms of robustness and accuracy on multiple
benchmarks.
2
Related Work
Top-Down Multi-Person Pose Estimation Existing approaches following
top-down strategy sequentially perform person detection and single-person pose
estimation. In [9], Gkioxari et al. proposed to adopt the Generalized Hough
Transform framework to Ô¨Årst generate person proposals and then classify joint
candidates based on the poselets. Sun et al. [25] presented a hierarchical part-
based model for jointly person detection and pose estimation. Recently, deep
learning techniques have been exploited to improve both person detection and
single-person pose estimation. In [13], Iqbal and Gall adopted Faster-RCNN [24]
based person detector and convolutional pose machine [27] based joint detector
for this task. Later, Fang et al. [8] utilized spatial transformer network [14]
and Hourglass network [18] to further improve the quality of joint detections
and partitions. Despite remarkable success, they suÔ¨Äer from limitations from
early commitment and high joint detection complexity. DiÔ¨Äerently, the proposed
PPN adopts a one feed-forward pass regression process for eÔ¨Éciently producing
4
X. Nie, J. Feng, J. Xing and S. Yan
(a) Joint detection (b) Dense regression
(d) Joint partition
(c) Centroid embedding
(e) Local greedy inference
Fig. 2. Overview of the proposed Pose Partition Network for multi-person pose esti-
mation. Given an image, PPN Ô¨Årst uses a CNN to predict (a) joint conÔ¨Ådence maps
and (b) dense joint-centroid regression maps. Then, PPN performs (c) centroid embed-
ding for all joint candidates in the embedding space via dense regression, to produce
(d) joint partitions within person detections. Finally, PPN conducts (e) local greedy
inference to generate joint conÔ¨Ågurations for each joint partition locally, giving pose
estimation results of multiple persons
person detections with partitioned joint candidates, oÔ¨Äering robustness to early
commitment as well as low joint detection complexity.
Bottom-Up Multi-Person Pose Estimation The bottom-up strategy pro-
vides robustness to early commitment and low joint detection complexity. Pre-
vious bottom-up approaches [3,11,19,22] mainly focus on improving either the
joint detector or joint aÔ¨Énity cues, beneÔ¨Åting the following joint partition and
conÔ¨Åguration inference. For joint detector, fully convolutional neural networks,
e.g., Residual networks [10] and Hourglass networks [18], have been widely ex-
ploited. As for joint aÔ¨Énity cues, Insafutdinov et al. [11] explored geometric
and appearance constraints among joint candidates. Cao et al. [3] proposed part
aÔ¨Énity Ô¨Åelds to encode location and orientation of limbs. Newell and Deng [19]
presented the associative embedding for grouping joint candidates. Nevertheless,
all these approaches partition joints based on partitioning the graph covering the
whole image, resulting in high inference complexity. In contrast, PPN performs
local inference with robust global aÔ¨Énity cues which is eÔ¨Éciently generated by
dense regressions from the centroid embedding space, reducing complexity for
joint partitions and improving pose estimation.
3
Approach
3.1
Pose Partition Model
The overall pipeline for the proposed Pose Partition Network (PPN) model is
shown in Fig. 2. Throughout the paper, we use following notations. Let I denote
an image containing multiple persons, p={p1, p2, . . . , pN} denote spatial coordi-
nates of N joint candidates from all persons in I with pv=(xv, yv)‚ä§, ‚àÄv=1, . . . , N,
and u={u1, u2, . . . , uN} denote the labels of corresponding joint candidates, in
which uv‚àà{1, 2, . . . , K} and K is the number of joint categories. For allocating
PPN for Multi-Person Pose Estimation
5
joints via local inference, we also consider the proximities between joints, denoted
as b‚ààRN√óN. Here b(v,w) encodes the proximity between the vth joint candidate
(pv, uv) and the wth joint candidate (pw, uw), and gives the probability for them
to be from the same person.
The proposed PPN with learnable parameters Œò aims to solve the multi-
person pose estimation task through learning to infer the conditional distribution
P(p, u, b|I, Œò). Namely, given the image I, PPN infers the joint locations p, labels
u and proximities b providing the largest likelihood probability. To this end, PPN
adopts a regression model to simultaneously produce person detections with
joint partitions implicitly and infers joint conÔ¨Åguration p and u for each person
detection locally. In this way, PPN reduces the diÔ¨Éculty and complexity of multi-
person pose estimation signiÔ¨Åcantly. Formally, PPN introduces latent variables
g={g1, g2, . . . , gM} to encode joint partitions, and each gi is a collection of joint
candidates (without labels) belonging to a speciÔ¨Åc person detection, and M is
the number of joint partitions. With these latent variables g, P(p, u, b|I, Œò) can
be factorized into
P(p, u, b|I, Œò) =
X
g
P(p, u, b, g|I, Œò) =
X
g
P(p|I, Œò)P(g|I, Œò, p)
|
{z
}
partition generation
P(u, b|I, Œò, p, g)
|
{z
}
joint conÔ¨Åguration
,
(1)
where P(p|I, Œò)P(g|I, Œò, p) models the joint partition generation process within
person detections based on joint candidates. Maximizing the above likelihood
probability gives optimal pose estimation for multiple persons in I.
However, directly maximizing the above likelihood is computationally in-
tractable. Instead of maximizing w.r.t. all possible partitions g, we propose to
maximize its lower bound induced by a single ‚Äúoptimal‚Äù partition, inspired by
the EM algorithm [6]. Such approximation could reduce the complexity signiÔ¨Å-
cantly without harming performance. Concretely, based on Eqn. (1), we have
P(p, u, b|I, Œò) ‚â•P(p|I, Œò)

max
g
P(g|I, Œò, p)

P(u, b|I, Œò, p, g).
(2)
Here, we Ô¨Ånd the optimal solution by maximizing the above induced lower bound
P(p, u, b, g|I, Œò), instead of maximizing the summation. The joint partitions g
disentangle independent joints and reduce inference complexity‚Äîonly the joints
falling in the same partition have non-zero proximities b. Then P(p, u, b, g|I, Œò)
is further factorized as
P(p, u, b, g|I, Œò) = P(p, g|I, Œò) √ó
Y
gi‚ààg
P(ugi|I, Œò, p, gi)P(bgi|I, Œò, p, gi, u), (3)
where ugi denotes the labels of joints falling in the partition gi and bgi denotes
their proximities. In the above probabilities, we deÔ¨Åne P(p, u, b, g|I, Œò) as a
Gibbs distribution:
P(p, u, b, g|I, Œò) ‚àùexp{‚àíE(p, u, b, g)},
(4)
6
X. Nie, J. Feng, J. Xing and S. Yan
(a)
(b)
pc
pv
r
ov
Centroid Embedding
Residual module
ConÔ¨Ådence maps
Convolution
Addition
Regression maps
Hourglass Module
Joint Detection Branch
Dense Regression Branch
Fig. 3. (a) Centroid embedding via dense joint regression. Left image shows centroid
embedding results for persons and right one illustrates construction of the regression
target for a pixel (Sec. 3.3). (b) Architecture of Pose Partition Network. Its backbone
is an Hourglass module (in blue block), followed by two branches: joint detection (in
green block) and dense regression for joint partition (in yellow block)
where E(p, u, b, g) is the energy function for the joint distribution P(p, u, b, g|I, Œò).
Its explicit form is derived from Eqn. (3) accordingly:
E(p, u, b, g) = ‚àíœï(p, g) ‚àí
X
gi‚ààg
 X
pv‚ààgi
œà(pv, uv) +
X
pv,pw‚ààgi
œÜ(pv, uv, pw, uw)

.
(5)
Here, œï(p, g) scores the quality of joint partitions g generated from joint candi-
dates p for the input image I, œà(pv, uv) scores how the position pv is compatible
with label uv, and œÜ(pv, uv, pw, uw) represents how likely the positions pv with
label uv and pw with label uw belong to the same person, i.e., characterizing
the proximity b(v,w). In the following subsections, we will give details for de-
tecting joint candidates p, generating optimal joint partitions g, inferring joint
conÔ¨Ågurations u and b along with the algorithm to optimize the energy function.
3.2
Joint Candidate Detection
To reliably detect human body joints, we use conÔ¨Ådence maps to encode probabil-
ities of joints presenting at each position in the image. The joint conÔ¨Ådence maps
are constructed by modeling the joint locations as Gaussian peaks, as shown in
Fig. 2 (a). We use Cj to denote the conÔ¨Ådence map for the jth joint with Ci
j
being the conÔ¨Ådence map of the jth joint for the ith person. For a position
pv in the given image, Ci
j(pv) is calculated by Ci
j(pv)= exp
 ‚àí‚à•pv‚àípi
j‚à•2
2/œÉ2
,
where pi
j denotes the groundtruth position of the jth joint of the ith person,
and œÉ is an empirically chosen constant to control variance of the Gaussian dis-
tribution and set as 7 in the experiments. The target conÔ¨Ådence map, which
the proposed PPN model learns to predict, is an aggregation of peaks of all the
persons in a single map. Here, we choose to take the maximum of conÔ¨Ådence
maps rather than average to remain distinctions between close-by peaks [3], i.e.
Cj(pv)= maxi Ci
j(pv). During testing, we Ô¨Årst Ô¨Ånd peaks with conÔ¨Ådence scores
PPN for Multi-Person Pose Estimation
7
greater than a given threshold œÑ (set as 0.1) on predicted conÔ¨Ådence maps ÀúC for
all types of joints. Then we perform Non-Maximum Suppression (NMS) to Ô¨Ånd
the joint candidate set Àúp={p1, p2, . . . , pN}.
3.3
Pose Partition via Dense Regression
Our proposed pose partition model performs dense regression over all the joint
candidates to localize centroids of multiple persons and partitions joints into
diÔ¨Äerent person instances accordingly, as shown in Fig. 2 (b) and (c). It learns
to transform all the pixels belonging to a speciÔ¨Åc person to an identical single
point in a carefully designed embedding space, where they are easy to cluster into
corresponding persons. Such a dense regression framework enables partitioning
joints by one single feed-forward pass, reducing joint detection complexity that
troubles top-down solutions.
To this end, we propose to parameterize the joint candidate embedding space
by the human body centroids, as they are stable and reliable to discriminate
diÔ¨Äerence person instances even in presence of some extreme poses. We denote
the constructed embedding space as H. In H, each person corresponds to a single
point (i.e., the centroid), and each point h‚àó‚ààH represents a hypothesis about
centroid location of a speciÔ¨Åc person instance. An example is given in the left
image of Fig. 3 (a).
Joint candidates are densely transformed into H and can collectively de-
termine the centroid hypotheses of their corresponding person instances, since
they are tightly related with articulated kinematics, as shown in Fig. 2 (c). For
instance, a candidate of the head joint would add votes for the presence of a
person‚Äôs centroid to the location just below it. A single candidate does not nec-
essarily provide suÔ¨Écient evidence for the exact centroid of a person instance,
but the population of joint candidates can vote for the correct centroid with large
probability and determine the joint partitions correctly. In particular, the prob-
ability of generating joint partition g‚àóat location h‚àóis calculated by summing
the votes from diÔ¨Äerent joint candidates together, i.e.
P(g‚àó|h‚àó) ‚àù
X
j
wj
  X
pv‚ààÀúp
‚ú∂[ ÀúCj(pv) ‚â•œÑ] exp{‚àí‚à•fj(pv) ‚àíh‚àó‚à•2
2}

,
(6)
where ‚ú∂[¬∑] is the indicator function and wj is the weight for the votes from
jth joint category. We set wj=1 for all joints assuming all kinds of joints equally
contribute to the localization of person instances in view of unconstrained shapes
of human body and uncertainties of presence of diÔ¨Äerent joints. The function
fj : p ‚ÜíH learns to densely transform every pixel in the image to the embedding
space H. For learning fj, we build the target regression map Ti
j for the jth joint
of the ith person as follows:
Ti
j(pv) =
oi
j,v/Z if pv ‚ààN i
j ,
0
otherwise,
oi
j,v = (pi
c ‚àípv) = (xi
c ‚àíxv, yi
c ‚àíyv),
(7)
where pi
c denotes the centroid position of the ith person, Z=
‚àö
H2 + W 2 is
the normalization factor, H and W denote the height and width of image I,
8
X. Nie, J. Feng, J. Xing and S. Yan
N i
j={pv| ‚à•pv‚àípi
j‚à•2‚â§r} denotes the neighbor positions of the jth joint of the
ith person, and r is a constant to deÔ¨Åne the neighborhood size, set as 7 in our
experiments. An example is shown in right image of Fig. 3 (a) for construction
of a regression target of a pixel in a given image. Then, we deÔ¨Åne the target
regression map Tj for the jth joint as the average for all persons by
Tj(pv) = 1
Nv
X
i
Ti
j(pv),
(8)
where Nv is the number of non-zero vectors at position pv across all persons.
During testing, after predicting the regression map ÀúTj, we deÔ¨Åne transforma-
tion function fj for position pv as fj(pv)=pv + Z ÀúTj(pv). After generating
P(g‚àó|h‚àó) for each point in the embedding space, we calculate the score œï(p, g)
as œï(p, g)= P
i log P(gi|hi).
Then the problem of joint partition generation is converted to Ô¨Ånding peaks
in the embedding space H. As there are no priors on the number of persons in
the image, we adopt the Agglomerative Clustering [2] to Ô¨Ånd peaks by clustering
the votes, which can automatically determine the number of clusters. We denote
the vote set as h={hv|hv=fj(pv), ÀúCj(pv)‚â•œÑ, pv‚ààÀúp}, and use C={C1, . . . , CM}
to denote the clustering result on h, where Ci represents the ith cluster and M
is the number of clusters. We assume the set of joint candidates casting votes in
each cluster corresponds to a joint partition gi, deÔ¨Åned by
gi = {pv|pv ‚ààÀúp, ÀúCj(pv) ‚â•œÑ, fj(pv) ‚ààCi}.
(9)
3.4
Local Greedy Inference for Pose Estimation
According to Eqn. (4), we maximize the conditional probability P(p, u, b, g|I, Œò)
by minimizing energy function E(p, u, b, g) in Eqn. (5). We optimize E(p, u, b, g)
in two sequential steps: 1) generate joint partition set based on joint candi-
dates; 2) conduct joint conÔ¨Åguration inference in each joint partition locally,
which reduces the joint conÔ¨Åguration complexity and overcomes the drawback
of bottom-up approaches.
After getting joint partition according to Eqn. (9), the score œï(p, g) becomes
a constant. Let Àúg denote the generated partition set. The optimization is then
simpliÔ¨Åed as
Àúu, Àúb = arg min
u,b

‚àí
X
gi‚ààÀúg
 X
pv‚ààgi
œà(pv, uv) +
X
pv,pw‚ààgi
œÜ(pv, uv, pw, uw)

. (10)
Pose estimation in each joint partition is independent, thus inference over dif-
ferent joint partitions becomes separate. We propose the following local greedy
inference algorithm to solve Eqn. (10) for multi-person pose estimation. Given
a joint partition gi, the unary term œà(pv, uv) is the conÔ¨Ådence score at pv from
the uvth joint detector: œà(pv, uv)= ÀúCuv(pv). The binary term œÜ(pv, uv, pw, uw)
PPN for Multi-Person Pose Estimation
9
Algorithm 1: Local greedy inference for multi-person pose estimation
input : joint candidates Àúp, joint partitions Àúg, joint conÔ¨Ådence maps ÀúC, dense
regression maps ÀúT, œÑ.
output: multi-person pose estimation R
initialization: R ‚Üê‚àÖ
for gi ‚ààÀúg do
while gi Ã∏= ‚àÖdo
Initialize single-person pose estimation P ‚Üê‚àÖ
for jth joint category, j = 1 to K do
if P = ‚àÖthen
Find root joint candidate in gi for P by:
p‚àó‚Üêarg maxpv‚ààgi ÀúCj(pv)
else
Find joint candidate closest to centroid c:
p‚àó‚Üêarg maxpv‚ààgi
‚ú∂[ Àú
Cj(pv)‚â•œÑ]
exp{‚à•fj(pv)‚àíc‚à•2
2}
end
if ÀúCj(p‚àó) ‚â•œÑ then
Update P‚ÜêP‚à™{(p‚àó, j)}, gi‚Üêgi\{p‚àó} Update c by averaging
the person centroid hypotheses: c ‚ÜêP
(pv,n)‚ààP fn(pv)/|P|
end
end
Update R ‚ÜêR ‚à™{P}
end
end
is the similarity score of votes of two joint candidates based on the global aÔ¨Énity
cues in the embedding space:
œÜ(pv, uv, pw, uw) = ‚ú∂[ ÀúCuv(pv) ‚â•œÑ]‚ú∂[ ÀúCuw(pw) ‚â•œÑ] exp{‚àí‚à•hv ‚àíhw‚à•2
2},
(11)
where hv=pv+Z ÀúTuv(pv) and hw=pw+Z ÀúTuw(pw).
For eÔ¨Écient inference in Eqn. (10), we adopt a greedy strategy which guar-
antees the energy monotonically decreases and eventually converges to a lower
bound. SpeciÔ¨Åcally, we iterate through each joint one by one, Ô¨Årst considering
joints around torso and moving out to limb. We start the inference with neck.
For a neck candidate, we use its embedding point in H to initialize the centroid
of its person instance. Then, we select the head top candidate closest to the per-
son centroid and associate it with the same person as the neck candidate. After
that, we update person centroid by averaging the derived hypotheses. We loop
through all other joint candidates similarly. Finally, we get a person instance
and its associated joints. After utilizing neck as root for inferring joint conÔ¨Åg-
urations of person instances, if some candidates remain unassigned, we utilize
joints from torso, then from limbs, as the root to infer the person instance. After
all candidates Ô¨Ånd their associations to persons, the inference terminates. See
details in Algorithm 1.
10
X. Nie, J. Feng, J. Xing and S. Yan
4
Learning Joint Detector and Dense Regressor with
CNNs
PPN is a generic model and compatible with various CNN architectures. Exten-
sive architecture engineering is out of the scope of this work. We simply choose
the state-of-the-art Hourglass network [18] as the backbone of PPN. Hourglass
network consists of a sequence of Hourglass modules. As shown in Fig. 3 (b),
each Hourglass module Ô¨Årst learns down-sized feature maps from the input im-
age, and then recovers full-resolution feature maps through up-sampling for pre-
cise joint localization. In particular, each Hourglass module is implemented as
a fully convolutional network. Skipping connections are added between feature
maps with the same resolution symmetrically to capture information at every
scale. Multiple Hourglass modules are stacked sequentially for gradually reÔ¨Åning
the predictions via reintegrating the previous estimation results. Intermediate
supervision is applied at each Hourglass module.
Hourglass network was proposed for single-person pose estimation. PPN ex-
tends it to multi-person cases. PPN introduces modules enabling simultaneous
joint detection (Sec. 3.2) and dense joint-centroid regression (Sec. 3.3), as shown
in Fig. 3 (b). In particular, PPN utilizes the Hourglass module to learn image
representations and then separates into two branches: one produces the dense re-
gression maps for detecting person centroids, via one 3√ó3 convolution on feature
maps from the Hourglass module and another 1√ó1 convolution for classiÔ¨Åcation;
the other branch produces joint detection conÔ¨Ådence maps. With this design,
PPN obtains joint detection and partition in one feed-forward pass. When using
multi-stage Hourglass modules, PPN feeds the predicted dense regression maps
at every stage into the next one through 1√ó1 convolution, and then combines
intermediate features with features from the previous stage.
For training PPN, we use ‚Ñì2 loss to learn both joint detection and dense
regression branches with supervision at each stage. The losses are deÔ¨Åned as
Lt
joint ‚âú
X
j
X
v
‚à•ÀúCt
j(pv) ‚àíCj(pv)‚à•2
2
Lt
regression ‚âú
X
j
X
v
‚à•ÀúTt
j(pv) ‚àíTj(pv)‚à•2
2,
(12)
where ÀúCt
j and ÀúTt
j represent predicted joint conÔ¨Ådence maps and dense regres-
sion maps at the tth stage, respectively. The groundtruth Cj(pv) and Tj(pv)
are constructed as in Sec. 3.2 and 3.3 respectively. The total loss is given by
L= PT
t=1(Lt
joint + Œ±Lt
regression), where T=8 is the number of Hourglass modules
(stages) used in implementation and weighting factor Œ± is empirically set as 1.
5
Experiments
5.1
Experimental Setup
Datasets We evaluate the proposed PPN on three widely adopted benchmarks:
MPII Human Pose Multi-Person (MPII) dataset [1], extended PASCAL-Person-
PPN for Multi-Person Pose Estimation
11
Table 1. Comparison with state-of-the-arts on the full testing set of MPII Human
Pose Multi-Person dataset (AP)
Method
Head
Shoulder
Elbow
Wrist
Hip
Knee
Ankle
Total
Time [s]
Iqbal and Gall [13]
58.4
53.9
44.5
35.0
42.2
36.7
31.1
43.1
10
Insafutdinov et al. [11]
78.4
72.5
60.2
51.0
57.2
52.0
45.4
59.5
485
Levinkov et al. [16]
89.8
85.2
71.8
59.6
71.1
63.0
53.5
70.6
-
Insafutdinov et al. [12]
88.8
87.0
75.9
64.9
74.2
68.8
60.5
74.3
-
Cao et al. [3]
91.2
87.6
77.7
66.8
75.4
68.9
61.7
75.6
1.24
Fang et al. [8]
88.4
86.5
78.6
70.4
74.4
73.0
65.8
76.7
1.5
Newell and Deng [19]
92.1
89.3
78.9
69.8
76.2
71.6
64.7
77.5
-
PPN (Ours)
92.2
89.7
82.1
74.4
78.6
76.4
69.3
80.4
0.77
Part dataset [28], and ‚ÄúWe Are Family‚Äù (WAF) dataset [7]. The MPII dataset
consists of 3,844 and 1,758 groups of multiple interacting persons for training and
testing respectively. Each person in the image is annotated for 16 body joints.
It also provides more than 28,000 training samples for single-person pose esti-
mation. The extended PASCAL-Person-Part dataset contains 3,533 challenging
images from the original PASCAL-Person-Part dataset [4], which are split into
1,716 for training and 1,817 for testing. Each person is annotated with 14 body
joints shared with MPII dataset, without pelvis and thorax. The WAF dataset
contains 525 web images (350 for training and 175 for testing). Each person is
annotated with 6 line segments for the upper-body.
Data Augmentation We follow conventional ways to augment training samples
by cropping original images based on the person center. In particular, we aug-
ment each training sample with rotation degrees sampled in [‚àí40‚ó¶, 40‚ó¶], scaling
factors in [0.7, 1.3], translational oÔ¨Äset in [‚àí40px, 40px] and horizontally mirror.
We resize each training sample to 256√ó256 pixels with padding.
Implementation For MPII dataset, we reserve 350 images randomly selected
from the training set for validation. We use the rest training images and all the
provided single-person samples to train the PPN for 250 epochs. For evaluation
on the other two datasets, we follow the common practice and Ô¨Ånetune the PPN
model pretrained on MPII for 30 epochs. To deal with some extreme cases where
centroids of persons are overlapped, we slightly perturb the centroids by adding
small oÔ¨Äset to separate them. We implement our model with PyTorch [21] and
adopt the RMSProp [26] for optimization. The initial learning rate is 0.0025 and
decreased by multiplying 0.5 at the 150th, 170th, 200th, 230th epoch. In testing,
we follow conventions to crop image patches using the given position and average
person scale of test images, and resize and pad the cropped samples to 384√ó384
as input to PPN. We search for suitable image scales over 5 diÔ¨Äerent choices.
Specially, when testing on MPII, following previous works [3,19], we apply a
single-person model [18] trained on MPII to reÔ¨Åne the estimations. We use the
standard Average Precision (AP) as performance metric on all the datasets, as
suggested by [11,28]. Our codes and pre-trained models will be made available.
12
X. Nie, J. Feng, J. Xing and S. Yan
Table 2. Comparison with state-of-the-arts on the testing set of the extended
PASCAL-Person-Part dataset (AP)
Method
Head
Shoulder
Elbow
Wrist
Hip
Knee
Ankle
Total
Chen and Yuille [5]
45.3
34.6
24.8
21.7
9.8
8.6
7.7
21.8
Insafutdinov et al. [11]
41.5
39.3
34.0
27.5
16.3
21.3
20.6
28.6
Xia et at. [28]
58.0
52.1
43.1
37.2
22.1
30.8
31.1
39.2
PPN (Ours)
66.9
60.0
51.4
48.9
29.2
36.4
33.5
46.6
Table 3. Comparison with state-of-the-arts on testing set of WAF dataset (AP)
Method
Head
Shoulder
Elbow
Wrist
Total
Chen and Yuile [5]
83.3
56.1
46.3
35.5
55.3
Pishchulin et al. [22]
76.6
80.8
73.7
73.6
76.2
Insafutdinov et al. [11]
92.6
81.1
75.7
78.8
82.0
PPN (Ours)
93.1
82.9
83.5
79.9
84.8
5.2
Results and Analysis
MPII Table 1 shows the evaluation results on the full testing set of MPII.
We can see that the proposed PPN achieves overall 80.4% AP and signiÔ¨Åcantly
outperforms previous state-of-the-art achieving 77.5% AP [19]. In addition, the
proposed PPN improves the performance for localizing all the joints consistently.
In particular, it brings remarkable improvement over rather diÔ¨Écult joints mainly
caused by occlusion and high degrees of freedom, including wrists (74.4% vs
69.8% AP), ankles (69.3% vs 64.7% AP), and knees (with absolute 4.8% AP
increase over [19]), conÔ¨Årming the robustness of the proposed pose partition
model and global aÔ¨Énity cues to these distracting factors. These results clearly
show PPN is outstandingly eÔ¨Äective for multi-person pose estimation. We also
report the computational speed of PPN1 in Table 1. PPN is about 2 times faster
than the bottom-up approach [3] with state-of-the-art speed for multi-person
pose estimation. This demonstrates the eÔ¨Éciency of performing joint detection
and partition simultaneously in our model.
PASCAL-Person-Part Table 2 shows the evaluation results. PPN provides ab-
solute 7.4% AP improvement (46.6% vs 39.2% AP) over the state-of-the-art [28].
Moreover, the proposed PPN brings signiÔ¨Åcant improvement on diÔ¨Écult joints,
such as wrist (48.9% vs 37.2% AP). These results further demonstrate the eÔ¨Äec-
tiveness and robustness of our model for multi-person pose estimation.
WAF As shown in Table 3, PPN achieves overall 84.8% AP, bringing 3.4%
improvement over the best bottom-up approach [11]. PPN achieves the best
performance for all upper-body joints. In particular, it gives the most signiÔ¨Åcant
performance improvement on the elbow, about 10.3% higher than previous best
1 The runtime time is measured on CPU Intel I7-5820K 3.3GHz and GPU TITAN X
(Pascal). The time is counted with 5 scale testing, not including the reÔ¨Ånement time
by single-person pose estimation.
PPN for Multi-Person Pose Estimation
13
Table 4. Ablation experiments on MPII validation set (AP)
Method
Head
Shoulder
Elbow
Wrist
Hip
Knee
Ankle
Total
InferTime [ms]
PPN-Full
94.4
90.0
81.3
72.1
77.8
72.7
64.7
79.0
1.9
PPN-w/o-Partition
93.2
89.3
79.9
70.1
78.8
73.1
65.7
78.6
3.4
PPN-w/o-LGI
93.1
89.1
79.5
68.5
79.0
71.4
64.4
77.8
-
PPN-w/o-ReÔ¨Ånement
90.4
86.8
79.3
69.8
77.5
69.3
61.9
76.4
-
PPN-256√ó256
91.0
87.1
78.6
70.2
76.7
70.5
60.0
76.3
-
PPN-Vanilla
90.5
86.4
77.1
69.4
72.2
67.7
60.2
74.8
-
results. These results verify the eÔ¨Äectiveness of the proposed PPN for tackling
the multi-person pose estimation problem.
Qualitative Results Visualization examples of pose partition, local inference,
and multi-person pose estimation by the proposed PPN on these three datasets
are provided in the supplemental materials.
5.3
Ablation Analysis
We conduct ablation analysis for the proposed PPN model using the MPII vali-
dation set. We evaluate multiple variants of our proposed PPN model by remov-
ing certain components from the full model (‚ÄúPPN-Full‚Äù). ‚ÄúPPN-w/o-Partition‚Äù
performs inference on the whole image without using obtained joint partition in-
formation, which is similar to the pure bottom-up approaches. ‚ÄúPPN-w/o-LGI‚Äù
removes the local greedy inference phase. It allocates joint candidates to persons
through Ô¨Ånding the most activated position for each joint in each joint parti-
tion. This is similar to the top-down approaches. ‚ÄúPPN-w/o-ReÔ¨Ånement‚Äù does
not perform reÔ¨Ånement by using single-person pose estimator. We use ‚ÄúPPN-
256√ó256‚Äù to denote testing over 256√ó256 images and ‚ÄúPPN-Vanilla‚Äù to denote
single scale testing without reÔ¨Ånement.
From Table 4, ‚ÄúPPN-Full‚Äù achieves 79.0% AP and the joint partition in-
ference only costs 1.9ms, which is very eÔ¨Écient. ‚ÄúPPN-w/o-Partition‚Äù achieves
slightly lower AP (78.6%) with slower inference speed (3.4ms). The results con-
Ô¨Årm eÔ¨Äectiveness of generating joint partitions by PPN‚Äîinference within each
joint partition individually reduces complexity and improves pose estimation
over multi-persons. Removing the local greedy inference phase as in ‚ÄúPPN-w/o-
LGI‚Äù decreases the performance to 77.8% AP, showing local greedy inference is
beneÔ¨Åcial to pose estimation by eÔ¨Äectively handling false alarms of joint can-
didate detection based on global aÔ¨Énity cues in the embedding space. Com-
parison of ‚ÄúPPN-w/o-ReÔ¨Ånement‚Äù(76.4% AP) with the full model demonstrates
that single-person pose estimation can reÔ¨Åne joint localization. ‚ÄúPPN-Vanilla‚Äù
achieves 74.8% AP, verifying the stableness of our approach for multi-person
pose estimation even in the case of removing reÔ¨Ånement and multi-scale testing.
We also evaluate the pose estimation results from 4 diÔ¨Äerent stages of the
PPN model and plot the results in Fig. 4 (a). The performance increases mono-
tonically when traversing more stages. The Ô¨Ånal results achieved at the 8th stage
give about 23.4% improvement comparing with the Ô¨Årst stage (79.0% vs 64.0%
14
X. Nie, J. Feng, J. Xing and S. Yan
0
0.1
0.2
0.3
0.4
0.5
Normalized Distance
0
0.2
0.4
0.6
0.8
1
Mean Average Precision
8-stage
4-stage
2-stage
1-stage
Person Num of Groundtruth
1
2
3
4
5
6
7
8
9
10
Person Num of Pose Partition
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0.0%
0.0%
7
2.0%
218
62.3%
11
3.1%
3
0.9%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
91.2%
8.8%
0
0.0%
6
1.7%
53
15.1%
4
1.1%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
84.1%
15.9%
0
0.0%
0
0.0%
3
0.9%
19
5.4%
1
0.3%
1
0.3%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
79.2%
20.8%
0
0.0%
0
0.0%
1
0.3%
1
0.3%
7
2.0%
2
0.6%
0
0.0%
1
0.3%
0
0.0%
0
0.0%
58.3%
41.7%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
6
1.7%
2
0.6%
0
0.0%
0
0.0%
0
0.0%
75.0%
25.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
1
0.3%
0
0.0%
0
0.0%
0
0.0%
100%
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
1
0.3%
1
0.3%
1
0.3%
33.3%
66.7%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0.0%
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0
0.0%
0.0%
0.0%
0.0%
100%
97.3%
2.7%
77.9%
22.1%
70.4%
29.6%
87.5%
12.5%
66.7%
33.3%
33.3%
66.7%
50.0%
50.0%
0.0%
100%
0.0%
100%
87.1%
12.9%
1
2
3
4
5
6
7
8
9
10
(a)
(b)
Fig. 4. (a) Ablation study on multi-stage Hourglass network. (b) Confusion matrix on
person number inferred from pose partition (Sec. 3.3) with groundtruth. Mean square
error is 0.203. Best viewed in color and 2√ó zoom
AP). This is because the proposed PPN can recurrently correct errors on the
dense regression maps along with the joint conÔ¨Ådence maps conditioned on pre-
vious estimations in the multi-stage design, yielding gradual improvement on
the joint detections and partitions for multi-person pose estimation.
Finally, we evaluate the eÔ¨Äectiveness of pose partition model for partition
person instances. In particular, we evaluate how its produced partitions match
the real number of persons. The confusion matrix is shown in Fig. 4 (b). We
can observe the proposed pose partition model can predict very close number of
persons with the groundtruth, with mean square error as small as 0.203.
6
Conclusion
We presented the Pose Partition Network (PPN) to eÔ¨Éciently and eÔ¨Äectively
address the challenging multi-person pose estimation problem. PPN solves the
problem by simultaneously detecting and partitioning joints for multiple persons.
It introduces a new approach to generate partitions through inferring over joint
candidates in the embedding space parameterized by person centroids. Moreover,
PPN introduces a local greedy inference approach to estimate poses for person
instances by utilizing the partition information. We demonstrate that PPN can
provide appealing eÔ¨Éciency for both joint detection and partition, and it can
signiÔ¨Åcantly overcome limitations of pure top-down and bottom-up solutions on
three benchmarks multi-person pose estimation datasets.
Acknowledgement
Jiashi Feng was partially supported by NUS IDS R-263-000-C67-646, ECRA
R-263-000-C87-133 and MOE Tier-II R-263-000-D17-112.
PPN for Multi-Person Pose Estimation
15
References
1. Andriluka, M., Pishchulin, L., Gehler, P., Schiele, B.: 2d human pose estimation:
New benchmark and state of the art analysis. In: CVPR (2014) 3, 10
2. Bourdev, L., Malik, J.: Poselets: Body part detectors trained using 3d human pose
annotations. In: ICCV (2009) 8
3. Cao, Z., Simon, T., Wei, S.E., Sheikh, Y.: Realtime multi-person 2d pose estimation
using part aÔ¨Énity Ô¨Åelds. In: CVPR (2017) 2, 4, 6, 11, 12
4. Chen, X., Mottaghi, R., Liu, X., Fidler, S., Urtasun, R., Yuille, A.L.: Detect what
you can: Detecting and representing objects using holistic models and body parts.
In: CVPR (2014) 11
5. Chen, X., Yuille, A.L.: Parsing occluded people by Ô¨Çexible compositions. In: CVPR
(2015) 12
6. Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum likelihood from incomplete
data via the em algorithm. J. Royal Stat. Soc. B. 39(1), 1‚Äì38 (1977) 5
7. Eichner, M., Ferrari, V.: We are family: Joint pose estimation of multiple persons.
In: ECCV (2010) 1, 3, 11
8. Fang, H., Xie, S., Tai, Y., Lu, C.: RMPE: Regional multi-person pose estimation.
In: ICCV (2017) 1, 3, 11
9. Gkioxari, G., Hariharan, B., Girshick, R., Malik, J.: Using k-poselets for detecting
people and localizing their keypoints. In: CVPR (2014) 3
10. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR (2016) 4
11. Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka, M., Schiele, B.: Deepercut:
A deeper, stronger, and faster multi-person pose estimation model. In: ECCV
(2016) 2, 4, 11, 12
12. Insafutdinov, E., Andriluka, M., Pishchulin, L., Tang, S., Andres, B., Schiele, B.:
Articulated multi-person tracking in the wild. In: CVPR (2017) 2, 11
13. Iqbal, U., Gall, J.: Multi-person pose estimation with local joint-to-person associ-
ations. In: ECCV (2016) 1, 3, 11
14. Jaderberg, M., Simonyan, K., Zisserman, A., et al.: Spatial transformer networks.
In: NIPS (2015) 3
15. Ladicky, L., Torr, P.H., Zisserman, A.: Human pose estimation using a joint pixel-
wise and part-wise formulation. In: CVPR (2013) 2
16. Levinkov, E., Uhrig, J., Tang, S., Omran, M., Insafutdinov, E., Kirillov, A., Rother,
C., Brox, T., Schiele, B., Andres, B.: Joint graph decomposition & node labeling:
Problem, algorithms, applications. In: CVPR (2017) 2, 11
17. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.: Ssd:
Single shot multibox detector. In: ECCV (2016) 2
18. Newell, A., Yang, K., Deng, J.: Stacked hourglass networks for human pose esti-
mation. In: ECCV (2016) 1, 2, 3, 4, 10, 11
19. Newell, A., Deng, J.: Associative embedding: End-to-end learning for joint detec-
tion and grouping. In: NIPS (2017) 4, 11, 12
20. Papandreou, G., Zhu, T., Kanazawa, N., Toshev, A., Tompson, J., Bregler, C.,
Murphy, K.: Towards accurate multi-person pose estimation in the wild. In: CVPR
(2017) 1
21. Paszke, A., Gross, S., Chintala, S.: Pytorch (2017) 11
22. Pishchulin, L., Insafutdinov, E., Tang, S., Andres, B., Andriluka, M., Gehler, P.,
Schiele, B.: Deepcut: Joint subset partition and labeling for multi person pose
estimation. In: CVPR (2016) 1, 2, 4, 12
16
X. Nie, J. Feng, J. Xing and S. Yan
23. Pishchulin, L., Jain, A., Andriluka, M., Thorm¬®ahlen, T., Schiele, B.: Articulated
people detection and pose estimation: Reshaping the future. In: CVPR (2012) 1
24. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object de-
tection with region proposal networks. In: NIPS (2015) 2, 3
25. Sun, M., Savarese, S.: Articulated part-based model for joint object detection and
pose estimation. In: ICCV (2011) 3
26. Tieleman, T., Hinton, G.: Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural Networks for Machine Learn-
ing (2012) 11
27. Wei, S.E., Ramakrishna, V., Kanade, T., Sheikh, Y.: Convolutional pose machines.
In: CVPR (2016) 1, 2, 3
28. Xia, F., Wang, P., Chen, X., Yuille, A.L.: Joint multi-person pose estimation and
semantic part segmentation. In: CVPR (2017) 3, 11, 12